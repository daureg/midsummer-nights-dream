\section{Experimental Analysis}\label{s:exp}

We now evaluate our edge sign classification methods on representative real-world datasets of
varying density and label regularity. This shows showing that our methods compete well against
existing approaches in terms of both predictive and computational performance. We are especially
interested in small training set regimes, and have restricted our comparison to the batch learning
scenario since all competing methods we are aware of have been developed in that setting only.

\paragraph{Datasets}

We considered five real-world classification datasets. The first three are directed \ssn{} widely
used as benchmarks for this task~(\eg{}\autocites{Leskovec2010}{shahriari2014ranking}{wu2016troll}):
In \wik{}, there is an edge from user $u$ to user $v$ if $v$ applies for an admin position and $u$
votes for or against that promotion. In \sla{}, a news sharing and commenting website, member $u$
can tag other members $v$ as friends or foes. Finally, in \epi{}, an online shopping website, user
$v$ reviews products and, based on these reviews, another user $u$ can display whether he considers
$v$ to be reliable or not. In addition to these three datasets, we considered two other \ssn{} where
the signs are inferred automatically, rather than given explicitly by the users.  In
\kiw{}~\cite{wikiedits11}, an edge from Wikipedia user $u$ to user $v$ indicates whether they edited
the same article in a constructive manner or not.\footnote{This is the
\href{http://konect.uni-koblenz.de/networks/wikisigned-k2}{KONECT version of the
\enquote{Wikisigned} dataset}, from which we removed self-loops.} Finally, in the
\aut{}~\cite{kumar2016structure} network, an author $u$ cites another author $v$ by either endorsing
or criticizing $v$'s work. The edge sign is derived by classifying the citation sentiment with a
simple, yet powerful, keyword-based technique using a list of positive and negative words. See
\cite{kumar2016structure} for more details.\footnote{We again removed self-loops and merged
multi-edges which are all of the same sign.}

\begin{table*}[bt]
  \centering
  \caption{Dataset properties. The 5th column gives the fraction of positive labels. The last two
  columns provide two different measures of label regularity ---see main text.\label{tab:dataset}}
  \begin{tabular}{lrrrrrrrr}
    \toprule
    Dataset &       $|V|$ &       $|E|$ &$\frac{|E|}{|V|}$ & $\frac{|E^+|}{|E|}$ & $\frac{\Psi^2_{G''}(Y)}{|E|}$ & $\frac{\Psi_G(Y)}{|E|}$ \\ % & twin edges & twin disagreement \\
    \midrule
    \aut{} &   \np{4831} &  \np{39452} & 8.1  &              72.3\% &                    .076 &                       .191 \\ %&      5.07\% &            27.10\% \\
    \wik{} &   \np{7114} & \np{103108} & 14.5 &              78.8\% &                    .063 &                       .142 \\ %&      5.56\% &            10.04\% \\
    \sla{} &  \np{82140} & \np{549202} & 6.7  &              77.4\% &                    .059 &                       .143 \\ %&     17.74\% &             4.00\% \\
    \kiw{} & \np{138587} & \np{740106} & 5.3  &              87.9\% &                    .034 &                       .086 \\ %&      6.55\% &             2.09\% \\
    \epi{} & \np{131580} & \np{840799} & 6.4  &              85.3\% &                    .031 &                       .074 \\ %&     30.83\% &            14.64\% \\
    \bottomrule
  \end{tabular}
\end{table*}

\autoref{tab:dataset} summarizes statistics for these datasets. We note that most edge labels are
positive. Hence, test set accuracy is not an appropriate measure of prediction performance. We
instead evaluated our performance using the so-called Matthews Correlation Coefficient
(MCC)~\autocite{MCC00}), defined as \Todo{For completeness, we could also report accuracy results in
the supplementary material.}  
\[
	\mathrm{MCC} = \frac{tp\times tn-fp\times fn}%
        {\sqrt{ (tp + fp) ( tp + fn ) ( tn + fp ) ( tn + fn ) } } = \pm \sqrt{\frac{\chi^2}{n}}
\]
MCC combines all the four quantities found in a binary confusion matrix ($t$rue $p$ositive, $t$rue
$n$egative, $f$alse $p$ositive and $f$alse $n$egative) into a single metric which ranges from $-1$
(when all predictions are incorrect) to $+1$ (when all predictions are correct) through $0$ (when
predictions are made uniformly at random).

Although the semantics of the edge signs is not the same across these networks, we can see from
\autoref{tab:dataset} that our generative model essentially fits all of them. Specifically, the last
two columns of the table report the rate of label (ir)regularity, as measured by
$\Psi^2_{G''}(Y)/|E|$ (second-last column) and $\Psi_{G}(Y)/|E|$ (last column), where 
\[
\Psi^2_{G''}(Y) = \min_{(\bp,\bq)} \left(f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)\right)\,,
\]
$f_{E_0}$ and $f_{E\setminus E_0}$ being the quadratic criterions of \autoref{ss:passive}, viewed as
functions of both $(\bp,\bq)$, and $y_{i,j}$ when all labels are known, and $\Psi_{G}(Y)$ is the
label regularity measure adopted in the online setting, as defined in \autoref{s:prel}. It is
reasonable to expect that higher label irregularity corresponds to lower prediction performance.
This trend is in fact confirmed by our experimental findings: whereas \epi{} tends to be easy,
\aut{} tends to be hard, and this holds for all algorithms we tested, even if they do not explicitly
comply with our inductive bias principles. Moreover, $\Psi^2_{G''}(Y)/|E|$ tends to be proportional
to $\Psi_{G}(Y)/|E|$ across datasets, hence confirming the anticipated connection between the two
regularity measures.

\iffalse
***********************************************************************
Finally, there is a low fraction of reciprocal (or \emph{twin}) edges (i.e.\
$\eij \in E$ and $\eji \in E$, which is a common mechanism of link formation in
directed networks~\cite{DirectedReciprocity04}), and in most cases they do not
disagree, meaning they have the same sign. In practice, we use that fact to
improve our accuracy at no additional computational cost: when predicting $\eij
\in E_{\mathrm{test}}$, if the reciprocal edge \eji{} is part of the training set, we
set $\sgn(\eij) = \sgn(\eji)$.

from http://www.nature.com/articles/srep02729

The study of link reciprocity in binary directed networks1,2, or the tendency
of vertex pairs to form mutual connections, has received an increasing
attention in recent years3,4,5,6,7,8,9,10,11,12,13,14. Among other things,
reciprocity has been shown to be crucial in order to classify3 and model4
directed networks, understand the effects of network structure on dynamical
processes (e.g. diffusion or percolation processes5,6,7), explain patterns of
growth in out-of-equilibrium networks (as in the case of the Wikipedia8 or the
World Trade Web9,10), and study the onset of higher-order structures such as
correlations11,12 and triadic motifs13,14,15,16
************************************************************************
\fi

\paragraph{Algorithms and parameter tuning.} 
We compared the following algorithms:

\begin{enumerate}[label=\textbf{\arabic*.}]
  \item The label propagation algorithm of \autoref{ss:passive} (referred to as \uslpropGsec{}).
    The actual binarizing threshold was set by cross-validation on the training set.

  \item The algorithm analyzed \autoref{ss:bayes_approx}, which we call \usrule{}
    (Bayes Learning Classifier based on \emph{tr}ollness and \emph{un}trustworthiness). After
    computing $\htr(u)$ and $\hun(u)$ on training set $E_0$ for all $u \in V$ (or setting those
    values to $\frac{1}{2}$ in case there is no outgoing or incoming edges for some node), we use
    equation \eqref{eq:predictor} and estimate $\tau$ on $E_0$.

  \item A logistic regression model where each edge $(u,v)$ is associated with the features
    $[1-\htr(u), 1-\hun(v)]$ computed again on \trainset{} (we call this method \uslogregp{}). Best
    binary thresholding is again computed on \trainset{}. Experimenting with this logistic model
    serves to support the claim we made in the introduction that our generative model in
    \autoref{s:gen} is a good fit for the data.

  \item  The solution obtained by directly solving the unregularized problem \eqref{e:quadratic}
    through a fast constrained minimization algorithm (referred to as \qoptim{}). Again, the actual
    binarizing threshold was set by cross-validation on the training set.\footnote{We have also
    tried to minimize \eqref{e:quadratic} by removing the $[-1,+1]$ constraints, but got similar MCC
    results as the ones we report for \qoptim{}}

  \item  The matrix completion method from~\autocite{LowRankCompletion14} based on \complowrank{}
    matrix factorization. Since the authors showed their method to be robust to the choice of the
    rank parameter $k$, we picked $k=7$ in our experiments.

  \item A logistic regression model built on \comptriads{} features derived from status
    theory~\autocite{Leskovec2010}.

  \item The PageRank-inspired algorithm from \autocite{wu2016troll}, where a recursive notion of
    trollness is computed by solving a suitable set of nonlinear equations through an iterative
    method, and then used to assign ranking scores to nodes, from which (un)trustworthiness features
    are finally extracted for each edge. We call this method \compranknodes{}. As for hyperparameter
    tuning ($\beta$ and $\lambda_1$ in~\autocite{wu2016troll}), we closely followed the authors'
    suggestion of doing cross validation.

  \item  The last competitor is the logistic regression model whose features have been build
    according to \autocite{Bayesian15}. We call this method \compbayesian{}.
\end{enumerate}

The above methods can be roughly divided into \emph{local} and \emph{global} methods. A local method
hinges on building local predictive features, based on neighborhoods: \usrule{}, \uslogregp{},
\comptriads{}, and \compbayesian{} essentially fall into this category. The remaining methods
(\uslpropGsec{}, \complowrank{}, and \compranknodes{}) are global in that their features are
designed to depend on global properties of the graph topology.

\iffalse
%Given a training set \trainset{}, we exploit the revealed signs in two ways.
First, with label propagation (referred to as \uslprop{} in the following). We
initialize the labels vector $f_0$ to $|V'|$ random values drawn uniformly in
$[0, 1]$ and set the label of the training edges equal to their observed value.
% TODO: starting with zeros gives lower MCC and accuracy after a fixed number
% of iterations
Let the adjacency matrix of $G'$ be $A$, its diagonal degree matrix
be $D$, its diameter be $\diam{G'}$ and $P$ be the following sparse matrix $P=D^{-1}A$. This allow performing
one round of label propagation as $f_{t+1} = Pf_t$, followed by clamping the
training labels. Each round thus involves $2|E|$ multiplications and we do
$\diam{G'}$ of them\footnote{$\diam{G'}$ is a small constant, ranging from $16$
for \aut{} to $38$ for \epi{}}. With the computed labels, we can associate each
edge $\eij$ with $a_{i,j} = \nicefrac{1}{2}\left(f_{\diam{G'}}(\iout) +
f_{\diam{G'}}(\jin)\right)$.
% This is what I do in the code but now I realize this is just an extra
% propagation round only on the square node so maybe simplify the description
% (actually no, it's needed for ERM)
By sorting the $a$ values of the training edges, we can find the threshold $t$
that minimizes the number of mistakes on the training set while predicting
$\yij = \sgn\left(a_{i,j} - t\right)$ in $O(|\trainset{}|\log |\trainset{}|)$
time.

Then we again associate each edge $\eij$ with $a_{i,j} = \left(1-\htr(i)\right) +
\left(1-\hun(j)\right)$ and find the $\tau$ of \eqref{eq:predictor} that minimizes the empirical
risk.  Another way to exploit those quantities is to train a Logistic Regression model where each
edge is associated with two features: $[1-\htr(i), 1-\hun(j)]$ (we call this method \uslogregp{}).
\fi

\begin{table*}[t]
  \centering
  \setlength{\tabcolsep}{3pt}
  \scriptsize
  \caption{MCC with increasing training set size, with one standard deviation over 12 random
  sampling of $\trainset$. The last four columns refer to the methods we took from the literature.
For the sake of readability, we multiplied all MCC values by 100. The best number in each row is
highlighted in \textbf{\textcolor{brown}{bold brown}} and the second one in
\textit{\textcolor{red}{italic red}}. If the difference is statistically significant ($p$-value of a
paired Student's $t$-test less than $0.005$), the best score is underlined. The ``time" rows contain
the time taken to train on a $15\%$ training set.\label{tab:all_mcc}}
  \begin{tabular}{lrcccc|cccc}
    \toprule
    & $\frac{|\trainset{}|}{|E|}$ &                 \uslpropGsec{} &                       \usrule{} &                    \uslogregp{} &         \qoptim{} &     \complowrank{} &      \comptriads{} &   \compranknodes{} &              \compbayesian{} \\
    \midrule
    \multirow{7}{*}{\rotatebox[origin=c]{90}{\aut{}}} & $5\%$                       & $\vfirstSig{24.54} \pm 0.69$  & $\vsecondSig{20.21} \pm 0.66$ & $20.19 \pm 0.71$              & $15.86 \pm 0.81$ & $12.76 \pm 0.65$ & $11.04 \pm 0.81$ & $17.18 \pm 1.11$             & $15.28 \pm 1.31$ \\
                                                      & $10\%$                      & $\vfirstSig{31.20} \pm 0.58$  & $\vsecondSig{27.54} \pm 0.56$ & $27.49 \pm 0.62$              & $25.36 \pm 0.78$ & $17.81 \pm 0.76$ & $16.99 \pm 0.63$ & $25.36 \pm 0.85$             & $24.74 \pm 0.59$ \\
                                                      & $15\%$ &  $\vfirstSig{35.66} \pm 0.68$  &  $\vsecondSig{32.87} \pm 0.58$  &               $32.79 \pm 0.60$  &  $31.39 \pm 0.75$  &  $22.58 \pm 0.53$  &  $21.55 \pm 0.91$  &  $30.60 \pm 0.87$  &            $31.71 \pm 0.99$  \\
                                                      & $20\%$ &  $\vfirstSig{38.67} \pm 0.48$  &  $\vsecondSig{36.94} \pm 0.51$  &               $36.86 \pm 0.48$  &  $35.47 \pm 0.41$  &  $25.80 \pm 0.94$  &  $24.27 \pm 0.56$  &  $35.01 \pm 0.83$  &            $36.13 \pm 0.75$  \\
                                                      & $25\%$ &     $\vfirst{41.05} \pm 0.73$  &               $39.83 \pm 0.58$  &               $39.76 \pm 0.59$  &  $38.48 \pm 0.55$  &  $29.67 \pm 0.78$  &  $26.85 \pm 0.87$  &  $38.06 \pm 0.86$  &  $\vsecond{40.34} \pm 0.94$  \\
                                                      & time   &                           19.6 &                             0.6 &                             2.6 &               2835 &               3279 &                6.2 &                155 &                         4813 \\
    \midrule
    \multirow{7}{*}{\rotatebox[origin=c]{90}{\wik{}}} & $5\%$                       & $\vfirstSig{39.46} \pm 0.79$  & $38.03 \pm 0.97$              & $\vsecondSig{38.50} \pm 0.87$ & $35.72 \pm 0.70$ & $24.58 \pm 1.18$ & $9.59 \pm 1.10$  & $33.60 \pm 0.64$             & $26.45 \pm 0.57$ \\
                                                      & $10\%$                      & $\vsecond{47.17} \pm 0.35$    & $46.03 \pm 0.49$              & $\vfirst{47.22} \pm 0.40$     & $44.53 \pm 0.48$ & $31.72 \pm 0.61$ & $26.36 \pm 0.83$ & $43.21 \pm 0.81$             & $40.28 \pm 0.69$ \\
                                                      & $15\%$ &     $\vsecond{50.49} \pm 0.33$  &  $49.89 \pm 0.40$  &      $\vfirst{50.87} \pm 0.36$  &  $49.08 \pm 0.33$  &  $35.77 \pm 0.58$  &  $33.64 \pm 0.83$  &  $48.50 \pm 0.47$  &  $47.07 \pm 0.38$  \\
                                                      & $20\%$ &  $\vsecondSig{52.74} \pm 0.31$  &  $52.24 \pm 0.49$  &   $\vfirstSig{53.13} \pm 0.27$  &  $51.79 \pm 0.35$  &  $37.90 \pm 0.27$  &  $38.41 \pm 0.53$  &  $51.49 \pm 0.43$  &  $50.54 \pm 0.39$  \\
                                                      & $25\%$ &     $\vsecond{54.00} \pm 0.63$  &  $53.42 \pm 0.59$  &      $\vfirst{54.26} \pm 0.37$  &  $53.31 \pm 0.37$  &  $40.16 \pm 0.57$  &  $41.34 \pm 1.07$  &  $53.30 \pm 0.37$  &  $52.92 \pm 0.48$  \\
                                                      & time   &                            41.9 &                1.6 &                             6.0 &              10629 &               8523 &               14.8 &                249 &              12507 \\
    \midrule
    \multirow{7}{*}{\rotatebox[origin=c]{90}{\sla{}}} & $5\%$                       & $\vsecondSig{40.77} \pm 0.20$ & $36.13 \pm 0.57$              & $37.00 \pm 0.29$              & $33.49 \pm 0.32$ & $36.83 \pm 0.47$ & $27.10 \pm 0.75$ & $\vfirstSig{45.16} \pm 0.59$ & $29.25 \pm 0.23$ \\
                                                      & $10\%$                      & $\vsecondSig{46.61} \pm 0.29$ & $41.89 \pm 0.39$              & $43.15 \pm 0.21$              & $40.92 \pm 0.23$ & $39.57 \pm 0.27$ & $40.38 \pm 1.47$ & $\vfirstSig{47.84} \pm 0.50$ & $38.25 \pm 0.21$ \\
                                                      & $15\%$ &   $\vfirstSig{49.62} \pm 0.22$  &  $45.42 \pm 0.36$  &  $46.42 \pm 0.16$  &  $45.56 \pm 0.19$  &  $41.21 \pm 0.19$  &  $45.88 \pm 1.01$  &  $\vsecondSig{48.75} \pm 0.71$  &  $43.47 \pm 0.16$  \\
                                                      & $20\%$ &     $\vsecond{51.88} \pm 0.24$  &  $47.78 \pm 0.25$  &  $48.66 \pm 0.10$  &  $48.10 \pm 0.30$  &  $42.74 \pm 0.44$  &  $48.79 \pm 0.57$  &      $\vfirst{52.10} \pm 0.33$  &  $46.89 \pm 0.27$  \\
                                                      & $25\%$ &     $\vsecond{53.12} \pm 0.20$  &  $49.39 \pm 0.24$  &  $50.22 \pm 0.12$  &  $50.11 \pm 0.20$  &  $44.24 \pm 0.44$  &  $50.62 \pm 0.53$  &      $\vfirst{53.29} \pm 0.22$  &  $49.42 \pm 0.22$  \\
                                                      & time   &                             677 &                8.3 &               32.8 &              78537 &              69988 &                131 &                            2441 &              68085 \\
    \midrule
    \multirow{7}{*}{\rotatebox[origin=c]{90}{\epi{}}} & $5\%$                       & $\vsecondSig{54.83} \pm 0.16$ & $46.94 \pm 0.80$              & $49.16 \pm 0.32$              & $42.79 \pm 0.34$ & $39.96 \pm 0.60$ & $42.94 \pm 2.06$ & $\vfirstSig{56.04} \pm 0.76$ & $37.99 \pm 0.49$ \\
                                                      & $10\%$                      & $\vsecondSig{58.94} \pm 0.27$ & $54.03 \pm 0.46$              & $55.90 \pm 0.13$              & $53.43 \pm 0.39$ & $44.50 \pm 0.52$ & $50.29 \pm 1.07$ & $\vfirstSig{60.60} \pm 0.32$ & $49.90 \pm 0.36$ \\
                                                      & $15\%$ &  $\vsecondSig{61.47} \pm 0.21$  &  $57.63 \pm 0.45$  &  $59.25 \pm 0.17$  &  $58.80 \pm 0.32$  &  $48.24 \pm 0.58$  &  $54.64 \pm 1.62$  &  $\vfirstSig{62.69} \pm 0.21$  &            $56.94 \pm 0.65$  \\
                                                      & $20\%$ &  $\vsecondSig{63.17} \pm 0.13$  &  $60.15 \pm 0.40$  &  $61.45 \pm 0.17$  &  $61.86 \pm 0.13$  &  $52.21 \pm 0.37$  &  $57.27 \pm 1.42$  &  $\vfirstSig{64.10} \pm 0.12$  &            $61.18 \pm 0.45$  \\
                                                      & $25\%$ &               $64.05 \pm 0.20$  &  $61.88 \pm 0.38$  &  $62.89 \pm 0.12$  &  $63.42 \pm 0.14$  &  $54.68 \pm 0.62$  &  $58.42 \pm 1.59$  &     $\vfirst{65.40} \pm 0.85$  &  $\vsecond{64.59} \pm 0.30$  \\
                                                      & time   &                            1329 &               10.1 &               54.0 &             143881 &             127654 &                209 &                           3174 &                       104305 \\
    \midrule
    \multirow{7}{*}{\rotatebox[origin=c]{90}{\kiw{}}} & $5\%$                       & $\vfirstSig{36.36} \pm 0.53$  & $\vsecondSig{30.89} \pm 0.28$ & $30.81 \pm 0.20$              & $21.69 \pm 0.25$ & $23.15 \pm 0.26$ & $3.04 \pm 0.46$  & $26.63 \pm 0.44$             & $26.68 \pm 0.34$ \\
                                                      & $10\%$                      & $\vfirstSig{38.58} \pm 0.74$  & $35.68 \pm 0.22$              & $\vsecondSig{35.93} \pm 0.16$ & $29.75 \pm 0.21$ & $27.07 \pm 0.44$ & $12.34 \pm 0.79$ & $33.85 \pm 0.33$             & $35.00 \pm 0.34$ \\
                                                      & $15\%$ &  $\vsecondSig{39.08} \pm 0.55$  &               $37.77 \pm 0.22$  &               $38.27 \pm 0.19$  &  $33.61 \pm 0.11$  &  $30.05 \pm 0.29$  &  $17.95 \pm 0.92$  &  $36.88 \pm 0.32$  &  $\vfirstSig{40.00} \pm 0.26$  \\
                                                      & $20\%$ &               $39.04 \pm 0.69$  &               $38.88 \pm 0.36$  &  $\vsecondSig{39.55} \pm 0.11$  &  $35.04 \pm 0.17$  &  $32.17 \pm 0.31$  &  $21.44 \pm 0.67$  &  $38.60 \pm 0.31$  &  $\vfirstSig{43.32} \pm 0.22$  \\
                                                      & $25\%$ &               $38.90 \pm 0.45$  &               $39.41 \pm 0.16$  &  $\vsecondSig{40.44} \pm 0.14$  &  $36.18 \pm 0.20$  &  $33.94 \pm 0.74$  &  $23.41 \pm 0.41$  &  $39.75 \pm 0.32$  &  $\vfirstSig{45.76} \pm 0.29$  \\
                                                      & time   &                             927 &                             9.6 &                            46.8 &             219109 &             129460 &                177 &               3890 &                          92719 \\
    \bottomrule
  \end{tabular}
\end{table*}

\paragraph{Results}
Our main results are summarized in \autoref{tab:all_mcc}, reporting MCC test set performance after
training on sets of varying size (from 5\% to 25\%). Results have been averaged over 12 repetitions.
Because scalability is a major concern when training on sizeable datasets, we also give an idea of
relative training times (in milliseconds) by reporting the time it took to train a single run of
each algorithm on a training set of size\footnote{Comparison of training time performances is fair
since all algorithms have been carefully implemented using the same stack of Python libraries, and
run on the same machine (16 Xeon cores and 192Gb Ram).} 15\% of $|E|$, and then predict on the test
set. Though our experiments are not conclusive, some trends can be readily spotted:

\begin{enumerate}[leftmargin=2em,label=\textbf{\arabic*.}]

  \item Global methods tend to outperform local methods in terms of prediction performance, but are
    also significantly (or even much) slower (running times can differ by as much as three orders of
    magnitude). This is not surprising, and is in line with previous experimental findings (e.g.,
    \autocites{shahriari2014ranking}{wu2016troll}). \compbayesian{} looks like an exception to this
    rule, but its running time is indeed in the same ballpark as global methods.

  \item \uslpropGsec{} always ranks first or at least second in this comparison when MCC is
    considered. On top of it, \uslpropGsec{} is fastest among the global methods (one or even two
    orders of magnitude faster), thereby showing the benefit of our approach to edge sign
    prediction.

  \item The regularized solution computed by \uslpropGsec{} is always better than the unregularized
    one computed by \qoptim{} in terms of both MCC and running time.

  \item As claimed in the introduction, our Bayes approximator \usrule{} closely mirrors in
    performance the more involved \uslogregp{} model. In fact, supporting our generative model of
    \autoref{s:gen}, the logistic regression weights for features $1-\htr(i)$ and $1-\hun(j)$
    are almost equal (see Table~2 in the supplementary material), thereby suggesting that
    predictor~\eqref{eq:predictor}, derived from the theoretical results in
    \autoref{ss:bayes_approx}, is \emph{also} the best logistic model based on trollness and
    untrustworthiness.
\end{enumerate}
