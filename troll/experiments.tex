\section{Experimental Analysis}\label{s:exp}

We now evaluate our edge sign classification methods on representative real-world datasets of
varying density and label regularity. This shows that our methods compete well against
existing approaches in terms of both predictive and computational performance. We are especially
interested in small training set regimes, and have restricted our comparison to the batch learning
scenario since all competing methods we are aware of have been developed in that setting only.

\paragraph{Datasets}

We consider six real-world classification datasets. The first four are \dssn{} widely
used as benchmarks for this
task~(\eg{}\autocites{Leskovec2010}{shahriari2014ranking}{wu2016troll}{OnlineCompletion17}):
\begin{newcontent}
In \adv{}, a trust-based social network for open source developers, a user $u$ can certify another
user $v$ with different degrees of trust: \enquote{Observer}, \enquote{Apprentice} (both of which we
consider negative), \enquote{Journeyer} and \enquote{Master} (both of which we consider
positive).\footnote{We download the 7\thup{} of July, 2014 version from
\url{http://www.trustlet.org/datasets/advogato/}.} A full description of this trust metric and its
resistance to attacks are available in the PhD thesis of the website's creator~\autocite[Section
4]{AdvogatoTrustThesis02}).
\end{newcontent}
In \wik{}, there is an edge from user $u$ to user $v$ if $v$ applies for an admin position and $u$
votes for or against that promotion. In \sla{}, a news sharing and commenting website, member $u$
can tag other members $v$ as friends or foes. Finally, in \epi{}, an online shopping website, user
$v$ reviews products and, based on these reviews, another user $u$ can display whether he considers
$v$ to be reliable or not. In addition to these four datasets, we considered two other \ssn{} where
the signs are inferred automatically, rather than given explicitly by the users.  In
\kiw{}~\cite{wikiedits11}, an edge from Wikipedia user $u$ to user $v$ indicates whether they edited
the same article in a constructive manner or not.\footnote{This is the
\href{http://konect.uni-koblenz.de/networks/wikisigned-k2}{KONECT version of the
\enquote{Wikisigned} dataset}, from which we removed self-loops.} Finally, in the
\aut{}~\cite{kumar2016structure} network, an author $u$ cites another author $v$ by either endorsing
or criticizing $v$'s work. The edge sign is derived by classifying the citation sentiment with a
simple, yet powerful, keyword-based technique using a list of positive and negative words. See
\cite{kumar2016structure} for more details.\footnote{We again removed self-loops and merged
multi-edges which are all of the same sign.}

\begin{table*}[bt]
  \centering
  \small
  \caption{Dataset properties. The 5\thup{} column gives the fraction of positive labels. The next two
  columns provide two different measures of label regularity, while the last two columns give the
  proportion of reciprocal edges, and among them the fraction with different signs.\label{tab:dataset}}
  \begin{tabular}{lrrrrrrrr}
    \toprule
    Dataset & $|V|$       & $|E|$       & $\frac{|E|}{|V|}$ & $\frac{|E^+|}{|E|}$ & $\frac{\Psi^2_{G''}(Y)}{|E|}$ & $\frac{\Psi_G(Y)}{|E|}$ & twin edges & twin disagreement \\
    \midrule                                                                                                   
    \aut{}  & \np{4831}   & \np{39452}  & 8.2               & 72.3\%              & .076                          & .191                    & 5.1\%      & 27.1\%            \\
    \adv{}  & \np{5417}   & \np{51312}  & 9.5               & 75.1\%              & .061                          & .132                    & 33.6\%     & 28.6\%            \\
    \wik{}  & \np{7114}   & \np{103108} & 14.5              & 78.8\%              & .063                          & .142                    & 5.6\%      & 10.0\%            \\
    \sla{}  & \np{82140}  & \np{549202} & 6.7               & 77.4\%              & .059                          & .143                    & 17.7\%     & 4.0\%             \\
    \epi{}  & \np{131580} & \np{840799} & 6.4               & 85.3\%              & .031                          & .074                    & 30.8\%     & 2.1\%             \\
    \kiw{}  & \np{138587} & \np{740106} & 5.3               & 87.9\%              & .034                          & .086                    & 6.5\%      & 14.6\%            \\
    \bottomrule
  \end{tabular}
\end{table*}

\autoref{tab:dataset} summarizes statistics for these datasets. We note that most edge labels are
positive. Hence, test set accuracy is not an appropriate measure of prediction performance. We
instead evaluated our performance using the so-called Matthews Correlation Coefficient
(MCC)~\autocite{MCC00}), defined as
\[
	\mathrm{MCC} = \frac{tp\times tn-fp\times fn}%
        {\sqrt{ (tp + fp) ( tp + fn ) ( tn + fp ) ( tn + fn ) } } = \pm \sqrt{\frac{\chi^2}{n}}
\]
MCC combines all the four quantities found in a binary confusion matrix ($t$rue $p$ositive, $t$rue
$n$egative, $f$alse $p$ositive and $f$alse $n$egative) into a single metric which ranges from $-1$
(when all predictions are incorrect) to $+1$ (when all predictions are correct) through $0$ (when
predictions are made uniformly at random).

Although the semantics of the edge signs is not the same across these networks, we can see from
\autoref{tab:dataset} that our generative model essentially fits all of them. Specifically, the last
two columns of the table report the rate of label (ir)regularity, as measured by
$\Psi^2_{G''}(Y)/|E|$ (second-last column) and $\Psi_{G}(Y)/|E|$ (last column), where 
\[
\Psi^2_{G''}(Y) = \min_{(\bp,\bq)} \left(f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)\right)\,,
\]
$f_{E_0}$ and $f_{E\setminus E_0}$ being the quadratic criterions of \autoref{ss:passive}, viewed as
functions of both $(\bp,\bq)$, and $y_{i,j}$ when all labels are known, and $\Psi_{G}(Y)$ is the
label regularity measure adopted in the online setting, as defined in \autoref{s:prel}. It is
reasonable to expect that higher label irregularity corresponds to lower prediction performance.
This trend is in fact confirmed by our experimental findings: whereas \epi{} tends to be easy,
\aut{} tends to be hard, and this holds for all algorithms we tested, even if they do not explicitly
comply with our inductive bias principles. Moreover, $\Psi^2_{G''}(Y)/|E|$ tends to be proportional
to $\Psi_{G}(Y)/|E|$ across datasets, hence confirming the anticipated connection between the two
regularity measures.

\begin{newcontent}
Finally, there is a low fraction of reciprocal (or \emph{twin}) edges (\ie{} both $\euv \in E$ and $\evu
\in E$), which is a common mechanism of link formation in directed
networks~\autocites{DirectedReciprocity04}{Reciprocity13}). Moreover, in most cases, such twin edges do not disagree,
\ie{} they have the same sign. In practice, we can use this fact to improve our accuracy at no
additional computational cost: when predicting $\euv \in E_{\mathrm{test}}$, if the reciprocal edge
\evu{} is part of the training set, we set $\yhat_{u,v} = y_{v,u}$. For clarity, when comparing
our methods with existing approaches, we not use that trick. But afterwards, we show in
\autoref{tab:twin} when it can be beneficial.
% more references in  http://www.nature.com/articles/srep02729
\end{newcontent}

\paragraph{Algorithms and parameter tuning.} 
We compared the following algorithms:

\begin{enumerate}[label=\textbf{\arabic*.}]
  \item The label propagation algorithm of \autoref{ss:passive} (referred to as \uslpropGsec{}).
    The actual binarizing threshold was set by cross-validation on the training set.

  \item The algorithm analyzed \autoref{ss:bayes_approx}, which we call \usrule{}
    (Bayes Learning Classifier based on \emph{tr}ollness and \emph{un}trustworthiness). After
    computing $\htr(u)$ and $\hun(u)$ on training set $E_0$ for all $u \in V$ (or setting those
    values to $\frac{1}{2}$ in case there is no outgoing or incoming edges for some node), we use
    equation \eqref{eq:predictor} and estimate $\tau$ on $E_0$.

  \item A logistic regression model where each edge $(u,v)$ is associated with the features
    $[1-\htr(u), 1-\hun(v)]$ computed again on \trainset{} (we call this method \uslogregp{}). Best
    binary thresholding is again computed on \trainset{}. Experimenting with this logistic model
    serves to support the claim we made in the introduction that our generative model in
    \autoref{s:gen} is a good fit for the data.

  \item  The solution obtained by directly solving the unregularized problem \eqref{e:quadratic}
    through a fast constrained minimization algorithm (referred to as \qoptim{}). Again, the actual
    binarizing threshold was set by cross-validation on the training set.\footnote{We have also
    tried to minimize \eqref{e:quadratic} by removing the $[-1,+1]$ constraints, but got similar MCC
    results as the ones we report for \qoptim{}}

  \item  The matrix completion method from~\autocite{LowRankCompletion14} based on \complowrank{}
    matrix factorization. Since the authors showed their method to be robust to the choice of the
    rank parameter $k$, we picked $k=7$ in our experiments.

  \item A logistic regression model built on \comptriads{} features derived from status
    theory~\autocite{Leskovec2010}.

  \item The PageRank-inspired algorithm from \autocite{wu2016troll}, where a recursive notion of
    trollness is computed by solving a suitable set of nonlinear equations through an iterative
    method, and then used to assign ranking scores to nodes, from which (un)trustworthiness features
    are finally extracted for each edge. We call this method \compranknodes{}. As for hyperparameter
    tuning ($\beta$ and $\lambda_1$ in~\autocite{wu2016troll}), we closely followed the authors'
    suggestion of doing cross validation.

  \item  The last competitor is the logistic regression model whose features have been build
    according to \autocite{Bayesian15}. We call this method \compbayesian{}.
\end{enumerate}

The above methods can be roughly divided into \emph{local} and \emph{global} methods. A local method
hinges on building local predictive features, based on neighborhoods: \usrule{}, \uslogregp{},
\comptriads{}, and \compbayesian{} essentially fall into this category. The remaining methods
(\uslpropGsec{}, \complowrank{}, and \compranknodes{}) are global in that their features are
designed to depend on global properties of the graph topology.

\iffalse
%Given a training set \trainset{}, we exploit the revealed signs in two ways.
First, with label propagation (referred to as \uslprop{} in the following). We
initialize the labels vector $f_0$ to $|V'|$ random values drawn uniformly in
$[0, 1]$ and set the label of the training edges equal to their observed value.
% TODO: starting with zeros gives lower MCC and accuracy after a fixed number
% of iterations
Let the adjacency matrix of $G'$ be $A$, its diagonal degree matrix
be $D$, its diameter be $\diam{G'}$ and $P$ be the following sparse matrix $P=D^{-1}A$. This allow performing
one round of label propagation as $f_{t+1} = Pf_t$, followed by clamping the
training labels. Each round thus involves $2|E|$ multiplications and we do
$\diam{G'}$ of them\footnote{$\diam{G'}$ is a small constant, ranging from $16$
for \aut{} to $38$ for \epi{}}. With the computed labels, we can associate each
edge $\eij$ with $a_{i,j} = \nicefrac{1}{2}\left(f_{\diam{G'}}(\iout) +
f_{\diam{G'}}(\jin)\right)$.
% This is what I do in the code but now I realize this is just an extra
% propagation round only on the square node so maybe simplify the description
% (actually no, it's needed for ERM)
By sorting the $a$ values of the training edges, we can find the threshold $t$
that minimizes the number of mistakes on the training set while predicting
$\yij = \sgn\left(a_{i,j} - t\right)$ in $O(|\trainset{}|\log |\trainset{}|)$
time.

Then we again associate each edge $\eij$ with $a_{i,j} = \left(1-\htr(i)\right) +
\left(1-\hun(j)\right)$ and find the $\tau$ of \eqref{eq:predictor} that minimizes the empirical
risk.  Another way to exploit those quantities is to train a Logistic Regression model where each
edge is associated with two features: $[1-\htr(i), 1-\hun(j)]$ (we call this method \uslogregp{}).
\fi


\paragraph{Results}
Our main results are summarized in \autoref{tab:all_mcc}, reporting MCC test set performance after
training on sets of varying size (from 5\% to 25\%, plus 50\% and 90\%). Results have been averaged
over 12 repetitions.
Because scalability is a major concern when training on sizeable datasets, we also give an idea of
relative training times (in milliseconds) by reporting the time it took to train a single run of
each algorithm on a training set of size\footnote{Comparison of training time performances is fair
since all algorithms have been carefully implemented using the same stack of Python libraries, and
run on the same machine (16 Xeon cores and 192Gb Ram).} 15\% of $|E|$, and then predict on the test
set. Though our experiments are not conclusive, some trends can be readily spotted:

\begin{enumerate}[leftmargin=2em,label=\textbf{\arabic*.}]

  \item Global methods tend to outperform local methods in terms of prediction performance, but are
    also significantly (or even much) slower (running times can differ by as much as three orders of
    magnitude). This is not surprising, and is in line with previous experimental findings (e.g.,
    \autocites{shahriari2014ranking}{wu2016troll}). \compbayesian{} looks like an exception to this
    rule, but its running time is indeed in the same ballpark as global methods.

  \item \uslpropGsec{} always ranks first or at least second in this comparison when MCC is
    considered. On top of it, \uslpropGsec{} is fastest among the global methods (one or even two
    orders of magnitude faster), thereby showing the benefit of our approach to edge sign
    prediction.

  \item The regularized solution computed by \uslpropGsec{} is always better than the unregularized
    one computed by \qoptim{} in terms of both MCC and running time.

  \item As claimed in the introduction, our Bayes approximator \usrule{} closely mirrors in
    performance the more involved \uslogregp{} model. In fact, supporting our generative model of
    \autoref{s:gen}, the logistic regression weights for features $1-\htr(i)$ and $1-\hun(j)$
    are almost equal (see \autoref{tab:coeff} in the supplementary material), thereby suggesting that
    predictor~\eqref{eq:predictor}, derived from the theoretical results in
    \autoref{ss:bayes_approx}, is \emph{also} the best logistic model based on trollness and
    untrustworthiness.
\end{enumerate}

\input{all_mcc}

\begin{newcontent}
We perform two other sets of experiments. The first one assesses the effect of predicting reciprocal
edges by their value if available. As expected, the results in \autoref{tab:twin} demonstrate that
it improves MCC when the network has enough twin edges with low disagreements, like in \sla{} and
\epi{}. It has no effect when there are few twin edges, like in \aut{}, \wik{} and \kiw{}. And it is
detrimental when there are many twin edges that do not agree enough, like in \adv{}, in which
case it is better to rely solely on the learned model.

\input{diff_mcc_pp}

In the second set of experiments, we study the effect of our hypothesis that the labeled edges of the training set are
chosen \uar{}. In two of our datasets (\wik{} and \epi{}), edges come with the timestamp at which they
were created. A more realistic way of choosing the training set is therefore, for a given training
size $m$, to let $E_0$ be the $m$ oldest edges, and try to predict the remaining, newest ones. This
is a common experimental setting in link prediction, where the goal is to infer which pair of nodes
will be connected in the future~\autocite{linkPredSurvey16}. However, as showed in
\autoref{tab:troll_ts_mcc}, this makes the problem much more challenging, both for our methods and
out competitors. It is quite surprising that even when the training set is as large as 90\% of all
edges, the MCC are so low compared with those reported in \autoref{tab:all_mcc}.

\input{ts_mcc}

The explanation of
why these two sampling strategies produce so different results can be inferred from
\autoref{tab:troll_early_sampling}. Letting $V_{\mathrm{out}}$ be the subset of node of $V$ with at
least one outgoing edge, and defining similarly $V_{\mathrm{in}}$, we see that the node coverage is
naturally larger when sampling at random. Yet, when the training set reaches 90\% size, both
sampling strategies cover roughly 90\% of the nodes in $V_{\mathrm{out}}$ and in
$V_{\mathrm{out}}$. The difference in MCC is thus explained by the last three columns. It shows, for
testing edges \euv{}, the breakdown between three situations, from the least to the most
informative:
\begin{enumerate}[(i),nosep]
  \item no sampled edge outgoing from $u$ nor incoming to $v$;
  \item some sampled edges, but only either outgoing from $u$ or incoming to $v$; and
  \item some sampled edges, both outgoing from $u$ and incoming to $v$;
\end{enumerate}
When the training set is build \uar{}, the vast majority of testing edges falls into the case (iii).
This is no longer true with temporal training set, where situations (ii) and even (i) are more
common, making prediction more difficult. The difference of repartition is particularly strong in
\wik{}, justifying the better performance in \epi{}.

\input{sampling}

\end{newcontent}
