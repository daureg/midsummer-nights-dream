\section{Experimental Analysis}\label{s:exp}

We now evaluate our \esp{} methods on representative real-world datasets of
varying density and label regularity.
After presenting the data and our evaluation criterion, we proceed in two steps. First, we simulate
our generative model on real networks to give them signs, then we study to which extent we can recover
the parameters $p$ and $q$ of each node, and how predictions based on these estimation compare with
the Bayes optimal. Second, we select random training sets from the actual signs.
This shows that our methods compete well against
existing approaches in terms of both predictive and computational performance. We are especially
interested in small training set regimes, and have restricted our comparison to the batch learning
scenario since all competing methods we are aware of have been developed in that setting only.

\subsection{Datasets}

We consider six real-world classification datasets. The first four are \dssn{} widely
used as benchmarks for this
task~(\eg{}\autocites{Leskovec2010}{shahriari2014ranking}{wu2016troll}{OnlineCompletion17}):
\begin{newcontent}
In \adv{}, a trust-based social network for open source developers, a user $u$ can certify another
user $v$ with different degrees of trust: \enquote{Observer}, \enquote{Apprentice} (both of which we
consider negative), \enquote{Journeyer} and \enquote{Master} (both of which we consider
positive).\footnote{We download the 7\thup{} of July, 2014 version from
\url{http://www.trustlet.org/datasets/advogato/}.} A full description of this trust metric, and its
resistance to attacks, is available in the PhD thesis of the website's creator~\autocite[Section
4]{AdvogatoTrustThesis02}).
\end{newcontent}
In \wik{}, there is an edge from user $u$ to user $v$ if $v$ applies for an admin position and $u$
votes for or against that promotion. In \sla{}, a news sharing and commenting website, member $u$
can tag other members $v$ as friends or foes. Finally, in \epi{}, an online shopping website, user
$v$ reviews products and, based on these reviews, another user $u$ can display whether he considers
$v$ to be reliable or not. In addition to these four datasets, we considered two other \ssn{} where
the signs are inferred automatically, rather than given explicitly by the users.  In
\kiw{}~\cite{wikiedits11}, an edge from Wikipedia user $u$ to user $v$ indicates whether they edited
the same article in a constructive manner or not.\footnote{This is the
\href{http://konect.uni-koblenz.de/networks/wikisigned-k2}{KONECT version of the
\enquote{Wikisigned} dataset}, from which we removed self-loops.} Finally, in the
\aut{}~\cite{kumar2016structure} network, an author $u$ cites another author $v$ by either endorsing
or criticizing $v$'s work. The edge sign is derived by classifying the citation sentiment with a
technique using a list of positive and negative words; see
\cite{kumar2016structure} for more details.\footnote{We again removed self-loops and merged
multi-edges which are all of the same sign.}

\begin{table}[bt]
  \centering
  \small
  \caption{Dataset properties. The 5\thup{} column gives the fraction of positive labels. The next two
  columns provide two different measures of label regularity, while the last two columns give the
  proportion of reciprocal edges, and among them the fraction with different signs.\label{tab:dataset}}
  \begin{tabular}{lrrrrrrrr}
    \toprule
    Dataset & \thead{$|V|$}       & \thead{$|E|$}       & \thead{$\frac{|E|}{|V|}$} &
    \thead{$\frac{|E^+|}{|E|}$} & \thead{$\frac{\Psi^2_{G''}(Y)}{|E|}$} &
    \thead{$\frac{\Psi_G(Y)}{|E|}$} & \thead{reciprocal\\ edges} &
    \thead{reciprocal\\ disagreement} \\
    \midrule                                                                                                   
    \aut{}  & \np{4831}   & \np{39452}  & 8.2               & 72.3\%              & .076                          & .191                    & 5.1\%      & 27.1\%            \\
    \adv{}  & \np{5417}   & \np{51312}  & 9.5               & 75.1\%              & .061                          & .132                    & 33.6\%     & 28.6\%            \\
    \wik{}  & \np{7114}   & \np{103108} & 14.5              & 78.8\%              & .063                          & .142                    & 5.6\%      & 10.0\%            \\
    \sla{}  & \np{82140}  & \np{549202} & 6.7               & 77.4\%              & .059                          & .143                    & 17.7\%     & 4.0\%             \\
    \epi{}  & \np{131580} & \np{840799} & 6.4               & 85.3\%              & .031                          & .074                    & 30.8\%     & 2.1\%             \\
    \kiw{}  & \np{138587} & \np{740106} & 5.3               & 87.9\%              & .034                          & .086                    & 6.5\%      & 14.6\%            \\
    \bottomrule
  \end{tabular}
\end{table}

\autoref{tab:dataset} summarizes statistics for these datasets. We note that most edge labels are
positive. Hence, test set accuracy is not an appropriate measure of prediction performance. We
instead evaluated our performance using the so-called Matthews Correlation Coefficient
(MCC)~\autocite{MCC00}, defined as
\[
	\mathrm{MCC} = \frac{tp\times tn-fp\times fn}%
        {\sqrt{ (tp + fp) ( tp + fn ) ( tn + fp ) ( tn + fn ) } }% = \pm \sqrt{\frac{\chi^2}{n}}
\]
MCC combines all the four quantities found in a binary confusion matrix ($t$rue $p$ositive, $t$rue
$n$egative, $f$alse $p$ositive and $f$alse $n$egative) into a single metric which ranges from $-1$
(when all predictions are incorrect) to $+1$ (when all predictions are correct) through $0$ (when
predictions are made uniformly at random).

Although the semantics of the edge signs is not the same across these networks, we can see from
\autoref{tab:dataset} that our generative model essentially fits all of them. Specifically,
two columns of the table report the rate of label (ir)regularity, as measured by
$\Psi^2_{G''}(Y)/|E|$ (6\thup{} column) and $\Psi_{G}(Y)/|E|$ (7\thup{} column), where 
\[
\Psi^2_{G''}(Y) = \min_{(\bp,\bq)} \left(f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)\right)\,,
\]
$f_{E_0}$ and $f_{E\setminus E_0}$ being the quadratic criterions of \autoref{ss:passive}, viewed as
functions of both $(\bp,\bq)$, and \yuv{} when all labels are known, and $\Psi_{G}(Y)$ is the
label regularity measure adopted in the online setting, as defined in \autoref{s:prel}. It is
reasonable to expect that higher label irregularity corresponds to lower prediction performance.
This trend is in fact confirmed by our experimental findings: whereas \epi{} tends to be easy,
\aut{} tends to be hard, and this holds for all algorithms we tested, even if they do not explicitly
comply with our inductive bias principles. Moreover, $\Psi^2_{G''}(Y)/|E|$ tends to be proportional
to $\Psi_{G}(Y)/|E|$ across datasets, hence confirming the anticipated connection between the two
regularity measures.

\begin{newcontent}
Finally, there is a low fraction of reciprocal edges (\ie{} both $\euv \in E$ and $\evu
\in E$), which is a common mechanism of link formation in directed
networks~\autocites{DirectedReciprocity04}{Reciprocity13}). Moreover, in most cases, such reciprocal edges do not disagree,
\ie{} they have the same sign. In practice, we can use this fact to improve our accuracy at no
additional computational cost: when predicting $\euv \in E_{\mathrm{test}}$, if the reciprocal edge
\evu{} is part of the training set, we set $\yhat_{u,v} = y_{v,u}$. For clarity, when comparing
our methods with existing approaches, we not use that trick. But afterwards, we show in
\autoref{tab:twin} when it can be beneficial.
% more references in  http://www.nature.com/articles/srep02729
\end{newcontent}

\begin{newcontent}
\subsection{Synthetic signs}

Recall that according to our generative model of \autoref{s:gen}, each node $u$ has a parameter $p_u$
governing its sending behavior and each node $v$ has a parameter $q_v$ governing its receiving
behavior, such that the sign of the edge \euv{} is positive with probability $\frac{p_u+q_v}{2} =
\etauv$. Given a topology $G=(V,E)$, we start by assigning a $p$ and $q$ values to each node. If we
want \etauv{} to be uniform over $E$, we have to take into account the out-degree of nodes with at
least one outgoing edge, and likewise the in-degree of nodes with at least one incoming edge. For
that, in the case of $p$, we partition the interval $[0,1]$ into $|V|$ segments of size proportional
to $\dout(u_1),
\dout(u_2), \ldots, \dout(u_{|V|})$. We shuffle these segments and draw a number \uar{} from each of
them, which we set as the $p$ value of the corresponding node. Then, because we want to model real
sign distribution and have more
positive edges, we apply an exponential transformation \[ p \rightarrow \frac{1}{1-e^{-\lambda}}
\left( 1-e^{-\lambda p} \right) \,,\] where we choose $\lambda=3$. We do the same for $q$, then for
every edge \euv{}, we set it positive with probability \etauv{}. An example of the distributions we
obtain for the \wik{} graph is showed on \autoref{fig:troll_synth_distrib}, giving $71.9\%$ of
positive edges.

\vspace{\baselineskip}
\noindent\begin{minipage}{187mm} 
  \begin{minipage}{46mm}
    \centering
    \includegraphics[width=.95\linewidth]{raw/synth_wik_pdense.pdf}
    \captionof{subfigure}{Density of $p$.}
    % \label{fig:figure1}
  \end{minipage}%
  \begin{minipage}{46mm}
    \centering 
    \includegraphics[width=.95\linewidth]{raw/synth_wik_qdense.pdf} 
    \captionof{subfigure}{Density of $q$.} 
    % \label{fig:figure2} 
  \end{minipage} 
  \begin{minipage}{46mm}
    \centering 
    \includegraphics[width=.95\linewidth]{raw/synth_wik_etadense.pdf} 
    \captionof{subfigure}{Density of $\eta$.} 
    % \label{fig:figure2} 
  \end{minipage} 
  \begin{minipage}{46mm}
    \centering 
    \includegraphics[width=.95\linewidth]{raw/synth_wik_signdense.pdf}
    \captionof{subfigure}{Density of $\eta$ for both signs.} 
    % \label{fig:figure2} 
  \end{minipage} 
  \captionof{figure}{Synthetic distributions on \wik{}.} 
  \label{fig:troll_synth_distrib} 
\end{minipage}
\vspace{\baselineskip}

Once the signs are generated, we select a training set \trainset{} \uar{}, and predict
the signs of the remaining edges in the testing set $E\setminus \trainset{}$. The Bayes optimal
predictor classifies an edge \euv{} as positive if $\etauv \geq \shalf$ and negative otherwise. 
We compare it with the algorithm analyzed in \autoref{ss:bayes_approx}, which we call \usrule{}
(Bayes Learning Classifier based on \emph{tr}ollness and \emph{un}trustworthiness). In practice,
\usrule{} proceeds as follow: after computing $\htr(u)$ and $\hun(u)$ on training set $E_0$ for all
$u \in V$ (or setting those values to \shalf{} in case there is no outgoing or incoming edges for
some node), we use the equation \eqref{eq:predictor}\footnote{We reproduce this equation here for
convenience: $\sgn\Big(\big(1-\htr(u)\big) + \big(1-\hun(v)\big) -\tau - \tfrac{1}{2}\Big)$.},
having estimated $\tau$ on $E_0$.

The result of this comparison on the six datasets is showed in \autoref{tab:troll_synth_global}.
For each network, we generated the signs once. Then, for different training size (20\%, 40\% or 80\%
of $E$), we sampled a training set and predict using the knowledge of the true $\eta$ (Bayes
predictor) or its estimated value (\usrule{}).
The MCC of the Bayes predictor is the same on all datasets (around $30$\footnote{Here and in the
following, we multiply all MCC value by 100 to improve readability.}), and so is its accuracy
(around $75\%$). The corresponding values for \usrule{} are close on all datasets (although some are
easier) and the gap naturally decreases as the training size increases. Another interesting
quantity is how close can \usrule{} estimates $\eta$ using $\wh{\eta} = \left(1-\htr(u)\right) +
\left(1-\hun(v)\right) -\tauhat$ as defined in \autoref{ss:bayes_approx}. We compute the mean
absolute error (MAE) between $\eta$ and $\wh{\eta}$ for all testing edges, and then specifically for
the testing edges whose both endpoints have been sampled above a certain threshold in the training
set. As expected, the estimation is more accurate with increasing training set size, and with
increasing number of samples for a given edge.

\input{synth_res}

Finally, we can also look in more details at the role of $p$ and $q$ by building a 2D histogram of
the testing edges \euv{} based on their coordinate $(p_u, q_v)$. For instance on
\autoref{fig:tr_synth_mae}, we see that the MAE between $\eta$ and $\wh{\eta}$ is not uniform over
the $p,q$-space. It seems to decrease along the diagonal from $(0,0)$ to $(1,1)$. Note though that
\autoref{fig:tr_synth_sizemae} suggests this is likely due to the higher number of edges in the top
right corner (reflecting the imbalance toward positive edges), which allows better estimation. 
In this case, we cannot use the MCC because off-diagonal, the Bayes predictor classifies edges
either all positive or all negative, resulting in a division by zero in the definition. We thus
fall back on accuracy and display in \autoref{fig:tr_synth_acc} the difference between the accuracy
of the Bayes predictor and the accuracy of \usrule{}. This time, the anti diagonal where
$\frac{p+q}{2}=\lhalf$ seems to play a special role. Indeed, as showed more clearly in
\autoref{fig:tr_synth_cmpacc}, the gap in accuracy between the Bayes predictor and \usrule{}
increases symmetrically as $\frac{p+q}{2}$ moves away from \shalf{}. The gap also tends to disappear
when $\frac{p+q}{2}=\lhalf$, for in that region, both predictors can only rely on random predictions.

While we show those patterns on \wik{} with a 40\% training size, they are consistent across
datasets and training size.
\end{newcontent}

\begin{figure*}[hbtp]
  % \centering
  \begin{subfigure}[b]{0.504\textwidth} 
    \includegraphics[width=\textwidth]{raw/synthgen_wik_rule_40_mae.pdf} 
    \caption{Mean absolute error (MAE) of the \etauv{} estimation.} 
    \label{fig:tr_synth_mae}
  \end{subfigure}~
  \begin{subfigure}[b]{0.504\textwidth} 
    \includegraphics[width=\textwidth]{raw/synthgen_wik_rule_40_sizemae.pdf} 
    \caption{MAE in each cell as a function of the number of sampled edges.} 
    \label{fig:tr_synth_sizemae}
  \end{subfigure}

  \begin{subfigure}[b]{0.504\textwidth} 
    \includegraphics[width=\textwidth]{raw/synthgen_wik_rule_40_acc.pdf} 
    \caption{100 times the accuracy.} 
    \label{fig:tr_synth_acc}
  \end{subfigure}~
  \begin{subfigure}[b]{0.504\textwidth} 
    \includegraphics[width=\textwidth]{raw/synthgen_wik_rule_40_cmpacc.pdf} 
    \caption{Difference of accuracy between \usrule{} and Bayes as $\frac{p+q}{2}$ moves away from $\frac{1}{2}$.} 
    \label{fig:tr_synth_cmpacc}
  \end{subfigure}
  \caption{Results on \wik{} with \usrule{} on a training set of size 40\%}
\end{figure*}

\subsection{Real signs} 
We compared the following algorithms:

\begin{enumerate}[label=\textbf{\arabic*.}]
  \item The label propagation algorithm of \autoref{ss:passive} (referred to as \uslpropGsec{}).
    The actual binarizing threshold was set by cross-validation on the training set.

  \item The \usrule{} algorithm, as described above.

  \item A logistic regression model where each edge $(u,v)$ is associated with the features
    $[1-\htr(u), 1-\hun(v)]$ computed again on \trainset{} (we call this method \uslogregp{}). The best
    binary thresholding is again computed on \trainset{}. Experimenting with this logistic model
    serves to support the claim we made in the introduction that our generative model in
    \autoref{s:gen} is a good fit for the data.

  \item  The solution obtained by directly solving the unregularized problem \eqref{e:quadratic}
    through a fast constrained minimization algorithm (referred to as \qoptim{}). Again, the actual
    binarizing threshold was set by cross-validation on the training set.\footnote{We have also
    tried to minimize \eqref{e:quadratic} by removing the $[-1,+1]$ constraints, but got similar MCC
    results as the ones we report for \qoptim{}}

  \item  The matrix completion method from~\autocite{LowRankCompletion14} based on \complowrank{}
    matrix factorization. Since the authors showed their method to be robust to the choice of the
    rank parameter $k$, we picked $k=7$ in our experiments.

  \item The other \compmaxnorm{} matrix completion method from~\autocite{OnlineCompletion17},
    setting the parameter $\lambda$ to $1.2$ as advised in their paper.

  \item A logistic regression model built on \comptriads{} features derived from status
    theory~\autocite{Leskovec2010}.

  \item The TrollTrust algorithm from~\autocite{wu2016troll}, naming it \compranknodes{}. As for
    hyperparameter tuning ($\beta$ and $\lambda_1$ in~\autocite{wu2016troll}), we closely followed
    the authors' suggestion of doing cross validation.

  \item  The last competitor is the logistic regression model whose features have been build
    according to \autocite{Bayesian15}. We call this method \compbayesian{}.
\end{enumerate}

The above methods can be roughly divided into \emph{local} and \emph{global} methods. A local method
hinges on building local predictive features, based on neighborhoods: \usrule{}, \uslogregp{},
\comptriads{}, and \compbayesian{} essentially fall into this category. The remaining methods
(\uslpropGsec{}, \complowrank{}, \compmaxnorm{} and \compranknodes{}) are global in that their
features are
designed to depend on global properties of the graph topology.

\iffalse
%Given a training set \trainset{}, we exploit the revealed signs in two ways.
First, with label propagation (referred to as \uslprop{} in the following). We
initialize the labels vector $f_0$ to $|V'|$ random values drawn uniformly in
$[0, 1]$ and set the label of the training edges equal to their observed value.
% TODO: starting with zeros gives lower MCC and accuracy after a fixed number
% of iterations
Let the adjacency matrix of $G'$ be $A$, its diagonal degree matrix
be $D$, its diameter be $\diam{G'}$ and $P$ be the following sparse matrix $P=D^{-1}A$. This allow performing
one round of label propagation as $f_{t+1} = Pf_t$, followed by clamping the
training labels. Each round thus involves $2|E|$ multiplications and we do
$\diam{G'}$ of them\footnote{$\diam{G'}$ is a small constant, ranging from $16$
for \aut{} to $38$ for \epi{}}. With the computed labels, we can associate each
edge $\eij$ with $a_{i,j} = \nicefrac{1}{2}\left(f_{\diam{G'}}(\iout) +
f_{\diam{G'}}(\jin)\right)$.
% This is what I do in the code but now I realize this is just an extra
% propagation round only on the square node so maybe simplify the description
% (actually no, it's needed for ERM)
By sorting the $a$ values of the training edges, we can find the threshold $t$
that minimizes the number of mistakes on the training set while predicting
$\yij = \sgn\left(a_{i,j} - t\right)$ in $O(|\trainset{}|\log |\trainset{}|)$
time.

Then we again associate each edge $\eij$ with $a_{i,j} = \left(1-\htr(i)\right) +
\left(1-\hun(j)\right)$ and find the $\tau$ of \eqref{eq:predictor} that minimizes the empirical
risk.  Another way to exploit those quantities is to train a Logistic Regression model where each
edge is associated with two features: $[1-\htr(i), 1-\hun(j)]$ (we call this method \uslogregp{}).
\fi

\bigskip

Our main results are summarized in \autoref{tab:all_mcc}, reporting MCC test set performance after
training on sets of varying size (from 5\% to 25\%, plus 50\% and 90\%). Results have been averaged
over 12 repetitions.
Because scalability is a major concern when training on sizeable datasets, we also give an idea of
relative training times (in milliseconds) by reporting the time it took to train a single run of
each algorithm on a training set of size\footnote{Comparison of training time performances is fair
since all algorithms have been carefully implemented using the same stack of Python libraries, and
run on the same machine (16 Xeon cores and 192Gb Ram).} 15\% of $|E|$, and then predict on the test
set. Though our experiments are not conclusive, some trends can be readily spotted:

\begin{enumerate}[leftmargin=2em,label=\textbf{\arabic*.}]

  \item Global methods tend to outperform local methods in terms of prediction performance, but are
    also significantly (or even much) slower (running times can differ by as much as three orders of
    magnitude). This is not surprising, and is in line with previous experimental findings (e.g.,
    \autocites{shahriari2014ranking}{wu2016troll}). \compbayesian{} looks like an exception to this
    rule, but its running time is indeed in the same ballpark as global methods.

  \item \uslpropGsec{} always ranks first or at least second in this comparison when MCC is
    considered. On top of it, \uslpropGsec{} is fastest among the global methods (one or even two
    orders of magnitude faster), thereby showing the benefit of our approach to edge sign
    prediction.

  \item The regularized solution computed by \uslpropGsec{} is always better than the unregularized
    one computed by \qoptim{} in terms of both MCC and running time.

  \item As claimed in the introduction, our Bayes approximator \usrule{} closely mirrors in
    performance the more involved \uslogregp{} model. In fact, supporting our generative model of
    \autoref{s:gen}, the logistic regression weights for features $1-\htr(i)$ and $1-\hun(j)$
    are almost equal (see \autoref{tab:coeff} in the supplementary material), thereby suggesting that
    predictor~\eqref{eq:predictor}, derived from the theoretical results in
    \autoref{ss:bayes_approx}, is \emph{also} the best logistic model based on trollness and
    untrustworthiness.
\end{enumerate}

\input{all_mcc}

\begin{newcontent}
\subsection{Additional experiments}
  
We perform two other sets of experiments. The first one evaluates the effect of predicting reciprocal
edges by their value if available. As expected, the results in \autoref{tab:twin} demonstrate that
it improves MCC when the network has enough reciprocal edges with low disagreements, like in \sla{} and
\epi{}. It has no effect when there are few reciprocal edges, like in \aut{}, \wik{} and \kiw{}. And it is
detrimental when there are many reciprocal edges that do not agree enough, like in \adv{}, in which
case it is better to rely solely on the learned model.

\input{diff_mcc_pp}

In the second set of experiments, we study the effect of our hypothesis that the labeled edges of the training set are
chosen \uar{}. In two of our datasets (\wik{} and \epi{}), edges come with the timestamp at which they
were created. A more realistic way of choosing the training set is therefore, for a given training
size $m$, to let $E_0$ be the $m$ oldest edges, and try to predict the remaining, newest ones. This
is a common experimental setting in link prediction, where the goal is to infer which pair of nodes
will be connected in the future~\autocite{linkPredSurvey16}. However, as showed in
\autoref{tab:troll_ts_mcc}, this makes the problem much more challenging, both for our methods and
out competitors. It is quite surprising that even when the training set is as large as 90\% of all
edges, the MCC are so low compared with those reported in \autoref{tab:all_mcc}.

\input{ts_mcc}

The explanation of
why these two sampling strategies produce so different results can be inferred from
\autoref{tab:troll_early_sampling}. Letting $V_{\mathrm{out}}$ be the subset of node of $V$ with at
least one outgoing edge, and defining similarly $V_{\mathrm{in}}$, we see that the node coverage is
naturally larger when sampling at random. Yet, when the training set reaches 90\% size, both
sampling strategies cover roughly 90\% of the nodes in $V_{\mathrm{out}}$ and in
$V_{\mathrm{out}}$. The difference in MCC is thus explained by the last three columns. It shows, for
testing edges \euv{}, the breakdown between three situations, from the least to the most
informative:
\begin{enumerate}[(i),nosep]
  \item no sampled edge outgoing from $u$ nor incoming to $v$;
  \item some sampled edges, but only either outgoing from $u$ or incoming to $v$; and
  \item some sampled edges, both outgoing from $u$ and incoming to $v$;
\end{enumerate}
When the training set is build \uar{}, the vast majority of testing edges falls into the case (iii).
This is no longer true with temporal training set, where situations (ii) and even (i) are more
common, making prediction more difficult. The difference of repartition is particularly strong in
\wik{}, justifying the better performance in \epi{}.

\input{sampling}

\end{newcontent}
