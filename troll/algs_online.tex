\section{Algorithms in the Online Setting}\label{s:algonline}

\begin{newcontent}
In the online scenario, recall we do not assume anymore that the signs are generated by the model of
\autoref{s:gen}. Instead, they are chosen adversarially, and presented sequentially in order to
force as many mistakes as possible. This is achieved by deviating from the regular labeling where
every user is either perfectly troll or trustworthy, as measured by the regularity measure
$\Psi_G(Y)$ described at the end of \autoref{s:prel}. We start by presenting an algorithm that
combines randomized Weighted Majority instances and makes little more than $\Psi_G(Y)$ mistakes on
any edge-labeled graph $G(Y) = (V,E(Y))$.  We then show there is not much room for improvement, for
as long as $\Psi_G(Y)$ is within a budget of $K$ irregularities, any online algorithm is condemned
to err at least $\nicefrac{K}{2}$ times.
\end{newcontent}

\begin{theorem}\label{t:online}
  There exists a randomized online prediction algorithm $A$ whose expected number of mistakes
  satisfies $\E M_A(Y) = \Psi_G(Y) + O\left(\sqrt{|V|\Psi_G(Y)} + |V| \right)$
  on any edge-labeled graph $G(Y) = (V,E(Y))$.
\end{theorem}
\begin{proof}

Let each node $u \in V$ host two instances of the randomized Weighted Majority (RWM)
algorithm~\cite{LittlestoneWa94} with an online tuning of their learning rate~\cite{cb+97,acg02}:
one instance for predicting the sign of outgoing edges $(u,v)$, and one instance for predicting the
sign of incoming edges $(v,u)$. Both instances simply compete against the two constant experts,
predicting always $+1$ or always $-1$. Denote by $M(u,v)$ the indicator function (zero-one loss) of
a mistake on edge $(u,v)$. Then the expected number of mistakes of each RWM instance
satisfy~\cite{cb+97,acg02}:
\[
    \sum_{v \in \NNout(u)} \E\,M(u,v) = \Psiout(u,Y) + O\left(\sqrt{\Psiout(u,Y)}+ 1\right)
\]
and
\[
    \sum_{u \in \NNin(v)} \E\,M(u,v) = \Psiin(v,Y) + O\left(\sqrt{\Psiin(v,Y)} + 1\right)~.
\]
We then define two meta-experts: an ingoing expert, which predicts $\yuv$ using the prediction of
the ingoing RWM instance for node $v$, and the outgoing expert, which predicts $\yuv$ using the
prediction of the outgoing RWM instance for node $u$.
The number of mistakes of these two experts satisfy
\begin{align*}
  \sum_{u \in V}\sum_{v \in \NNout(u)} \E\,M(u,v)
                                       &= \Psiout(Y) + O\left(\sqrt{|V|\Psiout(Y)} + |V|\right)\\
  \sum_{v \in V}\sum_{u \in \NNin(v)} \E\,M(u,v)
                                      &= \Psiin(Y)  + O\left(\sqrt{|V|\Psiin(Y)}  + |V|\right)~,
\end{align*}
where we used $\sum_{v \in V} \sqrt{\Psiin(v,Y)} \leq \sqrt{|V|\Psiin(Y)}$, and similarly for
$\Psiout(Y)$.
Finally, let the overall prediction of our algorithm be a RWM instance run on top of the ingoing and
the outgoing experts. Then the expected number of mistakes of this predictor satisfies~
\begin{align*}
  \sum_{(u,v) \in E} \E\,M(u,v) &= \Psi_G(Y) + O\Biggl(\sqrt{|V|\Psi_G(Y)} + |V|
    + \sqrt{\left(\Psi_G(Y) + |V| + \sqrt{|V|\Psi_G(Y)}\right)} \Biggl)\\
    &= \Psi_G(Y) + O\left(\sqrt{|V|\Psi_G(Y)} + |V|\right)
\end{align*}
as claimed.
\end{proof}

We complement the above result by providing a
mistake lower bound. Like \autoref{t:online}, the following result holds for all graphs, and for all
label irregularity levels $\Psi_G(Y)$.

\begin{theorem}\label{t:mistake_bound}
  Given any edge-labeled graph $G(Y) = (V,E(Y))$ and any integer $K \le \big\lfloor
  \tfrac{|E|}{2}\big\rfloor$, a randomized labeling $Y\in\spin^{|E|}$ exists such that $\Psi_G(Y)
  \leq K$, and the expected number of mistakes that any online algorithm $A$ make can be forced to 
  satisfies $\E M_A(Y) \ge \frac{K}{2}$.
\end{theorem}

\begin{proof}
  Let $\mathcal{Y}_K$ be the set of all labelings $Y$ such that the total number of negative and
  positive edges are $K$ and $|E|-K$, respectively (without loss of generality we will focus on
  negative edges). Consider the randomized strategy that draws a labeling $Y \in \spin^{|E|}$ of the
  edges of the input graph \uar{} from $\mathcal{Y}_K$. For each node $u \in V$, we have
  $\Psiin(u,Y) \le \din^-(u)$, which implies $\Psiin(Y) \le K$. A very similar argument applies to
  the outgoing edges, leading to $\Psiout(Y) \le K$. The constraint $\Psi_G(Y) \le K$ is therefore
  always satisfied.

  The adversary will force on average \shalf{} mistakes in each one of the first $K$ rounds of the
  online protocol by repeating $K$ times the following: (i) A label value $\ell\in\{-1,+1\}$ is
  selected uniformly at random. (ii) An edge $(u,v)$ is sampled uniformly at random from the set of
  all edges that were not previously revealed and whose labels are equal to $\ell$. 

  The learner is required to predict $y_{u,v}$ and, in doing so, \shalf{} mistakes will be made on
  average because of the randomized labeling procedure. Observe that this holds even when $A$ knows
  the value of $K$ and $\Psi_G(Y)$. Hence, we can conclude that the expected number of mistakes that
  $A$ can be forced to make is always at least $K/2$, as claimed.
\end{proof}

In fact, we can refine the above statement by proving that, as $\frac{K}{|E|} \rightarrow 0$, the
lower bound gets arbitrarily close to $K$ for any $G(Y)$, hence asymptotically matching the upper
bound of \autoref{t:online}. A sketch of the proof can be found \vpageref{proof:troll_K_mistakes}.
