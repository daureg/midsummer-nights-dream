\section{Algorithms in the Batch Setting}\label{s:algbatch}

Given $G(Y) =(V,E(Y))$, in the batch setting we have at our disposal a training set \trainset{} of
labeled edges from $E(Y)$. We want to build a predictive model for the labels of the remaining
edges. We present two algorithms to do so, which both compute estimates of the all parameters $p$
and $q$ of our generative model. They differ in the approximation guarantees they provide, and in
the class of graphs to which they apply. The first algorithm runs on the graph $G$ and estimates
locally the parameters by their empirical means in the training set, which under some density
assumptions are showed to concentrate around their true values. The second algorithm, on the other
hand, exploits the reduced graph $G''$ and computes a maximum likelihood estimation of the
parameters through a global label propagation approach, without making any density assumption.

\subsection{Approximation to Bayes via dense sampling}\label{ss:bayes_approx}

Our first algorithm is an approximation to the Bayes optimal predictor $y^*(u,v)$. Let us denote by
$\htr(u)$ and $\hun(u)$ the trollness and the untrustworthiness of node $u$ when both are computed
on the subgraph induced by the training edges. We now design and analyze an edge classifier of the
form

\begin{equation}
  \label{eq:predictor}
  \sgn\Big(\big(1-\htr(u)\big) + \big(1-\hun(v)\big) -\tau - \tfrac{1}{2}\Big)~,
\end{equation}

where $\tau \ge 0$ is the only parameter to be trained. Despite its simplicity, this classifier
works reasonably well in practice, as demonstrated by our experiments (see \autoref{s:exp}).
Moreover, unlike previous edge sign prediction methods for directed graphs, our classifier comes
with a rigorous theoretical motivation, since it approximates the Bayes optimal classifier
$y^*(u,v)$ with respect to the generative model defined in \autoref{s:gen}. It is important to point
out that when we use $1-\htr(u)$ and $1-\hun(v)$ to estimate $p_u$ and $q_v$, an additive bias shows
up due to \eqref{e:pout} and \eqref{e:pin}. This motivates the need of a threshold parameter $\tau$
to cancel this bias. Yet, the presence of a prior distribution $\mu(p,q)$ ensures that this bias is
the same for all edges $(u,v) \in E$.

\iffalse

We now introduce a non-adaptive active learning algorithm approximating the Bayes optimal
predictions $y^*(u,v)$.

Given a positive integer parameter $Q < \tfrac{|E|}{2|V|+1}$ such that there exists a
set\footnote{This set is needed to find an estimate $\tauhat$ of $\tau$ in~(\ref{eq:predictor}), see
Step~3 of the algorithm. It can be any set of directed paths/cycles in $G$ that are pairwise
vertex-disjoint.} $E_L \ss E$ of size $Q$ where each vertex $u \in V$ appearing as an endpoint of
some edge in $E_L$ occurs at most once as origin ---ie{}., $(u,v)$, and at most once as destination
---ie{}., $(v,u)$. The algorithm performs the following steps:

\textbf{1.} For each $u \in V$ such that $\din(u) \ge Q$, $Q$ edges are drawn at random without
replacement from $\Nin(u)$, and their are labels queried. Let $\hdeltain(u) = \hdin^+(u)/Q$, where
$\hdin^+(u)$ is the number of positive edges sampled from $\Nin(u)$;

\textbf{2.} For each $u \in V$ such that $\dout(u) \ge Q$, $Q$ edges are drawn at random without
replacement from $\Nout(u)$ and their labels are queried. Let $ \hdeltaout(u) = \hdout^+(u)/Q $,
where $\hdout^+(u)$ is the number of positive edges sampled from $\Nout(u)$;

\textbf{3.} Sample any edge in $E_L$ not yet sampled, and let $\tauhat$ be the fraction of positive
edges in $E_L$;

\textbf{4.} Any remaining non-sampled edge $(u,v)$ is predicted as $ \yhat(u,v) =
\sgn\big(\hdeltaout(u) + \hdeltain(v) - \tfrac{1}{2} - \tauhat \big) $.
\fi

Our algorithm works under the assumption that for given parameters $Q$ (a positive integer) and
$\alpha \in (0,1)$, there exists a set\footnote{$E_L$ is needed to find an estimate $\tauhat$ of
$\tau$ in~(\ref{eq:predictor}), as done in Step~3 of the algorithm. Any undirected matching of $G$ of
size $O(\log|V|)$ can be used, obtained for instance by the blossom algorithm~\autocite{matching65}.
In practice, however, we never computed $E_L$, and estimated $\tau$
on the entire training set \trainset{}.} $E_L \ss E$ of size $\tfrac{2Q}{\alpha}$ where each vertex
$u \in V$ appearing as an endpoint of some edge in $E_L$ occurs at most once as origin ---\ie{}
$(u,v)$--- and at most once as destination ---\ie{} $(v,u)$. Moreover, we assume \trainset{} has
been drawn from $E$ at random {\em without} replacement, with $m = |E_0| = \alpha\,|E|$. The
algorithm performs the following steps:

\begin{enumerate}[label=\textbf{\arabic*.}]
	\item For each $v \in V$, let $\hun(v) = \hdin^-(v)/\hdin(v)$, \ie{} the fraction of
	  negative edges found in $\Nin(v) \cap \trainset{}$.

	\item For each $u \in V$, let $\htr(u) = \hdout^-(u)/\hdout(u)$, \ie{} the fraction of
	  negative edges found in $\Nout(u) \cap \trainset{}$.

	\item Let $\tauhat$ be the fraction of positive edges in $E_L\cap \trainset{}$.

	\item Any remaining edge $(u,v) \in E\setminus \trainset{}$ is predicted as
		$$
		\yhat(u,v) = \sgn\Big(\big(1-\htr(u)\big) + \big(1-\hun(v)\big) -\tauhat - \tfrac{1}{2}\Big)
		$$
\end{enumerate}

The next result shows that the above algorithm can approximate the Bayes optimal predictor on nodes
whose in-degree and out-degree is not too small.

\begin{theorem}
  \label{t:active}
  Let $G(Y) = (V,E(Y))$ be a directed graph with labels on the edges generated according to the
  model in \autoref{s:gen}.
  If the algorithm is run with parameter $Q = \Omega(\log|V|)$, and $\alpha \in (0,1)$ such that the
  above assumptions are satisfied, then $\yhat(u,v) = y^*(u,v)$ holds with high probability
  simultaneously for all test edges $(u,v) \in E$ such that $\dout(u),\din(v) = \Omega(\log|V|)$, and
  $\eta(u,v) = \Pr(\yuv=1)$ is bounded away from $\tfrac{1}{2}$.
\end{theorem}

While we defer the full proof of \autoref{t:active} to the additional material at the end of this
chapter (\autoref{ssec:troll_proof_bayes}), here we give a sketch of the method used, focusing on
parameter $p$. First, by \autoref{l:bins} on negatively associated random variables (stated
\vpageref{l:bins}), we show that with our choice of $Q$, for $\theta=\frac{2Q}{\alpha}$ and for any
$u$ having enough out neighbors (\ie{} $\NNout(u) \ge \theta$), at least $Q$ edges of $\Nout(u)$
are in the training set with high probability.
We use these $Q$ edges to compute $1-\htr(u)$, the empirical probability of drawing a $+1$-labeled
edge from $\Nout(i)$. We show that it is close to the real probability, which according to equation
\eqref{e:pout} is equal to $\frac{1}{2}\,\left(p_u + \qbar_u\right)$. $\qbar_u$ being a sample mean
of i.i.d.~$[0,1]$-valued random variables independently drawn from the prior marginal $\int_0^1
\mu\big(p,\cdot\big) dp$, it concentrates around its expectation $\mu_q$.

By following the same reasoning on the parameters $q$ and using an union bound, we then show that
$\left(1-\htr(u)\right) + \left(1-\hun(v)\right) - \frac{\mu_p+\mu_q}{2}$ is close to
$\frac{p_u+q_v}{2}$ simultaneously for all edges. Another set of concentration arguments then show
that our estimate $\tauhat$ is close to $\frac{\mu_p+\mu_q}{2}$. Combining these two facts ends the
proof.

\medskip

The approach leading to \autoref{t:active} needs the graph to be sufficiently dense. At first sight,
having $Q = \Omega(\log|V|)$ training edges per nodes does not sound that like such a stringent
requirement. Consider for instance Facebook ---which is neither signed nor directed though. It has
two billion
users~\bottomfootnote{\url{https://investor.fb.com/investor-news/press-release-details/2017/Facebook-Reports-Second-Quarter-2017-Results/}},
each of them having $155\approx 7.2\log|V|$ friends on average~\autocite{facebookFriend16}. However,
the constant hidden in the $\Omega$ notation, and needed to guarantee a good behavior, is much
larger than $7.2$. We will nonetheless see in the experiments that we can still apply the resulting
algorithm (we call it \usrule{}) with satisfying results. Additionally, and in order to address this
density limitation, we now introduce a second method based on label propagation.

\subsection{Approximation to Maximum Likelihood via Label Propagation}\label{ss:passive}

For simplicity, assume the joint prior distribution $\mu(p,q)$ is uniform over $[0,1]^2$ with
independent marginals, and suppose that we draw at random without replacement the training set $E_0
= \big((u_1,v_1),y_{u_1,v_1}), ((u_2,v_2),y_{u_2,v_2}), \ldots, ((u_m,v_m),y_{u_m,v_m}\big)$, with
$m = |E_0|$. Then a reasonable approach to approximate $y^*(u,v)$ would be to resort to a maximum
likelihood estimator of the parameters $\{p_i, q_i\}_{i=1}^{|V|}$ based on \trainset{}.
As showed in the supplementary material, the gradient of the log-likelihood function w.r.t.\ $\{p_i,
q_i\}_{i=1}^{|V|}$ satisfies

\begin{equation}\label{e:mlp}
\frac{\partial \log \Pr\left(E_0 \,\Big|\, \{p_i, q_i\}_{u=1}^{|V|}\right)}{\partial p_{\ell}}
=
\sum_{k=1}^m
\frac{\Ind{u_k = \ell,y_{\ell,v_k}=+1}}{p_{\ell}+q_{v_k}}\,
- \sum_{k=1}^m
\frac{\Ind{u_k = \ell,y_{\ell,v_k}=-1}}{2-p_{\ell}-q_{v_k}}
\end{equation}

\begin{align}
&\frac{\partial \log \Pr\left(E_0 \,\Big|\, \{p_i, q_i\}_{u=1}^{|V|}\right)}{\partial q_{\ell}}\label{e:mlq}\\
&\ \ =
\sum_{k=1}^m
\frac{\Ind{v_k = \ell,y_{u_k,\ell}=+1}}{p_{u_k}+q_{\ell}}\,
- \sum_{k=1}^m
\frac{\Ind{v_k = \ell,y_{u_k,\ell}=-1}}{2-p_{u_k}-q_{\ell}}\,,\notag
\end{align}

where $\Ind{\cdot}$ is the indicator function of the event at argument.
Unfortunately, equating \eqref{e:mlp} and \eqref{e:mlq} to zero, and solving for parameters
$\{p_i, q_i\}_{i=1}^{|V|}$ gives rise to a hard set of nonlinear equations.
Moreover, some such parameters may never occur in these equations, namely whenever $\Nout(u)$ or
$\Nin(v)$ are not represented in \trainset{} for some $u,v\in V$.

Our \emph{first approximation} is therefore to replace the nonlinear equations resulting
from \eqref{e:mlp} and \eqref{e:mlq} by the following set of linear equations\footnote{Details
are provided in the supplementary material.}, one for each $\ell \in V$:
\begin{equation*}
  \sum_{k=1}^m \Ind{u_k = \ell,y_{\ell,v_k}=+1} \left(2-p_{\ell}-q_{v_k}\right)
  = \sum_{k=1}^m \Ind{u_k = \ell,y_{\ell,v_k}=-1}
  (p_{\ell}+q_{v_k})
\end{equation*}

\begin{align*}
  \sum_{k=1}^m &\Ind{v_k = \ell,y_{u_k,\ell}=+1} \left(2-p_{u_k}-q_{\ell}\right)\\
  =
  &\sum_{k=1}^m  \Ind{v_k = \ell,y_{u_k,\ell}=-1}
  \left(p_{u_k}+q_{\ell}\right)
\end{align*}

The solution to these equations are precisely the points where the gradient w.r.t.\ $(\bp,\bq)
=\{p_i, q_i\}_{i=1}^{|V|}$ of the quadratic function
\[
f_{E_0}(\bp,\bq) = \sum_{(u,v) \in E_0} \left(\frac{1+\yuv}{2} - \frac{p_u+q_v}{2} \right)^2
\]
vanishes.
We follow a label propagation approach by adding to $f_{E_0}$ the corresponding test set function
$f_{E\setminus \trainset}$, and treat the sum of the two as the function to be minimized during
training w.r.t.\ both $(\bp,\bq)$ and all $\yuv \in [-1,+1]$ for $(u,v)\in E\setminus \trainset{}$:

\begin{equation}\label{e:quadratic}
  \min_{(\bp,\bq), \yuv \in [-1,+1],\,(u,v)\in E\setminus E_0}
  \left(f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)\right)
\end{equation}

Binary $\pm 1$ predictions on the test set $E\setminus \trainset{}$ are then obtained by
thresholding the computed values $\yuv$ at $0$.

\iffalse
********************************************************
and then solve for parameters $\{p_u, q_i\}_{u=1}^{|V|}$.
Yet, it may well be the case that some such parameters never occur in all these equations.\footnote
{
On the other hand, recall that no pairing $(u,v) \in E$ can occur more than once here, since \trainset{} is sampled without replacement.
}
This will happen precisely whenever $\Nout(u)$ or $\Nin(v)$ are not represented in \trainset{}. Specifically, if $E_0 \cap \Nout(u) = \emptyset$ then $p_u$ does not occur, and if $E_0 \cap \Nin(v) = \emptyset$ then $q_v$ does not occur. Hence, for each unsampled edge $(\ell,v)\in E\setminus \trainset{}$, we add to~(\ref{e:mlp2}) the equations
\[
p_{\ell}+q_v = 1+y_{\ell,v}~,
\]
motivated by the fact that $\E \left[\frac{1+y_{\ell,v}}{2}\,|\, (\ell,v)\right] =  \frac{p_{\ell}+q_v}{2}$. Similarly, we add to~(\ref{e:mlq2}) the equations
\[
p_u+q_{\ell} = 1+y_{u,\ell}~.
\]
This gives rise to the following set of equations
%
\begin{align}
p_{u}+ \frac{1}{\dout(u)}\,\sum_{v \in \Nout(u)} q_{v}
&=
\frac{1}{\dout(u)}\,\sum_{v=1}^{|V|} (1+\yuv), \qquad u = 1, \ldots, |V|~,\label{e:mlpa}\\
q_{v} +  \frac{1}{\din(v)}\,\sum_{u \in \Nin(v)} p_{u}
&=
\frac{1}{\din(v)}\,\sum_{u=1}^{|V|} (1+\yuv), \qquad v = 1, \ldots, |V|\,,\label{e:mlqa}\\
1+\yuv &= p_u+q_{v},\qquad\qquad\qquad\quad\,\, (u,v) \in E\setminus E_0~.\label{e:mlya}
\end{align}
%
*******************************************************
\fi

We now proceed to solve \eqref{e:quadratic} via label propagation~\autocite{LabelPropa03} on the
graph $G''$ obtained through the $G \rightarrow G''$ reduction of \autoref{s:prel}.
However, because of the presence of negative edge weights in $G''$, we first have to symmetrize\footnote{%
While we note here that such linear transformation of the variables does not change the problem, we
provide more details in Section~1.3 of the supplementary material.} variables $p_i, q_i$ and $\yuv$
so as they all lie in the interval $[-1,+1]$.
After this step, one can see that, once we get back to the original variables, label propagation
computes the harmonic solution minimizing the function

\begin{equation*}
{\widehat f}\big(\bp,\bq,{\yuv}_{(u,v) \in E\setminus E_0}\big)
= f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)
+ \frac{1}{2}\sum_{u\in V}
\left(\dout(u)\Bigl(p_u-\frac{1}{2}\Bigl)^2+\din(u)\Bigl(q_i-\frac{1}{2}\Bigl)^2\right)
\end{equation*}

The function ${\widehat f}$ is thus a regularized version of the target function $f_{E_0} +
f_{E\setminus E_0}$ in (\ref{e:quadratic}), where the regularization term tries to enforce the extra
constraint that whenever a node $u$ has a high out-degree then the corresponding $p_u$ should be
close to \shalf. Thus, on any edge $(u,v)$ departing from $u$, the Bayes optimal predictor $y^*(u,v)
= \sgn(p_u+q_v-1)$ will mainly depend on $q_v$ being larger or smaller than \shalf{} (assuming $v$
has small in-degree). Similarly, if $u$ has a high in-degree, then the corresponding $q_u$ should be
close to \shalf{} implying that on any edge $(v,u)$ arriving at $u$ the Bayes optimal predictor
$y^*(v,u)$ will mainly depend on $p_v$ (assuming $v$ has small out-degree). Put differently, a node
having a huge out-neighborhood makes each outgoing edge \enquote{count less} than a node having only
a small number of outgoing edges, and similarly for in-neighborhoods.

The label propagation algorithm operating on $G''$ does so (see again
\autoref{fig:troll_reduction_gsecond}) by iteratively updating as follows:

\begin{align*}
  p_{u}  & \leftarrow \frac{-\sum_{v \in \NNout(u)} q_{v} + \sum_{v \in \NNout(u)} (1+\yuv)}{3\,\dout(u)}\,\quad\forall u\in V\\
  q_{v}  & \leftarrow \frac{-\sum_{u \in  \NNin(v)} p_{u} + \sum_{u \in \NNin(v) } (1+\yuv)}{3\,\din(v)}\qquad\forall v \in V\\
  \yuv & \leftarrow \frac{p_u + q_v}{2}~ \quad \forall (u,v) \in E\setminus E_0~.
\end{align*}

The algorithm is guaranteed to converge~\autocite{LabelPropa03} to the minimizer of ${\widehat f}$.
Notice that the presence of negative weights on the edges of $G''$ does not prevent label
propagation from converging. Indeed, any node classification algorithm handling both positive and
negative weights on the edges of $G''$ could be used instead of label propagation. One alternative
would be the \textsc{wta} algorithm from \autocite{WTA13}.  This is the algorithm we will be
championing in our experiments of Section~\ref{s:exp}.
