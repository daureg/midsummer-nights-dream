\section{Algorithms in the Batch Setting}\label{s:algbatch}

Given $G(Y) =(V,E(Y))$, in the batch setting we have at our disposal a training set \trainset{} of
labeled edges from $E(Y)$. Formally, we have a training set $\trainset
= \big((u_1,v_1),y_{u_1,v_1}), ((u_2,v_2),y_{u_2,v_2}), \ldots, ((u_m,v_m),y_{u_m,v_m}\big)$, that
has been drawn from $E\times Y$ \uar{} with replacement, with
$m = |E_0|$. Then a reasonable approach to approximate $y^*(u,v)$ would be to resort to a maximum
 We want to build a predictive model for the labels of the remaining
edges. We present two algorithms to do so, which both compute estimates of the all parameters $p$
and $q$ of our generative model. They differ in the approximation guarantees they provide, and in
the class of graphs to which they apply. The first algorithm runs on the graph $G$ and estimates
locally the parameters by their empirical means in the training set, which under some density
assumptions are showed to concentrate around their true values. The second algorithm, on the other
hand, exploits the reduced graph $G''$ and computes a maximum likelihood estimation of the
parameters through a global label propagation approach, without making any density assumption.

\subsection{Approximation to Bayes via dense sampling}\label{ss:bayes_approx}

Our first algorithm is an approximation to the Bayes optimal predictor $y^*(u,v)$. Let us denote by
$\htr(u)$ and $\hun(u)$ the trollness and the untrustworthiness of node $u$ when both are computed
on the subgraph induced by the training edges.
Recall that the Bayes optimal predictor classifies an edge \euv{} using the following rule:
$y^*(u,v) = \sgn\left( \frac{p_u+q_v}{2} - \frac{1}{2} \right)$. Our approximation thus consists in
using the quantities $\htr(u)$ and $\hun(u)$ to estimate $\frac{p_u+q_v}{2}$. This results in the
following rule:
\begin{equation}
  \label{eq:predictor}
  \sgn\Big(\big(1-\htr(u)\big) + \big(1-\hun(v)\big) - \tau - \tfrac{1}{2}\Big)~,
\end{equation}
where $\tau \ge 0$ is the only parameter to be trained. We now give an intuition to justify this
equation, and defer the technical arguments to end of the chapter,
\vpageref{ssec:troll_proof_bayes}, where we will formalize what it means for a quantity to be
\enquote{close} to another. Note first that $1-\htr(u) = \frac{\hdout^+(u)}{\hdout(u)}$ is the
empirical mean\footnote{The hat symbol denote quantities computed solely on the training set.} of
the probability of a random edge outgoing from $u$ to be positive, and is therefore \enquote{close}
to $\frac{1}{2}\,\left(p_u + \qbar_u\right)$ according to \eqref{e:pout}. By the same reasoning,
$1-\hun(v) \approx \frac{1}{2}\,\left(q_v + \pbar_v\right)$ in accordance with \eqref{e:pin}. At
this stage, $\left( 1 - \htr(u) \right) + \left( 1 - \hun(v) \right) - \frac{p_u+q_v}{2} \approx
\lhalf \left( \pbar_v + \qbar_u \right)$. Since $\pbar_v$ is a sample mean of i.i.d.~$[0,1]$-valued
random variables independently drawn from the prior marginal $\int_0^1 \mu\big(\cdot, q\big) dq$, it
concentrates around its expectation $\mu_p$. Likewise, $\qbar_u \approx \mu_q$. Now we seek an
estimation of $ \lhalf \left( \pbar_v + \qbar_u \right) \approx \lhalf \left( \mu_p + \mu_q
\right)$. For that, we compute the expected number of positive edges in the graph, which, as in the
previous section, is the expected value of the random variable $Z = \sum_{\euv \in E} \Ind{\yuv =
+1}$. Let $V_{\mathrm{out}}$ be the subset of node of $V$ with at least one outgoing edge, and
define similarly $V_{\mathrm{in}}$ as $\theset{v\in V}{\din(v) > 0}$. According to our generative
model, $\E\left(Z \right) = \sum_{\euv \in E} \frac{p_u+q_v}{2} = \sum_{u \in V_{\mathrm{out}}}
\left( \sum_{v \in \NNout(u)} \frac{p_u+q_v}{2} \right)$. Observe that
\begin{align*}
  \sum_{u \in V_{\mathrm{out}}} \left( \sum_{v \in \NNout(u)} p_u \right) &=
  \sum_{v \in V_{\mathrm{in}}} \left( \sum_{u \in \NNin(v)} p_u \right) =
  \sum_{v \in V_{\mathrm{in}}} \din(v) \pbar_v \approx
  \sum_{v \in V_{\mathrm{in}}} \din(v) \mu_p = |E| \mu_p \label{eq:troll_mup}\quad \text{and} \\
  \sum_{u \in V_{\mathrm{out}}} \left( \sum_{v \in \NNout(u)} q_v \right) &=
  \sum_{u \in V_{\mathrm{out}}} \dout(u) \qbar_u \approx
  \sum_{u \in V_{\mathrm{out}}} \dout(u) \mu_q = |E| \mu_q \,.% \label{eq:troll_muq} \,.
\end{align*}
Therefore, letting $\tau$ be the fraction of positive edge in the graph, we have that $\tau =
\frac{\E\left(Z \right)}{|E|} \approx \lhalf \left( \mu_p + \mu_q \right)$. This means that if
$\tauhat$ is the quantity $\tau$ computed on the training, then $\tauhat$ is close to $\left(
\pbar_v + \qbar_u \right)$. Putting together all this approximations thus provide an informal
justification of the rule \eqref{eq:predictor}.

We are now ready to describe our algorithm, which follows naturally from the formula derived above.
It takes as input a training set \trainset{} drawn at random with replacement and outputs a sign
prediction for all the edges in $E\setminus \trainset$. We call it \usrule{}, which stands for Bayes
Learning Classifier based on \emph{tr}ollness and \emph{un}trustworthiness.
\iffalse
Despite its apparent simplicity, \usrule{}
works reasonably well in practice, as demonstrated by our experiments (see \autoref{s:exp}).
Moreover, unlike previous edge sign prediction methods for directed graphs, our classifier comes
with a rigorous theoretical motivation, since it approximates the Bayes optimal classifier
$y^*(u,v)$ with respect to the generative model defined in \autoref{s:gen}. It is important to point
out that when we use $1-\htr(u)$ and $1-\hun(v)$ to estimate $p_u$ and $q_v$, an additive bias shows
up due to \eqref{e:pout} and \eqref{e:pin}. This motivates the need of a threshold parameter $\tau$
to cancel this bias. Yet, the presence of a prior distribution $\mu(p,q)$ ensures that this bias is
the same for all edges $(u,v) \in E$.


We now introduce a non-adaptive active learning algorithm approximating the Bayes optimal
predictions $y^*(u,v)$.

Given a positive integer parameter $Q < \tfrac{|E|}{2|V|+1}$ such that there exists a
set\footnote{This set is needed to find an estimate $\tauhat$ of $\tau$ in~(\ref{eq:predictor}), see
Step~3 of the algorithm. It can be any set of directed paths/cycles in $G$ that are pairwise
vertex-disjoint.} $E_L \ss E$ of size $Q$ where each vertex $u \in V$ appearing as an endpoint of
some edge in $E_L$ occurs at most once as origin ---ie{}., $(u,v)$, and at most once as destination
---ie{}., $(v,u)$. The algorithm performs the following steps:

\textbf{1.} For each $u \in V$ such that $\din(u) \ge Q$, $Q$ edges are drawn at random without
replacement from $\Nin(u)$, and their are labels queried. Let $\hdeltain(u) = \hdin^+(u)/Q$, where
$\hdin^+(u)$ is the number of positive edges sampled from $\Nin(u)$;

\textbf{2.} For each $u \in V$ such that $\dout(u) \ge Q$, $Q$ edges are drawn at random without
replacement from $\Nout(u)$ and their labels are queried. Let $ \hdeltaout(u) = \hdout^+(u)/Q $,
where $\hdout^+(u)$ is the number of positive edges sampled from $\Nout(u)$;

\textbf{3.} Sample any edge in $E_L$ not yet sampled, and let $\tauhat$ be the fraction of positive
edges in $E_L$;

\textbf{4.} Any remaining non-sampled edge $(u,v)$ is predicted as $ \yhat(u,v) =
\sgn\big(\hdeltaout(u) + \hdeltain(v) - \tfrac{1}{2} - \tauhat \big) $.

Our algorithm works under the assumption that for given parameters $Q$ (a positive integer) and
$\alpha \in (0,1)$, there exists a set\footnote{$E_L$ is needed to find an estimate $\tauhat$ of
$\tau$ in~(\ref{eq:predictor}), as done in Step~3 of the algorithm. Any undirected matching of $G$ of
size $O(\log|V|)$ can be used, obtained for instance by the blossom algorithm~\autocite{matching65}.
In practice, however, we never computed $E_L$, and estimated $\tau$
on the entire training set \trainset{}.} $E_L \ss E$ of size $\tfrac{2Q}{\alpha}$ where each vertex
$u \in V$ appearing as an endpoint of some edge in $E_L$ occurs at most once as origin ---\ie{}
$(u,v)$--- and at most once as destination ---\ie{} $(v,u)$. Moreover, we assume \trainset{} has
been drawn from $E$ at random {\em without} replacement, with $m = |E_0| = \alpha\,|E|$. The
algorithm performs the following steps:
\fi

\begin{enumerate}[label=\textbf{\arabic*.}]
	\item For each $u \in V$, let $\htr(u) = \hdout^-(u)/\hdout(u)$, \ie{} the fraction of
	  negative edges found in $\Nout(u) \cap \trainset{}$.

	\item For each $v \in V$, let $\hun(v) = \hdin^-(v)/\hdin(v)$, \ie{} the fraction of
	  negative edges found in $\Nin(v) \cap \trainset{}$.

	\item Let $\tauhat$ be the fraction of positive edges in $E_L\cap \trainset{}$ (this set
	  $E_L$ is a technical requirement that we shall define shortly).

	\item Any edge $(u,v) \in E\setminus \trainset{}$ is predicted as
		$$
		\yhat(u,v) = \sgn\Big(\big(1-\htr(u)\big) + \big(1-\hun(v)\big) -\tauhat - \tfrac{1}{2}\Big)
		$$
\end{enumerate}

Despite its apparent simplicity, \usrule{}
works reasonably well in practice, as demonstrated by our experiments (see \autoref{s:exp}).
Moreover, unlike previous edge sign prediction methods for directed graphs, our classifier comes
with a rigorous theoretical motivation, since it approximates the Bayes optimal classifier
$y^*(u,v)$ with respect to the generative model defined in \autoref{s:gen}. Indeed, we quantify this
approximation on nodes whose in-degree and out-degree are not too small in the next result. However,
this first requires a few extra assumption on the training set. Namely, given parameters $Q$ (a
positive integer) and $\alpha = \nicefrac{|\trainset|}{|E|} \in (0,1)$, we assume there exists a set
$E_L \ss E$ of size $\tfrac{2Q}{\alpha}$ where each vertex $u \in V$ appearing as an endpoint of
some edge in $E_L$ occurs at most once as origin ---\ie{} \euv{}--- and at most once as destination
---\ie{} \evu{}. While the definition of $E_L$ is not immediately intuitive, this set is needed to
find an estimate $\tauhat$ of $\tau$ in~\eqref{eq:predictor} during Step~3 of \usrule{}, and its
definition allows us to applying an Hoeffding bound on independent variables. Any undirected
matching of $G$ of size $O(\log|V|)$ can be used, obtained for instance by the blossom
algorithm~\autocite{matching65}. In practice, however, we never computed $E_L$, and estimated $\tau$
on the entire training set \trainset{} (instead of $E_L \cap \trainset$).

\begin{theorem}
  \label{t:active}
  Let $G(Y) = (V,E(Y))$ be a directed graph with labels on the edges generated according to the
  model in \autoref{s:gen}.
  If the algorithm is run with parameter $Q = \Omega(\log|V|)$, and $\alpha \in (0,1)$ such that the
  above assumption about $E_L$ is satisfied, then $\yhat(u,v) = y^*(u,v)$ holds with high probability
  simultaneously for all test edges $(u,v) \in E$ such that $\dout(u),\din(v) = \Omega(\log|V|)$, and
  $\eta(u,v) = \Pr(\yuv=1)$ is bounded away from $\tfrac{1}{2}$.
\end{theorem}

While we defer the full proof of \autoref{t:active} to the additional material at the end of this
chapter (\autoref{ssec:troll_proof_bayes}), here we give a sketch of the method used.
First, by \autoref{l:bins} on negatively associated random variables (stated
\vpageref{l:bins}), we show that with our choice of $Q$, for $\theta=\frac{2Q}{\alpha}$ and for any
$u$ having enough out neighbors (\ie{} $\dout(u) \ge \theta$), at least $Q$ edges of $\Nout(u)$
are in the training set with high probability. Likewise, for any $v$ with enough in neighbors (\ie
$\din(v) \ge \theta$), at least $Q$ edges of $\Nin(v)$ are in \trainset with high probability.
With this number of samples, we then show a chain of concentration results roughly following the
outline given earlier to justify the equation~\eqref{eq:predictor}, culminating in proving that 
for any $0 < \ve < \tfrac{1}{16}$,
\[ \left| \left( 1 - \htr(u) \right) + \left( 1 - \hun(v) \right) - \tauhat
  - \frac{p_u+q_v}{2} \right| \le 8\ve \]
simultaneously holds with high probability for each $(u,v) \in \theset{(u,v) \in E}{\din(v) \ge
  \theta,\, \dout(u) \ge \theta} \setminus E_0$.

\medskip

The approach leading to \autoref{t:active} requires the graph to be sufficiently dense. At first sight,
having $Q = \Omega(\log|V|)$ training edges per nodes appears to be a reasonable assumption.
Consider for instance Facebook ---which is neither signed nor directed though. It has two billion
users as of 2017~\bottomfootnote{%
\url{https://investor.fb.com/investor-news/press-release-details/2017/Facebook-Reports-Second-Quarter-2017-Results/}},
each of them having $155\approx 7.2\log|V|$ friends on average~\autocite{facebookFriend16}. However,
the constant in the $\Omega$ notation is a trade off between the number of edges to sample per
node, and the quality guarantee of the $\frac{p_u+q_v}{2}$ estimation. In the Facebook example,
having a good guarantee that holds simultaneously for all the test edges might require more than
$7.2\log|V|$ samples per node. We will nonetheless see in the experiments that we can still apply
\usrule{} with satisfying results, especially since its simplicity makes it very scalable.
Additionally, and in order to sidestep this density limitation, we now introduce a second method
based on label propagation.

\subsection{Approximation to Maximum Likelihood via Label Propagation}\label{ss:passive}

Remember we suppose that the training set \trainset{} has been drawn \uar{} with replacement, with
$m = |E_0|$. Then a reasonable approach to approximate $y^*(u,v)$ would be to resort to a maximum
likelihood estimator of the parameters $\{p_u, q_u\}_{u=1}^{|V|}$ based on \trainset{}.
If we further assume, in order to make the computation more tractable, that
the joint prior distribution $\mu(p,q)$ is uniform over $[0,1]^2$ with
independent marginals,\footnote{As we will see, in real data, around $80\%$ if the edges are
positive, meaning this assumption of uniformity over $[0,1]^2$ is unlikely to fully holds, for
otherwise the signs would more balanced}
we show in the supplementary material \vpageref{ssec:troll_mle_deriv} that for $\ell \in \{1,
\ldots, |V|\}$ the gradient of the log-likelihood function with respect to a given $p_\ell$ and
$q_\ell$ satisfies

\begin{equation}\label{e:mlp}
\frac{\partial \log \Pr\left(E_0 \,\Big|\, \{p_u, q_u\}_{u=1}^{|V|}\right)}{\partial p_{\ell}}
=
\sum_{k=1}^m
\frac{\Ind{u_k = \ell,y_{\ell,v_k}=+1}}{p_{\ell}+q_{v_k}}\,
- \sum_{k=1}^m
\frac{\Ind{u_k = \ell,y_{\ell,v_k}=-1}}{2-p_{\ell}-q_{v_k}}
\end{equation}

\begin{equation}\label{e:mlq}
\frac{\partial \log \Pr\left(E_0 \,\Big|\, \{p_u, q_u\}_{u=1}^{|V|}\right)}{\partial q_{\ell}}
= \sum_{k=1}^m
\frac{\Ind{v_k = \ell,y_{u_k,\ell}=+1}}{p_{u_k}+q_{\ell}}\,
- \sum_{k=1}^m
\frac{\Ind{v_k = \ell,y_{u_k,\ell}=-1}}{2-p_{u_k}-q_{\ell}}\,,
\end{equation}

where $\Ind{\cdot}$ is the indicator function of the event at argument.
Unfortunately, equating \eqref{e:mlp} and \eqref{e:mlq} to zero, and solving for parameters
$\{p_u, q_u\}_{u=1}^{|V|}$ gives rise to a hard set of nonlinear equations.
Moreover, some such parameters may never occur in these equations, namely whenever $\Nout(u)$ or
$\Nin(v)$ are not represented in \trainset{} for some $u,v\in V$.

Our \emph{first approximation} is therefore to replace the nonlinear equations resulting
from \eqref{e:mlp} and \eqref{e:mlq} by a set of linear equations. In the case of \eqref{e:mlp}, for
a given $\ell \in V$, we make the assumption that $(p_{\ell}+q_{v_k}) (2-p_{\ell}-q_{v_k})$ is
constant for every $k \in [1,\ldots,m]$. Multiplying \eqref{e:mlp} by this constant
quantity and setting the resulting equation to zero, we obtain for each $\ell \in V$:
\begin{equation}
  \label{eq:troll_appp}
  \sum_{k=1}^m \Ind{u_k = \ell,y_{\ell,v_k}=+1} \left(2-p_{\ell}-q_{v_k}\right)
  = \sum_{k=1}^m \Ind{u_k = \ell,y_{\ell,v_k}=-1}
  (p_{\ell}+q_{v_k})
\end{equation}

We apply a similar transformation to \eqref{e:mlq} in order to obtain, for each $\ell \in V$:
\begin{equation}
  \label{eq:troll_appq}
  \sum_{k=1}^m \Ind{v_k = \ell,y_{u_k,\ell}=+1} \left(2-p_{u_k}-q_{\ell}\right)
  = \sum_{k=1}^m  \Ind{v_k = \ell,y_{u_k,\ell}=-1}
  \left(p_{u_k}+q_{\ell}\right)
\end{equation}

At this point, we find convenient to take a step back and define one of the label
regularity measure we introduced in \autoref{s:prel}. Namely, recall we said $\Psi^2_{G''}(Y)$ could
be seen as a soft quadratic version of the cutsize. With the notation of our generative model, we
can write it as
\begin{equation*}
  \Psi^2_{G''}(Y) = \min_{(\bp,\bq)} \sum_{(u,v) \in E}
  \left(\frac{1+\yuv}{2} - \frac{p_u+q_v}{2} \right)^2 =  
  \min_{(\bp,\bq)} f_E(\bp, \bq)\, ,
\end{equation*}
where $(\bp,\bq) =\{p_u, q_u\}_{u=1}^{|V|}$ is the set of the model parameters. Intuitively,
minimizing $f_E$ with respect to $(\bp,\bq)$ is similar to the maximum likelihood approach, as we
seek the parameters $(\bp, \bq)$ that \enquote{best agree} with the observed signs. It also turns
out that if we restrict the minimization problem to the observed edges (\ie $\min_{(\bp,\bq)}
f_{\trainset}(\bp,\bq)$) and set the derivative of $f_{\trainset}$ with respect to $p_\ell$ and
$q_\ell$ to zero, we recover the equations \eqref{eq:troll_appp} and \eqref{eq:troll_appq}
respectively.

To include the full topology of the graph and not restrain ourselves to the observed edges,
we follow a label propagation approach by adding to $f_{E_0}$ the corresponding test set function
$f_{E\setminus \trainset}$, and treat the sum of the two as the function to be minimized during
training w.r.t.\ both $(\bp,\bq)$ and all $\yuv \in [-1,+1]$ for $(u,v)\in E\setminus \trainset{}$:

\begin{equation}\label{e:quadratic}
  \min_{(\bp,\bq), \yuv \in [-1,+1],\,(u,v)\in E\setminus E_0}
  \left(f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)\right)
\end{equation}

Binary $\pm 1$ predictions on the test set $E\setminus \trainset{}$ are then obtained by
thresholding the computed values $\yuv$.

\iffalse
********************************************************
and then solve for parameters $\{p_u, q_i\}_{u=1}^{|V|}$.
Yet, it may well be the case that some such parameters never occur in all these equations.\footnote
{
On the other hand, recall that no pairing $(u,v) \in E$ can occur more than once here, since \trainset{} is sampled without replacement.
}
This will happen precisely whenever $\Nout(u)$ or $\Nin(v)$ are not represented in \trainset{}. Specifically, if $E_0 \cap \Nout(u) = \emptyset$ then $p_u$ does not occur, and if $E_0 \cap \Nin(v) = \emptyset$ then $q_v$ does not occur. Hence, for each unsampled edge $(\ell,v)\in E\setminus \trainset{}$, we add to~(\ref{e:mlp2}) the equations
\[
p_{\ell}+q_v = 1+y_{\ell,v}~,
\]
motivated by the fact that $\E \left[\frac{1+y_{\ell,v}}{2}\,|\, (\ell,v)\right] =  \frac{p_{\ell}+q_v}{2}$. Similarly, we add to~(\ref{e:mlq2}) the equations
\[
p_u+q_{\ell} = 1+y_{u,\ell}~.
\]
This gives rise to the following set of equations
%
\begin{align}
p_{u}+ \frac{1}{\dout(u)}\,\sum_{v \in \Nout(u)} q_{v}
&=
\frac{1}{\dout(u)}\,\sum_{v=1}^{|V|} (1+\yuv), \qquad u = 1, \ldots, |V|~,\label{e:mlpa}\\
q_{v} +  \frac{1}{\din(v)}\,\sum_{u \in \Nin(v)} p_{u}
&=
\frac{1}{\din(v)}\,\sum_{u=1}^{|V|} (1+\yuv), \qquad v = 1, \ldots, |V|\,,\label{e:mlqa}\\
1+\yuv &= p_u+q_{v},\qquad\qquad\qquad\quad\,\, (u,v) \in E\setminus E_0~.\label{e:mlya}
\end{align}
%
*******************************************************
\fi


We now proceed to solve \eqref{e:quadratic} via label propagation~\autocite{LabelPropa03} on the
graph $G''$ obtained through the second reduction of \autoref{s:prel}.
Indeed, one can show\footnote{as we do in \autoref{ssec:troll_lprop_weights}.} that this objective is
equal ---up to a regularization term--- to the quadratic energy objective minimized by label
propagation methods.
However, because of the presence of negative edge weights in $G''$, we first have to symmetrize\footnote{%
While we note here that such linear transformation of the variables does not change the problem, we
provide more details in \autoref{ssec:troll_lprop_weights} of the supplementary material.} variables
$p_i, q_i$ and $\yuv$ so as they all lie in the interval $[-1,+1]$.
After this step, one can see that, once we get back to the original variables, label propagation
computes the harmonic solution minimizing the function

\begin{equation*}
{\widehat f}\big(\bp,\bq,{\yuv}_{(u,v) \in E\setminus E_0}\big)
= f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)
+ \frac{1}{2}\sum_{u\in V}
\left(\dout(u)\Bigl(p_u-\frac{1}{2}\Bigl)^2+\din(u)\Bigl(q_i-\frac{1}{2}\Bigl)^2\right)
\end{equation*}

The function ${\widehat f}$ is thus a regularized version of the target function $f_{E_0} +
f_{E\setminus E_0}$ in \eqref{e:quadratic}, where the regularization term tries to enforce the extra
constraint that whenever a node $u$ has a high out-degree then the corresponding $p_u$ should be
close to \shalf. Thus, on any edge $(u,v)$ departing from $u$, the Bayes optimal predictor $y^*(u,v)
= \sgn(p_u+q_v-1)$ will mainly depend on $q_v$ being larger or smaller than \shalf{} (assuming $v$
has small in-degree). Similarly, if $u$ has a high in-degree, then the corresponding $q_u$ should be
close to \shalf{} implying that on any edge $(v,u)$ arriving at $u$ the Bayes optimal predictor
$y^*(v,u)$ will mainly depend on $p_v$ (assuming $v$ has small out-degree). Put differently, a node
having a huge out-neighborhood makes each outgoing edge \enquote{count less} than a node having only
a small number of outgoing edges, and similarly for in-neighborhoods.

The label propagation algorithm operating on $G''$ does so (see again
\autoref{fig:troll_reduction_gsecond}) by iteratively updating as follows:

\begin{align*}
  p_{u}  & \leftarrow \frac{-\sum_{v \in \NNout(u)} q_{v} + \sum_{v \in \NNout(u)} (1+\yuv)}{3\,\dout(u)}\,\quad\forall u\in V\\
  q_{v}  & \leftarrow \frac{-\sum_{u \in  \NNin(v)} p_{u} + \sum_{u \in \NNin(v) } (1+\yuv)}{3\,\din(v)}\qquad\forall v \in V\\
  \yuv & \leftarrow \frac{p_u + q_v}{2}~ \quad \forall (u,v) \in E\setminus E_0~.
\end{align*}

The algorithm is guaranteed to converge~\autocite{LabelPropa03} to the minimizer of ${\widehat f}$.
Notice that the presence of negative weights on the edges of $G''$ does not prevent label
propagation from converging. In fact, any node classification algorithm handling both positive and
negative weights on the edges of $G''$ could be used instead of label propagation. One alternative
would thus be the \textsc{wta} algorithm from \autocite{WTA13}. However, our label propagation
algorithm is the one we will be championing in our experiments of \autoref{s:exp}.
