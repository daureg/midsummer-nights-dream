\section{Algorithms in the Batch Setting}\label{s:algbatch}

Given $G(Y) =(V,E(Y))$, we have at our disposal a training set \trainset{} of labeled edges from
$E(Y)$, our goal being that of building a predictive model for the labels of the remaining edges.

Our first algorithm is an approximation to the Bayes optimal predictor $y^*(u,v)$. Let us denote by
$\htr(u)$ and $\hun(u)$ the trollness and the untrustworthiness of node $u$ when both are computed
on the subgraph induced by the training edges. We now design and analyze an edge classifier of the
form

\begin{equation}
  \label{eq:predictor}
  \sgn\Big(\big(1-\htr(u)\big) + \big(1-\hun(v)\big) - \tfrac{1}{2} -\tau\Big)~,
\end{equation}

where $\tau \ge 0$ is the only parameter to be trained. Despite its simplicity, this classifier
works reasonably well in practice, as demonstrated by our experiments (see \autoref{s:exp}).
Moreover, unlike previous edge sign prediction methods for directed graphs, our classifier comes
with a rigorous theoretical motivation, since it approximates the Bayes optimal classifier
$y^*(u,v)$ with respect to the generative model defined in \autoref{s:gen}. It is important to point
out that when we use $1-\htr(u)$ and $1-\hun(v)$ to estimate $p_u$ and $q_v$, an additive bias shows
up due to \eqref{e:pout} and \eqref{e:pin}. This motivates the need of a threshold parameter $\tau$
to cancel this bias. Yet, the presence of a prior distribution $\mu(p,q)$ ensures that this bias is
the same for all edges $(u,v) \in E$.

\iffalse
\subsection{Approximation to Bayes via Active Learning}\label{ss:active}

We now introduce a non-adaptive active learning algorithm approximating the Bayes optimal
predictions $y^*(u,v)$.

Given a positive integer parameter $Q < \tfrac{|E|}{2|V|+1}$ such that there exists a
set\footnote{This set is needed to find an estimate $\tauhat$ of $\tau$ in~(\ref{eq:predictor}), see
Step~3 of the algorithm. It can be any set of directed paths/cycles in $G$ that are pairwise
vertex-disjoint.} $E_L \ss E$ of size $Q$ where each vertex $u \in V$ appearing as an endpoint of
some edge in $E_L$ occurs at most once as origin ---ie{}., $(u,v)$, and at most once as destination
---ie{}., $(v,u)$. The algorithm performs the following steps:

\textbf{1.} For each $u \in V$ such that $\din(u) \ge Q$, $Q$ edges are drawn at random without
replacement from $\Nin(u)$, and their are labels queried. Let $\hdeltain(u) = \hdin^+(u)/Q$, where
$\hdin^+(u)$ is the number of positive edges sampled from $\Nin(u)$;

\textbf{2.} For each $u \in V$ such that $\dout(u) \ge Q$, $Q$ edges are drawn at random without
replacement from $\Nout(u)$ and their labels are queried. Let $ \hdeltaout(u) = \hdout^+(u)/Q $,
where $\hdout^+(u)$ is the number of positive edges sampled from $\Nout(u)$;

\textbf{3.} Sample any edge in $E_L$ not yet sampled, and let $\tauhat$ be the fraction of positive
edges in $E_L$;

\textbf{4.} Any remaining non-sampled edge $(u,v)$ is predicted as $ \yhat(u,v) =
\sgn\big(\hdeltaout(u) + \hdeltain(v) - \tfrac{1}{2} - \tauhat \big) $.
\fi

Our algorithm works under the assumption that for given parameters $Q$ (a positive integer) and
$\alpha \in (0,1)$ there exists a set\footnote{$E_L$ is needed to find an estimate $\tauhat$ of
$\tau$ in~(\ref{eq:predictor}) ---see Step~3 of the algorithm. Any undirected matching of $G$ of
size $\mathcal{O}(\log|V|)$ can be used, for instance using the blossom
algorithm~\autocite{matching65}. In practice, however, we never computed $E_L$, and estimated $\tau$
on the entire training set \trainset{}.} $E_L \ss E$ of size $\tfrac{2Q}{\alpha}$ where each vertex
$u \in V$ appearing as an endpoint of some edge in $E_L$ occurs at most once as origin ---ie{}.,
$(u,v)$--- and at most once as destination ---ie{}., $(v,u)$. Moreover, we assume \trainset{} has
been drawn from $E$ at random {\em without} replacement, with $m = |E_0| = \alpha\,|E|$. The
algorithm performs the following steps:

\begin{enumerate}[label=\textbf{\arabic*.}]
	\item For each $v \in V$, let $\hun(v) = \hdin^-(v)/\hdin(v)$, \ie{} the fraction of
	  negative edges found in $\Nin(v) \cap \trainset{}$.

	\item For each $u \in V$, let $\htr(u) = \hdout^-(u)/\hdout(u)$, \ie{} the fraction of
	  negative edges found in $\Nout(u) \cap \trainset{}$.

	\item Let $\tauhat$ be the fraction of positive edges in $E_L\cap \trainset{}$.

	\item Any remaining edge $(u,v) \in E\setminus \trainset{}$ is predicted as
		$$
		\yhat(u,v) = \sgn\Big(\big(1-\htr(u)\big) + \big(1-\hun(v)\big) - \tfrac{1}{2} -\tauhat\Big)
		$$
\end{enumerate}

The next result\footnote{All proofs are in the supplementary material.} shows that if the graph is
not too sparse, then the above algorithm can approximate the Bayes optimal predictor on nodes whose
in-degree and out-degree is not too small.

\begin{theorem}
  \label{t:active}
  Let $G(Y) = (V,E(Y))$ be a directed graph with labels on the edges generated according to the
  model in \autoref{s:gen}.
  If the algorithm is run with parameter $Q = \Omega(\ln|V|)$, and $\alpha \in (0,1)$ such that the
  above assumptions are satisfied, then $\yhat(u,v) = y^*(u,v)$ holds with high probability
  simultaneously for all test edges $(u,v) \in E$ such that $\dout(u),\din(v) = \Omega(\ln|V|)$, and
  $\eta(u,v) = \Pr(\yuv=1)$ is bounded away from $\tfrac{1}{2}$.
  The overall number of training edges is $\Theta\big(|V|\ln|V|\big)$.
\end{theorem}

The approach leading to Theorem~\ref{t:active} lets us derive the \usrule{} algorithm assessed in
our experiments of \autoref{s:exp}, but it needs the graph to be sufficiently dense and the bias
$\tau$ to be the same for all edges. In order to address these limitations, we now introduce a
second method based on label propagation.

\subsection{Approximation to Maximum Likelihood via Label Propagation}\label{ss:passive}

For simplicity, assume the joint prior distribution $\mu(p,q)$ is uniform over $[0,1]^2$ with
independent marginals, and suppose that we draw at random without replacement the training set $E_0
= \big((u_1,v_1),y_{u_1,v_1}), ((u_2,v_2),y_{u_2,v_2}), \ldots, ((u_m,v_m),y_{u_m,v_m}\big)$, with
$m = |E_0|$. Then a reasonable approach to approximate $y^*(u,v)$ would be to resort to a maximum
likelihood estimator of the parameters $\{p_i, q_i\}_{i=1}^{|V|}$ based on \trainset{}.
As showed in the supplementary material, the gradient of the log-likelihood function w.r.t.\ $\{p_i,
q_i\}_{i=1}^{|V|}$ satisfies

\begin{equation}\label{e:mlp}
\frac{\partial \log \Pr\left(E_0 \,\Big|\, \{p_i, q_i\}_{u=1}^{|V|}\right)}{\partial p_{\ell}}
=
\sum_{k=1}^m
\frac{\Ind{u_k = \ell,y_{\ell,v_k}=+1}}{p_{\ell}+q_{v_k}}\,
- \sum_{k=1}^m
\frac{\Ind{u_k = \ell,y_{\ell,v_k}=-1}}{2-p_{\ell}-q_{v_k}}
\end{equation}

\begin{align}
&\frac{\partial \log \Pr\left(E_0 \,\Big|\, \{p_i, q_i\}_{u=1}^{|V|}\right)}{\partial q_{\ell}}\label{e:mlq}\\
&\ \ =
\sum_{k=1}^m
\frac{\Ind{v_k = \ell,y_{u_k,\ell}=+1}}{p_{u_k}+q_{\ell}}\,
- \sum_{k=1}^m
\frac{\Ind{v_k = \ell,y_{u_k,\ell}=-1}}{2-p_{u_k}-q_{\ell}}\,,\notag
\end{align}

where $\Ind{\cdot}$ is the indicator function of the event at argument.
Unfortunately, equating \eqref{e:mlp} and \eqref{e:mlq} to zero, and solving for parameters
$\{p_i, q_i\}_{i=1}^{|V|}$ gives rise to a hard set of nonlinear equations.
Moreover, some such parameters may never occur in these equations, namely whenever $\Nout(u)$ or
$\Nin(v)$ are not represented in \trainset{} for some $u,v\in V$.

Our \emph{first approximation} is therefore to replace the nonlinear equations resulting
from \eqref{e:mlp} and \eqref{e:mlq} by the following set of linear equations\footnote{Details
are provided in the supplementary material.}, one for each $\ell \in V$:
\begin{equation*}
  \sum_{k=1}^m \Ind{u_k = \ell,y_{\ell,v_k}=+1} \left(2-p_{\ell}-q_{v_k}\right)
  = \sum_{k=1}^m \Ind{u_k = \ell,y_{\ell,v_k}=-1}
  (p_{\ell}+q_{v_k})
\end{equation*}

\begin{align*}
  \sum_{k=1}^m &\Ind{v_k = \ell,y_{u_k,\ell}=+1} \left(2-p_{u_k}-q_{\ell}\right)\\
  =
  &\sum_{k=1}^m  \Ind{v_k = \ell,y_{u_k,\ell}=-1}
  \left(p_{u_k}+q_{\ell}\right)
\end{align*}

The solution to these equations are precisely the points where the gradient w.r.t.\ $(\bp,\bq)
=\{p_i, q_i\}_{i=1}^{|V|}$ of the quadratic function
\[
f_{E_0}(\bp,\bq) = \sum_{(u,v) \in E_0} \left(\frac{1+\yuv}{2} - \frac{p_u+q_v}{2} \right)^2
\]
vanishes.
We follow a label propagation approach by adding to $f_{E_0}$ the corresponding test set function
$f_{E\setminus \trainset}$, and treat the sum of the two as the function to be minimized during
training w.r.t.\ both $(\bp,\bq)$ and all $\yuv \in [-1,+1]$ for $(u,v)\in E\setminus \trainset{}$:

\begin{equation}\label{e:quadratic}
  \min_{(\bp,\bq), \yuv \in [-1,+1],\,(u,v)\in E\setminus E_0}
  \left(f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)\right)
\end{equation}

Binary $\pm 1$ predictions on the test set $E\setminus \trainset{}$ are then obtained by
thresholding the obtained values $\yuv$ at $0$.

\iffalse
********************************************************
and then solve for parameters $\{p_u, q_i\}_{u=1}^{|V|}$.
Yet, it may well be the case that some such parameters never occur in all these equations.\footnote
{
On the other hand, recall that no pairing $(u,v) \in E$ can occur more than once here, since \trainset{} is sampled without replacement.
}
This will happen precisely whenever $\Nout(u)$ or $\Nin(v)$ are not represented in \trainset{}. Specifically, if $E_0 \cap \Nout(u) = \emptyset$ then $p_u$ does not occur, and if $E_0 \cap \Nin(v) = \emptyset$ then $q_v$ does not occur. Hence, for each unsampled edge $(\ell,v)\in E\setminus \trainset{}$, we add to~(\ref{e:mlp2}) the equations
\[
p_{\ell}+q_v = 1+y_{\ell,v}~,
\]
motivated by the fact that $\E \left[\frac{1+y_{\ell,v}}{2}\,|\, (\ell,v)\right] =  \frac{p_{\ell}+q_v}{2}$. Similarly, we add to~(\ref{e:mlq2}) the equations
\[
p_u+q_{\ell} = 1+y_{u,\ell}~.
\]
This gives rise to the following set of equations
%
\begin{align}
p_{u}+ \frac{1}{\dout(u)}\,\sum_{v \in \Nout(u)} q_{v}
&=
\frac{1}{\dout(u)}\,\sum_{v=1}^{|V|} (1+\yuv), \qquad u = 1, \ldots, |V|~,\label{e:mlpa}\\
q_{v} +  \frac{1}{\din(v)}\,\sum_{u \in \Nin(v)} p_{u}
&=
\frac{1}{\din(v)}\,\sum_{u=1}^{|V|} (1+\yuv), \qquad v = 1, \ldots, |V|\,,\label{e:mlqa}\\
1+\yuv &= p_u+q_{v},\qquad\qquad\qquad\quad\,\, (u,v) \in E\setminus E_0~.\label{e:mlya}
\end{align}
%
*******************************************************
\fi

We now proceed to solve \eqref{e:quadratic} via label propagation~\autocite{LabelPropa03} on the
graph $G''$ obtained through the $G \rightarrow G''$ reduction of \autoref{s:prel}.
However, because of the presence of negative edge weights in $G''$, we first have to symmetrize\footnote{%
While we note here that such linear transformation of the variables does not change the problem, we
provide more details in Section~1.3 of the supplementary material.} variables $p_i, q_i$ and $\yuv$
so as they all lie in the interval $[-1,+1]$.
After this step, one can see that, once we get back to the original variables, label propagation
computes the harmonic solution minimizing the function

\begin{equation*}
{\widehat f}\big(\bp,\bq,{\yuv}_{(u,v) \in E\setminus E_0}\big)
= f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq)
+ \frac{1}{2}\sum_{u\in V}
\left(\dout(u)\Bigl(p_u-\frac{1}{2}\Bigl)^2+\din(u)\Bigl(q_i-\frac{1}{2}\Bigl)^2\right)
\end{equation*}

The function ${\widehat f}$ is thus a regularized version of the target function $f_{E_0} +
f_{E\setminus E_0}$ in (\ref{e:quadratic}), where the regularization term tries to enforce the extra
constraint that whenever a node $u$ has a high out-degree then the corresponding $p_u$ should be
close to \shalf. Thus, on any edge $(u,v)$ departing from $u$, the Bayes optimal predictor $y^*(u,v)
= \sgn(p_u+q_v-1)$ will mainly depend on $q_v$ being larger or smaller than \shalf{} (assuming $v$
has small in-degree). Similarly, if $u$ has a high in-degree, then the corresponding $q_u$ should be
close to \shalf{} implying that on any edge $(v,u)$ arriving at $u$ the Bayes optimal predictor
$y^*(v,u)$ will mainly depend on $p_v$ (assuming $v$ has small out-degree). Put differently, a node
having a huge out-neighborhood makes each outgoing edge \enquote{count less} than a node having only
a small number of outgoing edges, and similarly for in-neighborhoods.

The label propagation algorithm operating on $G''$ does so (see again
\autoref{fig:troll_reduction_gsecond}) by iteratively updating as follows:

\begin{align*}
  p_{u}  & \leftarrow \frac{-\sum_{v \in \NNout(u)} q_{v} + \sum_{v \in \NNout(u)} (1+\yuv)}{3\,\dout(u)}\,\quad\forall u\in V\\
  q_{v}  & \leftarrow \frac{-\sum_{u \in  \NNin(v)} p_{u} + \sum_{u \in \NNin(v) } (1+\yuv)}{3\,\din(v)}\qquad\forall v \in V\\
  \yuv & \leftarrow \frac{p_u + q_v}{2}~ \quad \forall (u,v) \in E\setminus E_0~.
\end{align*}

The algorithm is guaranteed to converge~\autocite{LabelPropa03} to the minimizer of ${\widehat f}$.
Notice that the presence of negative weights on the edges of $G''$ does not prevent label
propagation from converging. Indeed, any node classification algorithm handling both positive and
negative weights on the edges of $G''$ could be used instead of label propagation. One alternative
would be the \textsc{wta} algorithm from \autocite{WTA13}.  This is the algorithm we will be
championing in our experiments of Section~\ref{s:exp}.


\label{ssec:further_related}
{\bf Further related work.} The vast majority of existing edge sign prediction algorithms for
directed graphs are based on the computation of local features of the graph. These features are
evaluated on the subgraph induced by the training edges, and the resulting values are used to train
a supervised classification algorithm (e.g., logistic regression). The most basic set of local
features used to classify a given edge $(u,v)$ are defined by
$\din^+(v),\din^-(v),\dout^+(u),\dout^-(u)$ computed over the training set \trainset{}, and by the
embeddedness coefficient $\big|\Nout(u) \cap \Nin(v)\big|$. In turn, these can be used to define
more complicated features, such as
\(
	\frac{\din^+(v) + |E^+|\uin(v)}{\din(v) + \uin(v)}
\quad\text{and}\quad
	\frac{\dout^+(u) + p^+\uout(u)}{\dout(u) + \uout(u)}
\)
introduced in~\autocite{Bayesian15}, together with their negative counterparts, where $|E^+|$ is the
overall fraction of positive edges, and $\uin(v),\uout(u)$ are, respectively, the number of test
edges outgoing from $u$ and the number of test edges incoming to $v$.
Other types of features are derived from social status theory~(e.g., \autocite{Leskovec2010}), and
involve the so-called triads; namely, the triangles formed by $(u,v)$ together with $(u,w)$ and
$(w,v)$ for any $w \in \NNout(u) \cap \NNin(v)$. A third group of features is based on node ranking
scores. These scores are computed using a variery of methods, including
Prestige~\autocite{zolfaghar2010mining}, exponential ranking~\autocite{traag2010exponential},
PageTrust~\autocite{de2008pagetrust}, Bias and Deserve~\autocite{mishra2011finding},
TrollTrust~\autocite{wu2016troll}, and generalizations of PageRank and HITS to signed
networks~\autocite{shahriari2014ranking}. Examples of features using such scores are \textsl{reputation}
and \textsl{optimism}~\autocite{shahriari2014ranking}, defined for a node $u$ by
\(
	\frac{\sum_{v \in \NNin(u)} y_{v,u}\sigma(v)}{\sum_{v \in \NNin(u)} \sigma(v)}
\quad\text{and}\quad
	\frac{\sum_{v \in \NNout(u)} \yuv\sigma(v)}{\sum_{v \in \NNout(u)} \sigma(v)}\,,
\)
where $\sigma(v)$ is the ranking score assigned to node $v$. Some of these algorithms will be used
as representative competitors in our experimental study of \autoref{s:exp}.
