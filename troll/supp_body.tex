% \section{Proofs from \autoref{s:algbatch}}
\section{Additional material}
\subsection{Proofs from \autoref{s:algbatch}}

\subsubsection{Proof of \autoref{t:active}}
\label{ssec:troll_proof_bayes}

The following ancillary results will be useful.

\begin{lemma}[Hoeffding's inequality for sampling without replacement]
\label{l:hoeff-wr}
Let $\scX = \{x_1,\dots,x_N\}$ be a finite subset of $[0,1]$ and let
\[ \mu = \frac{1}{N}\sum_{i=1}^N x_i~.  \]
If $X_1,\dots,X_n$ is a random sample drawn at random from $\scX$ without replacement, then, for
every $\ve > 0$,
\[
  \Pr\left( \left|\frac{1}{n}\sum_{t=1}^n X_t - \mu \right| \ge \ve \right) \le 2e^{-2n\ve^2}~.
\]
\end{lemma}

\begin{lemma}\label{l:bins}
% Given a function $f\,:\, \Nat \rightarrow \R$ such that $f(n) \rightarrow \infty$ as $n \rightarrow \infty$, 
Let $\NN_1, \ldots, \NN_n$ be $n$ subsets of a finite set $E$.
% with $|E| \geq n\,f(n)$
% Let $\theta = \frac{|E|}{n\,f(n)}$, 
% and $k = k(\theta)$ be the number of subsets $\NN_i$ such that $|\NN_i| \geq \theta$. Let 
Let $E_0 \subseteq E$ be sampled uniformly at random {\em without} replacement from $E$, with $|E_0| = m$. Then, for $\delta \in (0,1)$, $Q > 0$, and $\theta \geq 2\times\max\left\{Q,4\log \frac{n}{\delta}\right\}$, we have
\[
\Pr\Bigl( \exists i \,:\, |\NN_i| \geq \theta, |\NN_i \cap E_0| < Q\Bigl) \leq \delta
\]
provided $|E| \geq m \geq \frac{2|E|}{\theta}\times\max\left\{Q,4\log \frac{n}{\delta}\right\}$.
\end{lemma}

\begin{proof}[Proof of \autoref{l:bins}]
  Set for brevity $p_i = \nicefrac{|\NN_i|}{|E|}$. Then, due to the sampling without replacement,
  each random
variable $|\NN_i \cap E_0|$ is the sum of $m$ dependent Bernoulli random variables $X_{i,1}, \ldots,
X_{i,m}$ such that $\Pr(X_{i,t} = 1) = p_i$, for $t = 1, \ldots, m$. Let $i$ be such that $|\NN_i|
\geq \theta$. Then, since $\theta \geq 2Q$, we may assume that $m\frac{2Q}{\theta} \leq |E|$, which implies
\[
Q \leq \frac{m\theta}{2|E|} \leq \frac{m\,p_i}{2} = \frac{\E\big[|\NN_i \cap E_0|\big]}{2}~.
\]
Since the variables $X_{i,j}$ are negatively associated, we may apply a (multiplicative) Chernoff
bound~\autocite[Section~3.1]{dpbook}.
This gives
\[
\Pr\big(|\NN_i \cap E_0| < Q\big) \leq e^{-\frac{m\,p_i}{8}} \leq e^{-\frac{m\,\theta}{8|E|}}
\]
so that
$
\Pr\bigl( \exists i\,:\, |\NN_i| \geq \theta, |\NN_i \cap E_0| < Q\bigr) \leq n\,e^{-\frac{m\,\theta}{8|E|}}
$,
which is in turn upper bounded by $\delta$ whenever 
$m \geq \frac{8|E|}{\theta}\log \frac{n}{\delta}$ (again a natural assumption since $\theta \geq
8\log\frac{n}{\delta}$). % = 8\,n\,f(n)\,\log \frac{n}{\delta}$.
\end{proof}

Let now $E_{\theta} = \theset{(u,v) \in E}{\din(v) \ge \theta,\, \dout(u) \ge \theta} \setminus
E_0$, where $E_0 \ss E$ is the set of sampled edges provided to the learning algorithm of
\autoref{ss:bayes_approx}. Then \autoref{t:active} in the main paper is an immediate consequence of the
following lemma. 

\begin{lemma}\label{l:active}
Let $G(Y) = (V,E(Y))$ be a directed graph with labels on the edges generated according to the model
in \autoref{s:gen}.
For all $0 < \alpha,\delta < 1$ and $0 < \ve < \tfrac{1}{16}$, if the learning algorithm of
\autoref{ss:bayes_approx} is run with parameters $Q = \tfrac{1}{2\ve^2}\log\frac{4|V|}{\delta}$ and
$\alpha$, then with probability at least $1-9\delta$ the predictions $\yhat(u,v)$ satisfy
$\yhat(u,v) = y^*(u,v)$ for all $(u,v) \in E_{\theta}$ such that $\big|\eta(u,v) - \tfrac{1}{2}\big|
> 8\ve$.
\end{lemma}

\begin{proof}[Proof of \autoref{l:active}]
We apply \autoref{l:bins} with $\theta = \tfrac{2Q}{\alpha} \ge 2\times\max\big\{Q,4\log
\frac{2|V|+1}{\delta}\big\}$ to the $2|V|+1$ subsets of $E$ consisting of $E_L$ and, for all $u \in
V$, of $\Nin(u)$ and $\Nout(u)$. We have that, with probability at least $1-\delta$, at least $Q$
edges of $E_L$ are sampled, at least $Q$ edges of $\Nin(v)$ are sampled for each $v$ such that
$\NNin(v) \ge \theta$, and at least $Q$ edges of $\Nout(u)$ are sampled for each $u$ such that
$\NNout(u) \ge \theta$.
For all $(u,v) \in E_{\theta}$ let
\[
  \pbar_v = \frac{1}{\din(v)}\sum_{u\in\NNin(v)} p_u \quad\text{and}\quad
  \qbar_u = \frac{1}{\dout(u)}\sum_{v\in\NNout(u)} q_v
\]
and set for brevity $\hdeltain(v) = 1-\hun(v)$ and $\hdeltaout(u) = 1-\htr(u)$.
We now prove that $\hdeltain(v)$ and $\hdeltaout(u)$ are concentrated around their expectations for
all $(u,v) \in E_{\theta}$. Consider $\hdeltaout(u)$ (the same argument works for $\hdeltain(v)$).
Let $V_1,\dots,V_Q$ be the first $Q$ draws in $E_0 \cap \NNout(u)$ and define
\[
  \muhat_p(u) = \frac{1}{Q} \sum_{t=1}^Q \frac{p_u+q_{V_t}}{2}~.
\]
Applying \autoref{l:hoeff-wr} to the set $\theset{\tfrac{p_u+q_v}{2}}{v \in \NNout(u)}$, and using
our choice of $Q$, we get that $\big|\muhat_p(u) - \mu_p(u)\big| \le \ve$ holds with probability at
least $1-\delta/(2|V|)$, where
\[
  \mu_p(u) = \frac{1}{\dout(u)}\sum_{v \in \NNout(u)} \frac{p_u + q_v}{2} = \frac{p_u + \qbar_u}{2}~.
\]
Now consider the random variables $Z_t = \Ind{y_{u,v_t}=1}$, for $t=1,\dots,Q$. Conditioned on
$V_1,\dots,V_Q$, these are independent Bernoulli random variables with $\E[Z_t \mid V_t] =
\tfrac{p_u+q_{V_t}}{2}$. Hence, applying a standard Hoeffding bound for independent variables and
using our choice of $Q$, we get that
\[
  \left|\frac{1}{Q}\sum_{t=1}^Q Z_t - \muhat_p(u) \right| \le \ve 
\]
with probability at least $1-\delta/(2|V|)$ for every realization of $V_1,\dots,V_Q$. 

Since $\hdeltaout(u) = (Z_1+\cdots+Z_Q)/Q$, we get that $\big|\hdeltaout(u) - \mu_p(u)\big| \le
2\ve$ with probability at least $1-2\delta/(2|V|)$. Applying the same argument to $\hdeltain(v)$,
and the union bound\footnote{The sample spaces for the ingoing and outgoing edges of the vertices
occurring as endpoints in $E_{\theta}$ overlap. Hence, in order to prove a uniform concentration
result, we need to apply the union bound over the random variables defined over these sample spaces.}
on the set $\theset{\hdeltain(v),\hdeltaout(u)}{(u,v) \in E_{\theta}}$, we get that
\begin{equation}
  \label{eq:first}
  \left| \hdeltaout(u) + \hdeltain(v) - \frac{\pbar_v + \qbar_u}{2} - \frac{p_u + q_v}{2} \right| \le 4\ve
\end{equation}
simultaneously holds for all $(u,v) \in E_{\theta}$ with probability at least $1 - 4\delta$. Now
notice that $\pbar_v$ is a sample mean of $Q$ i.i.d.\ $[0,1]$-valued random variables drawn from the
prior marginal $\int_0^1 \mu\big(\cdot,q\bigr) dq$ with expectation $\mu_p$. Similarly, $\qbar_u$ is
a sample mean of $Q$ i.i.d.\ $[0,1]$-valued random variables independently drawn from the prior
marginal $\int_0^1 \mu\big(p,\cdot\big) dp$ with expectation $\mu_q$. By applying Hoeffding bound
for independent variables, together with the union bound to the set of pairs of random variables
whose sample means are $\pbar_v$ and $\qbar_u$ for each $(u,v) \in E_{\theta}$ (there are at most
$2|V|$ of them) we obtain that
\begin{align*}
  \big|\pbar_v - \mu_p\big| \le \ve
  \qquad\text{and}\qquad
  \big|\qbar_u - \mu_q\big| \le \ve
\end{align*}
hold simultaneously for all $(u,v) \in E_{\theta}$ with probability at least $1-2\delta$. Combining
with~\eqref{eq:first} we obtain that
\begin{equation}
  \label{eq:this}
  \left| \hdeltaout(u) + \hdeltain(v) - \frac{\mu_p + \mu_q}{2} - \frac{p_u + q_v}{2} \right| \le 5\ve
\end{equation}
simultaneously holds for each $(u,v) \in E_{\theta}$ with probability at least $1 - 6\delta$. Next,
let $E_L'$ be the set of the first $Q$ edges drawn in $E_L \cap E_0$. Then
\[
  \E\big[\tauhat\big] = \frac{1}{Q}\sum_{(u,v) \in E_L'} \Pr\big(y_{u,v} = 1\big) = \frac{1}{Q}\sum_{(u,v) \in E_L'} \frac{p_u+q_v}{2}~,
\]
where the expectation is w.r.t.\ the independent draws of the labels $y_{u,v}$ for $(u,v) \in E_L'$.
Hence, by applying again Hoeffding bound (this time without the union bound) to the $Q =
\tfrac{1}{2\ve^2}\log\tfrac{4|V|}{\delta}$ independent Bernoulli random variables $\Ind{y_{u,v} = 1}$,
$(u,v) \in E_L'$, the event
$
\big| \tauhat - \E\big[\tauhat\big] \big| \le \ve
$
holds with probability at least $1-\delta$. Now, introduce the function
\[
  F(\bp,\bq) = \E\big[\tauhat\big] = \frac{1}{Q}\sum_{(u,v) \in E_L'} \frac{p_u+q_v}{2}~.
\]
For any realization $\bq_0$ of $\bq$, the function $F_1(\bp) = F(\bp,\bq_0)$ is a sample mean of $Q
= \tfrac{1}{2\ve^2}\log\tfrac{4|V|}{\delta}$ i.i.d.\ $[0,1]$-valued random variables
$\theset{p_u}{(u,v) \in E_L'}$ (recall that if $u \in V$ is the origin of an edge $(u,v) \in E_L'$,
then it is not the origin of any other edge $(u,v') \in E_L'$). Using again the standard Hoeffding
bound, we obtain that
\[
  \left| F(\bp,\bq) - E_{\bp}\big[F(\bp,\bq)\big] \right| \le \ve
\]
holds with probability at least $1 -\delta$ for each $\bq\in [0,1]^{|V|}$. With a similar argument,
we obtain that
\[
  \left| E_{\bp}\big[F(\bp,\bq)\big] - E_{\bp,\bq}\big[F(\bp,\bq)\big] \right| \le \ve
\]
also holds with probability at least $1 -\delta$. Since
\[
  E_{\bp,\bq}\big[F(\bp,\bq)\big] = \frac{\mu_p+\mu_q}{2}
\]
we obtain that
\begin{equation}
  \label{eq:that}
  \Big| \tauhat - \frac{\mu_p+\mu_q}{2} \Big| \le 3\ve
\end{equation}
with probability at least $1-3\delta$. Combining~(\ref{eq:this}) with~(\ref{eq:that}) we obtain
\[
  \left| \hdeltaout(u) + \hdeltain(v) - \tauhat - \frac{p_u+q_v}{2} \right| \le 8\ve
\]
simultaneously holds for each $(u,v) \in E_{\theta}$ with probability at least $1 - 9\delta$.
Putting together concludes the proof.
\end{proof}

\subsubsection{Derivation of the maximum likelihood equations}
\label{ssec:troll_mle_deriv}

Recall that the training set $\trainset = \theset{\big(u_k,v_k),y_{u_k,v_k}\big)}{k=1,\dots,m}$
is drawn uniformly at random from $E$ without replacement. 
We can write

\begin{align*}
  \Pr\left(E_0\,\Big|\, \{p_u, q_u\}_{u=1}^{|V|}\right) =& 
  \frac{1}{\binom{|E|}{m}\,m!}\prod_{k=1}^m \left(\frac{p_{u_k}+q_{v_k}}{2} \right)^{\Ind{y_{u_k,v_k}=+1}}\\
  &\times\prod_{k=1}^m\left(1-\frac{p_{u_k}+q_{v_k}}{2} \right)^{\Ind{y_{u_k,v_k}=-1}}\\
  =&
  \frac{1}{\binom{|E|}{m}\,m!}
  \prod_{\ell=1}^{|V|}\Biggl(\prod_{k=1}^m\,
    \left(\frac{p_{\ell}+q_{v_k}}{2} \right)^{\Ind{u_k = \ell,\,y_{\ell,v_k}=+1}}\,\times\\
    &\prod_{k=1}^m\left(1-\frac{p_{\ell}+q_{v_k}}{2} \right)^{\Ind{u_k = \ell,\,y_{\ell,v_k}=-1}}\Biggl)
\end{align*}

so that $\log \Pr\left(E_0\,\Big|\, \{p_u, q_u\}_{u=1}^{|V|}\right)$ is proportional to

\begin{align*}
  \sum_{\ell=1}^{|V|}\Bigl(&\sum_{k=1}^m \Ind{u_k = \ell,y_{\ell,v_k}=+1}\,
  \log\left(\frac{p_{\ell}+q_{v_k}}{2} \right)\, +\\
  &\sum_{k=1}^m \Ind{u_k = \ell,y_{\ell,v_k}=+1}\, 
\log\left(1-\frac{p_{\ell}+q_{v_k}}{2} \right)\Bigl)
\end{align*}

and

\begin{equation*}
  \frac{\partial \log \Pr\left(E_0\,\Big|\, \{p_u, q_u\}_{u=1}^{|V|}\right)}{\partial p_{\ell}}
  = \sum_{k=1}^m 
  \frac{\Ind{u_k = \ell,y_{\ell,v_k}=+1}}{p_{\ell}+q_{v_k}}\,
   - \,
  \sum_{k=1}^m \frac{\Ind{u_k = \ell,y_{\ell,v_k}=-1}}{2-p_{\ell}-q_{v_k}}\,.
\end{equation*}

By a similar argument,

\begin{align*}
  \Pr\left(E_0\,\Big|\, \{p_u, q_u\}_{u=1}^{|V|}\right) 
=
  \frac{1}{\binom{|E|}{m}\,m!}
  \prod_{\ell=1}^{|V|}\Biggl(&\prod_{k=1}^m\,
    \left(\frac{p_{u_k}+q_{\ell}}{2} \right)^{\Ind{v_k = \ell,\,y_{u_k,\ell}=+1}}\,
\times\\
&\prod_{k=1}^m\left(1-\frac{p_{u_k}+q_{\ell}}{2} \right)^{\Ind{v_k = \ell,\,y_{u_k,\ell}=-1}}\Biggl)
\end{align*}

so that

\begin{align*}
\frac{\partial \log \Pr\left(E_0\,\Big|\, \{p_u, q_u\}_{u=1}^{|V|}\right)}{\partial q_{\ell}}
= \sum_{k=1}^m \frac{\Ind{v_k = \ell,y_{u_k,\ell}=+1}}{p_{u_k}+q_{\ell}}\,
- \, \sum_{k=1}^m \frac{\Ind{v_k = \ell,y_{u_k,\ell}=-1}}{2-p_{u_k}-q_{\ell}}\,.
\end{align*}

\subsubsection{Label propagation on $G''$}
\label{ssec:troll_lprop_weights}

Here we provide more details on the choice of weight for the edges of $G''$, as well as an
explanation on why we temporarily use symmetrized variables lying in $[-1, 1]$ (which we will denote
with primes, so that for instance $p'_u = 2p_u-1$). Since only the ratio between the negative and
positive weights matters, we fix the negative weight of the edges in $E''\setminus E'$ to be $-1$
and we denote by $\eps$ the weight of edges in $E'$.  With these notations, Label Propagation on
$G''$ seeks the harmonic minimizer of the following expression 

\begin{equation*}
  \frac{1}{16} \sum_{u,v \in E} \Bigl[ \eps\left(y_{u,v} - p_u'\right)^2 + \eps\left(y_{u,v} - q_v'\right)^2 + (p'_u+q_v')^2 \Bigr]
\end{equation*}

which can be successively rewritten as

\begin{align*}
  \frac{1}{16} \sum_{u,v \in E} & \Bigl[ \eps\left(y_{u,v}+1 - 2p_u\right)^2 + \eps\left(y_{u,v}+1 - 2q_v\right)^2
	+ (2p_u +2q_v -2)^2 \Bigr]
\\=
	\frac{1}{8} \sum_{u,v \in E} & \Biggl[ 2\eps\!\left(\frac{y_{u,v}+1}{2} - p_u\right)^2\!\! +  2\eps\!\left(\frac{y_{u,v}+1}{2} - q_v\right)^2
	+ 8\left(\frac{p_u +q_v -1}{2}\right)^2 \Biggr]
\\=
  \frac{1}{8}\sum_{u,v \in E} & \Biggl[ 2\eps\left(\left(\frac{y_{u,v}+1}{2}\right)^2 -
    p_u(1+y_{u,v}) + p_u^2\right) + %\\
                              2\eps\left(\left(\frac{y_{u,v}+1}{2}\right)^2 - q_v(1+y_{u,v}) + q_v^2\right) +  \\
                              & 8\left(\left(\frac{p_u +q_v}{2}\right)^2 - \frac{p_u+q_v}{2} + \frac{1}{4} \right) \Biggr]
\\=
  \frac{1}{8}\sum_{u,v \in E} & 4\Biggl(\eps\!\left(\frac{y_{u,v}+1}{2}\right)^2\!\! -
  2\eps\!\left(\frac{y_{u,v}+1}{2}\right)\left(\frac{p_u+q_v}{2}\right)
% \\&
  + 2\!\left(\frac{p_u +q_v}{2}\right)^2\Biggr)
\\&+
  \sum_{u,v \in E} \Bigl[ \left(2\eps p_u^2 - 4p_u + 1\right) + \left(2\eps q_v^2 - 4q_v + 1\right) \Bigr]
\end{align*}

By setting $\eps=2$, we can factor this expression into 
\begin{align*}
  \sum_{u,v \in E} &\left(\frac{y_{u,v}+1}{2} - \frac{p_u+q_v}{2}\right)^2
  + \frac{1}{2}\sum_{u,v \in E} \left(\left(p_u - \frac{1}{2}\right )^2 + \left(q_v -
  \frac{1}{2}\right)^2 \right)~.
\end{align*}

\subsection{Proof from \autoref{s:algonline}}
\label{proof:troll_K_mistakes}

\begin{proof}[Proof sketch that, in the context of \autoref{t:mistake_bound}, as $\frac{K}{|E|}
  \rightarrow 0$ then $\E M_A(Y)=K$  ]

Let $\mathcal{E}$ be the following event: There is at least one unrevealed negative label.
The randomized iterative strategy used to force a number of mistakes arbitrarily close to $K$ is identical to the one described
in the proof of \autoref{t:mistake_bound} \vpageref{t:mistake_bound}, except for the stopping criterion.
Namely, the adversary draws a labeling $Y$ \uar{} from $\mathcal{Y}_K$, the set of all labelings $Y$
such that the total number of negative and positive edges are $K$ and $|E|-K$. Then the adversary
repeats the following two steps: (i) A label value $\ell\in\{-1,+1\}$ is selected \uar{}. (ii) An
edge $(u,v)$ is sampled \uar{} from the set of all edges that were not previously revealed and whose
labels are equal to $\ell$. This is done until $\mathcal{E}$ is true.

Let $m_{r,c}$ be defined as follows: For $c=1$ it is equal to the expected number of mistakes forced
in round $r$ when $K=1$. For $c > 1$ it is equal to the difference between the expected number of
mistakes forced in round $r$ when $K=c$ and $K=c-1$. One can see that $m_{r,c}$ is 
%either 
null when $r<c$. 
%and, for any given $|E|$ and $K$, when $r>c+|E|-K-1$.
%at least one of the following  conditions is met: , . 
%$c>K$, 
%
When $K=1$, the probability that $\mathcal{E}$ is true in round $r$ is clearly equal to
$\frac{1}{2^{r-1}}$.
Hence, the expected number of mistakes made by $A$ when $K=1$ in any round $r$ is equal to 
\(
\frac{1}{2}\,\frac{1}{2^{r-1}} = \frac{1}{2^{r}}.
\)
We can therefore conclude that $m_{r,1}=\frac{1}{2^{r}}$ for all $r$.

%When $r> 1$, 
A simple calculation shows that if $r=c$ then $m_{r,c}=\frac{1}{2^r}$. Furthermore, when $r>1$ and $c>1$, 
%for all $r$ and $c$ values such that $1 < c \le K$ and $c < r \le c+|E|-K-1$, 
we have the following recurrence:
%(holding even when $\frac{K}{|E|}$ does not converge to 0)
\[
m_{r,c}=\frac{m_{r-1,c}+m_{r-1,c-1}}{2}~.
\]  
In order to calculate $m_{r,c}$ for all $r$ and $c$, we will rest on the ancillary quantity
$s_j(i)$, defined recursively for any positive integer variables $i$ and $j$ by
$ s_0(i)=1 $
and
\[
s_j(i)=\sum_{k=1}^i s_{j-1}(k)~.
\]
(see \autoref{tab:sij}) for examples)


\begin{center}
  \begin{tabular}{|c |M{.45cm}|M{.45cm}|M{.45cm}|M{.45cm}|N}
    \hline
    \diaghead{j\ i}{\ \ \ 	{\footnotesize \textit{\textbf{j}}}\ }{\ \ {\footnotesize \textit{\textbf{i}}}\ \ \ } & \textbf{0}& \textbf{1} & \textbf{2} & \textbf{3}    &\\[13pt] \hline
    \textbf{1} & 1 & 1 & 1 & 1  &\\[13pt] \hline
    \textbf{2} & 1 & 2 & 3 & 4 &\\[13pt] \hline
    \textbf{3} & 1 & 3 & 6 & 10 &\\[13pt] \hline
    \textbf{4} & 1 & 4 & 10 & 20 &\\[13pt] \hline
    \textbf{5} & 1 & 5 & 15 & 35 &\\[13pt] \hline
    \textbf{6} & 1 & 6 & 21 & 56 &\\[13pt] \hline
    \textbf{7} & 1 & 7 & 28 & 84 &\\[13pt] \hline
  \end{tabular}
  \captionof{table}{Values of $s_j(i)$ for $i \le 7$  and $j \le 3$.} \label{tab:sij} 
\end{center}

It is not difficult to verify that 
\[
m_{r,c}=\frac{s_{c-1}(r-c+1)}{2^{r}}. 
\]
Since $s_j(i)=\frac{\langle i\rangle_j}{j!}$, where $\langle i\rangle_j$ is the rising factorial
$i(i+1)(i+2)\ldots(i+j-1)$, we have 
\[
m_{r,c}=\frac{\langle r-c+1\rangle_{c-1}}{(c-1)!2^{r}}.
\] 
(see \autoref{tab:mrc}) for examples)


\begin{center}
  \begin{tabular}{|c |M{.77cm}|M{.77cm}|M{.77cm}|M{.77cm}|N}
    \hline
    \diaghead{\theadfont i}%
    {{\footnotesize \textit{\textbf{c}}}}{{\footnotesize \textit{\textbf{r}}}}
               & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} &  \\ [13pt] \hline
    \textbf{1} & $1/2^1$    & 0          & 0          & 0          &  \\ [13pt] \hline
    \textbf{2} & $1/2^2$    & $1/2^2$    & 0          & 0          &  \\ [13pt] \hline
    \textbf{3} & $1/2^3$    & $2/2^3$    & $1/2^3$    & 0          &  \\ [13pt] \hline
    \textbf{4} & $1/2^4$    & $3/2^4$    & $3/2^4$    & $1/2^4$    &  \\ [13pt] \hline
    \textbf{5} & $1/2^5$    & $4/2^5$    & $6/2^5$    & $4/2^5$    &  \\ [13pt] \hline
    \textbf{6} & $1/2^6$    & $5/2^6$    & $10/2^6$   & $10/2^6$   &  \\ [13pt] \hline
    \textbf{7} & $1/2^7$    & $6/2^7$    & $15/2^7$   & $20/2^7$   &  \\ [13pt] \hline
  \end{tabular}
  \captionof{table}{Values of $m_{r,c}$ for $r \le 7$, $c \le 4$ and $|E| \rightarrow \infty$.}
  \label{tab:mrc} 
\end{center}

\bigskip
%Given any $K'>1$, the difference between the expected number of mistakes %forced when $K=K'$ and $K=K'-1$ is equal to 
%\[
%\sum_{r=K'}^{|E|-1} m_{r,K'}=
%\sum_{r=K'}^{|E|-1} \frac{\langle r-K'+1\rangle_{K'-1}}{(K'-1)!2^{r}}.
%\] 

When $\frac{K}{|E|} \rightarrow 0$, given any integer $K'>1$, the difference between the expected
number of mistakes forced when $K=K'$ and $K=K'-1$ is equal to
%
\begin{align*}
  \sum_{r=K'}^{\infty} m_{r,K'}
  &=
  \frac{1}{(K'-1)!}\sum_{r=K'}^{\infty} \frac{\langle r-K'+1\rangle_{K'-1}}{2^{r}}\\
  &=
  \frac{1}{(K'-1)!2^{K'-1}}\sum_{r'=1}^{\infty} \frac{\langle r'\rangle_{K'-1}}{2^{r'}}~,
\end{align*}
% 
where we set $r'=r-K'+1$.
%Taking into account the following recurrence relation sequence 
Setting $i'=i-1$ and recalling that
\[
\langle i\rangle_j=j!{{i+j-1}\choose{i-1}}~,
\]
we have
\[
\frac{1}{j!}\sum_{i=1}^{\infty}\frac{\langle i\rangle_j}{2^i}=
\sum_{i=1}^{\infty}\frac{{{i+j-1}\choose{i-1}}}{2^i}=
\sum_{i'=0}^{\infty}\frac{{{i'+j}\choose{i'}}}{2^{i'+1}}~.
\]
Now, using the identity
\[
{i'+j+1 \choose i'} = {i'+j \choose i'} + {i'+j \choose i'-1}~,
\]
we can easily prove by induction on $j$ that
\[
\sum_{i'=0}^{\infty}\frac{{{i'+j}\choose{i'}}}{2^{i'+1}}=2^j~.
\]
Hence, we have
%\[
%\frac{1}{j!}\sum_{i=1}^{\infty}\frac{\langle i\rangle_j}%{2^i}=2^j~,
%\]
\[
\sum_{r=K'}^{\infty} m_{r,K'}=1.
\]
Moreover, as shown earlier, $m_{r,1}=\frac{1}{2^{r}}$ for all $r$. Hence we can conclude that when $\frac{K}{|E|} \rightarrow 0$ 
\[
\E M_A(Y) \ge
\sum_{r=1}^{\infty} \frac{1}{2^{r}} +
\sum_{K'=2}^{K} \sum_{r=K'}^{\infty} m_{r,K'}=K
\]
for any edge-labeled graph $G(Y)$ and any constant $K$, as claimed.
\end{proof}


\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the special case when the bias is
\[
    \Psi(Y) = \min\big\{\Psiin(Y),\Psiout(Y)\big\}
\]
where
\[
    \Psiin(Y) = \sum_{j \in V} \min\big\{\din^-(v),\din^+(v)\big\}
\]
and
\[
    \Psiout(Y) = \sum_{i \in V} \min\big\{\dout^-(u),\dout^+(u)\big\}
\]
then using Randomized Weighted Majority on the two experts
\[
    \Yhat^{(1)}_{u,v} = \sgn\left(\frac{1}{2}-\frac{\hdout^-(u)}{\hdout(u)}\right)
\qquad\text{and}\qquad
    \Yhat^{(2)}_{u,v} = \sgn\left(\frac{1}{2}-\frac{\hdin^-(v)}{\hdin(v)}\right)
\]
we get an expected mistake bound of
\[
    2\Psi(Y) + \sqrt{(4\log 2) \Psi(Y)}~.
\]
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Further Experimental Results}

This section contains more evidence related to the experiments in \autoref{s:exp}.
In particular, we experimentally demonstrate the alignment between \usrule{} and \uslogregp{}.

\iffalse
\subsection{Accuracy Results}
%
\autoref{tab:all_acc} contains the accuracy results on training sets of increasing size. Figures are
averaged over 12 random draws of the training set.

\setlength{\tabcolsep}{3pt}
{\scriptsize
\begin{longtable}{lrccc|cccc}
\caption{Like in the main paper, we tabulate Accuracy $\pm$ one standard deviation with increasing training set size, averaged over 12 random draws of the training set. The best accuracy performance is highlighted in \textbf{\textcolor{brown}{bold brown}} and the second one in \textit{\textcolor{red}{italic red}}. If the difference is statistically significant (as defined by a paired Student's $t$-test $p$-value less than $0.005$), the best score is underlined. The "time" rows contain the time taken to train on a $15\%$ training set. \label{tab:all_acc}}\\
\toprule
                                                  & $\frac{|\trainset{}|}{|E|}$ &       \uslogregp{} &          \usrule{} &     \uslpropGsec{} &                \compranknodes{} &                \compbayesian{} &     \complowrank{} &                \comptriads{} \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{\aut{}}} & $3\%$  &  $57.14 \pm 0.78$  &  $57.15 \pm 1.31$  &  $60.76 \pm 2.76$  &               $68.99 \pm 0.89$  &    $\vsecond{72.02} \pm 0.26$  &  $70.18 \pm 0.33$  &   $\vfirst{72.44} \pm 0.13$  \\
                                                  %& $5\%$  &  $62.07 \pm 0.79$  &  $61.97 \pm 1.17$  &  $66.80 \pm 1.53$  &               $69.44 \pm 0.69$  &     $\vfirst{71.90} \pm 0.33$  &  $70.49 \pm 0.27$  &  $\vsecond{71.87} \pm 0.65$  \\
                                                  %& $7\%$  &  $65.01 \pm 0.50$  &  $65.98 \pm 0.72$  &  $68.76 \pm 1.63$  &  $\vsecondSig{71.21} \pm 0.69$  &  $\vfirstSig{72.89} \pm 0.54$  &  $71.03 \pm 0.27$  &            $70.69 \pm 0.54$  \\
                                                  & $9\%$  &  $67.07 \pm 0.56$  &  $68.29 \pm 0.79$  &  $70.65 \pm 1.04$  &  $\vsecondSig{71.75} \pm 0.65$  &  $\vfirstSig{73.37} \pm 0.78$  &  $71.41 \pm 0.42$  &            $70.48 \pm 0.77$  \\
                                                  & $15\%$ &  $71.07 \pm 0.36$  &  $72.79 \pm 0.38$  &  $74.10 \pm 1.07$  &  $\vsecondSig{74.47} \pm 0.56$  &  $\vfirstSig{75.29} \pm 0.61$  &  $72.72 \pm 0.34$  &            $70.46 \pm 0.59$  \\
                                                  & $20\%$ &  $73.10 \pm 0.21$  &  $74.63 \pm 0.64$  &  $75.17 \pm 1.09$  &  $\vsecondSig{75.71} \pm 0.49$  &  $\vfirstSig{76.44} \pm 0.43$  &  $73.48 \pm 0.23$  &            $71.30 \pm 0.49$  \\
                                                  & $25\%$ &  $74.79 \pm 0.26$  &  $76.32 \pm 0.28$  &  $76.74 \pm 0.78$  &     $\vsecond{77.06} \pm 0.59$  &     $\vfirst{77.46} \pm 0.40$  &  $74.49 \pm 0.35$  &            $71.66 \pm 0.35$  \\
                                                  & time (ms)   &                           1.9 &                           0.4 &                          16.1 &              128 &                            5398 &               1894 &              4.6 \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{\wik{}}} & $3\%$  &  $74.84 \pm 0.38$  &           $75.12 \pm 0.63$  &            $76.45 \pm 1.50$  &           $77.94 \pm 0.24$  &  $\vsecondSig{78.06} \pm 0.35$  &  $77.97 \pm 0.17$  &  $\vfirstSig{78.76} \pm 0.17$  \\
                                                  %& $5\%$  &  $78.32 \pm 0.34$  &           $78.86 \pm 0.37$  &            $79.33 \pm 0.79$  &  $\vfirst{79.67} \pm 0.48$  &     $\vsecond{79.42} \pm 0.60$  &  $78.78 \pm 0.25$  &              $78.83 \pm 0.17$  \\
                                                  %& $7\%$  &  $80.22 \pm 0.28$  &           $80.55 \pm 0.45$  &   $\vfirst{80.86} \pm 0.78$  &           $80.44 \pm 0.90$  &     $\vsecond{80.68} \pm 0.30$  &  $79.60 \pm 0.22$  &              $79.29 \pm 0.31$  \\
                                                  & $9\%$  &  $81.32 \pm 0.18$  &  $\vfirst{81.98} \pm 0.38$  &  $\vsecond{81.62} \pm 0.84$  &           $81.17 \pm 0.31$  &               $81.61 \pm 0.39$  &  $80.15 \pm 0.14$  &              $79.74 \pm 0.22$  \\
                                                  & $15\%$ &  $83.27 \pm 0.14$  &  $\vfirst{83.78} \pm 0.14$  &            $83.30 \pm 0.44$  &           $82.91 \pm 0.41$  &     $\vsecond{83.47} \pm 0.34$  &  $81.09 \pm 0.21$  &              $80.66 \pm 0.30$  \\
                                                  & $20\%$ &  $84.13 \pm 0.19$  &  $\vfirst{84.60} \pm 0.17$  &            $84.36 \pm 0.22$  &           $83.84 \pm 0.38$  &     $\vsecond{84.39} \pm 0.25$  &  $81.73 \pm 0.16$  &              $81.35 \pm 0.18$  \\
                                                  & $25\%$ &  $84.63 \pm 0.13$  &  $\vfirst{85.09} \pm 0.18$  &            $84.63 \pm 0.28$  &           $84.44 \pm 0.28$  &     $\vsecond{84.96} \pm 0.21$  &  $82.08 \pm 0.16$  &              $81.92 \pm 0.13$  \\
                                                  & time (ms)   &                           4.2 &              1.1 &                           35.3 &              210 &              14089 &              4858 &                                                   10.6\\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{\sla{}}} & $3\%$  &  $69.06 \pm 0.19$  &  $69.39 \pm 0.37$  &               $74.01 \pm 0.84$  &  $\vfirstSig{80.74} \pm 0.21$  &               $78.33 \pm 0.14$  &  $\vsecondSig{79.90} \pm 0.17$  &  $74.59 \pm 0.58$  \\
                                                  %& $5\%$  &  $72.92 \pm 0.16$  &  $73.34 \pm 0.32$  &               $76.68 \pm 0.76$  &  $\vfirstSig{81.82} \pm 0.29$  &               $79.41 \pm 0.10$  &  $\vsecondSig{80.38} \pm 0.11$  &  $76.57 \pm 0.54$  \\
                                                  %& $7\%$  &  $75.11 \pm 0.10$  &  $75.58 \pm 0.19$  &               $78.95 \pm 0.36$  &  $\vfirstSig{82.34} \pm 0.30$  &               $79.99 \pm 0.10$  &  $\vsecondSig{80.59} \pm 0.07$  &  $78.03 \pm 0.55$  \\
                                                  & $9\%$  &  $76.58 \pm 0.04$  &  $77.06 \pm 0.21$  &               $79.91 \pm 0.40$  &  $\vfirstSig{82.65} \pm 0.20$  &               $80.64 \pm 0.11$  &  $\vsecondSig{80.82} \pm 0.04$  &  $79.58 \pm 0.60$  \\
                                                  & $15\%$ &  $79.16 \pm 0.08$  &  $79.68 \pm 0.16$  &               $81.95 \pm 0.23$  &  $\vfirstSig{83.12} \pm 0.20$  &  $\vsecondSig{82.02} \pm 0.09$  &               $81.16 \pm 0.09$  &  $81.71 \pm 0.40$  \\
                                                  & $20\%$ &  $80.29 \pm 0.12$  &  $80.99 \pm 0.11$  &  $\vsecondSig{82.89} \pm 0.14$  &  $\vfirstSig{83.26} \pm 0.09$  &               $82.82 \pm 0.09$  &               $81.64 \pm 0.14$  &  $82.73 \pm 0.35$  \\
                                                  & $25\%$ &  $81.08 \pm 0.08$  &  $81.86 \pm 0.05$  &  $\vsecondSig{83.45} \pm 0.10$  &  $\vfirstSig{83.66} \pm 0.14$  &               $83.39 \pm 0.08$  &               $81.94 \pm 0.15$  &  $83.42 \pm 0.16$  \\
& time (ms)   &              21.2 &              5.7 &                           655 &                            1918 &              77042 &              56252 &              78 \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{\epi{}}} & $3\%$  &  $80.23 \pm 0.19$  &  $80.44 \pm 0.17$  &               $86.56 \pm 0.30$  &  $\vfirstSig{89.44} \pm 0.10$  &               $86.73 \pm 0.13$  &  $\vsecondSig{86.98} \pm 0.08$  &  $86.37 \pm 0.65$  \\
                                                  %& $5\%$  &  $83.60 \pm 0.14$  &  $83.96 \pm 0.10$  &  $\vsecondSig{88.09} \pm 0.25$  &  $\vfirstSig{90.28} \pm 0.08$  &               $87.59 \pm 0.10$  &               $87.35 \pm 0.06$  &  $86.64 \pm 0.66$  \\
                                                  %& $7\%$  &  $85.24 \pm 0.10$  &  $85.69 \pm 0.12$  &  $\vsecondSig{88.96} \pm 0.16$  &  $\vfirstSig{90.70} \pm 0.08$  &               $88.45 \pm 0.07$  &               $87.63 \pm 0.04$  &  $86.55 \pm 0.37$  \\
                                                  & $9\%$  &  $86.43 \pm 0.07$  &  $86.83 \pm 0.06$  &  $\vsecondSig{89.47} \pm 0.10$  &  $\vfirstSig{91.07} \pm 0.09$  &               $89.10 \pm 0.07$  &               $87.92 \pm 0.10$  &  $87.20 \pm 0.65$  \\
                                                  & $15\%$ &  $88.33 \pm 0.06$  &  $88.76 \pm 0.06$  &               $90.49 \pm 0.06$  &  $\vfirstSig{91.56} \pm 0.05$  &  $\vsecondSig{90.53} \pm 0.10$  &               $88.58 \pm 0.15$  &  $88.10 \pm 0.56$  \\
                                                  & $20\%$ &  $89.19 \pm 0.06$  &  $89.65 \pm 0.06$  &               $90.93 \pm 0.05$  &  $\vfirstSig{91.86} \pm 0.04$  &  $\vsecondSig{91.38} \pm 0.05$  &               $88.96 \pm 0.15$  &  $88.80 \pm 0.58$  \\
                                                  & $25\%$ &  $89.76 \pm 0.06$  &  $90.21 \pm 0.05$  &               $91.75 \pm 0.05$  &  $\vfirstSig{92.07} \pm 0.08$  &  $\vsecondSig{91.95} \pm 0.07$  &               $89.42 \pm 0.20$  &  $89.37 \pm 0.63$  \\
                                                  & time (ms)   &              32 &              7.1 &                            1226 &                           2341 &             116838 &             121530 &              129 \\
\midrule
\multirow{7}{*}{\rotatebox[origin=c]{90}{\kiw{}}} & $3\%$  &  $73.91 \pm 0.37$  &  $73.93 \pm 0.58$  &  $80.44 \pm 1.05$  &  $86.95 \pm 0.19$  &  $\vsecondSig{87.72} \pm 0.32$  &               $87.02 \pm 0.05$  &   $\vfirstSig{87.88} \pm 0.01$  \\
                                                  %& $5\%$  &  $77.89 \pm 0.21$  &  $78.22 \pm 0.20$  &  $82.32 \pm 0.46$  &  $87.65 \pm 0.08$  &   $\vfirstSig{88.40} \pm 0.22$  &               $87.24 \pm 0.05$  &  $\vsecondSig{87.90} \pm 0.01$  \\
                                                  %& $7\%$  &  $79.65 \pm 0.20$  &  $80.34 \pm 0.15$  &  $83.43 \pm 0.23$  &  $86.72 \pm 0.73$  &   $\vfirstSig{88.71} \pm 0.13$  &               $87.37 \pm 0.05$  &  $\vsecondSig{87.53} \pm 0.35$  \\
                                                  & $9\%$  &  $80.87 \pm 0.09$  &  $81.61 \pm 0.09$  &  $83.97 \pm 0.27$  &  $86.53 \pm 0.20$  &   $\vfirstSig{89.02} \pm 0.07$  &  $\vsecondSig{87.49} \pm 0.07$  &               $87.23 \pm 0.38$  \\
                                                  & $15\%$ &  $82.57 \pm 0.07$  &  $83.38 \pm 0.06$  &  $85.03 \pm 0.11$  &  $86.97 \pm 0.29$  &   $\vfirstSig{89.67} \pm 0.05$  &  $\vsecondSig{87.76} \pm 0.04$  &               $87.12 \pm 0.33$  \\
                                                  & $20\%$ &  $83.34 \pm 0.06$  &  $84.11 \pm 0.09$  &  $85.48 \pm 0.13$  &  $87.02 \pm 0.24$  &   $\vfirstSig{90.00} \pm 0.07$  &  $\vsecondSig{87.92} \pm 0.07$  &               $87.12 \pm 0.21$  \\
                                                  & $25\%$ &  $83.82 \pm 0.08$  &  $84.56 \pm 0.06$  &  $85.81 \pm 0.11$  &  $87.02 \pm 0.16$  &   $\vfirstSig{90.29} \pm 0.04$  &  $\vsecondSig{88.15} \pm 0.07$  &               $87.24 \pm 0.20$  \\
                                                  & time (ms)   &                           28 &                           6.6 &                           824 &               2938 &                         103264 &             130036 &              104 \\
\bottomrule
\end{longtable}}
********************************************************
\fi



After training on the two features $1-\htr(u)$ and $1-\hun(v)$, \uslogregp{} has learned three
weights $w_0$, $w_1$ and $w_2$, which allow to predict $y_{u,v}$ according to 
\[ 
  \sgn\Big(\big(w_1(1-\htr(u)\big) + w_2 \big(1-\hun(v)\big) + w_0\Big)~.
\]
This can be rewritten as 
\[
\sgn\Big(\big(1-\htr(u)\big) + w_2'\big(1-\hun(v)\big) - \tfrac{1}{2} -\tau'\Big)~,
\]
with $w_2' = \frac{w_2}{w_1}$ and $\tau' = - \left(\frac{1}{2} + \frac{w_0}{w_1}\right)$~.
	
As shown in \autoref{tab:coeff}, and in accordance with the predictor built out of Equation
\eqref{eq:predictor}, $w_2'$ is almost $1$ on \emph{all datasets}, while $\tau'$ tends to be always
close the fraction of positive edges in the dataset.

\input{all_coeff}
