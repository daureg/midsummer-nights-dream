\subsection{Problem setting and applications}
\label{sub:problem_setting}

Like other clustering frameworks, in \pcc{}, we are given a set of objects and we want to gather
them into groups (called clusters) so that objects belonging to one cluster are similar to each
other while being dissimilar to objects from all the other clusters.

In \pcc{}, we formalize this problem by considering objects as the nodes of a graph $G$, whose edges
weight encode similarity. Namely, in the most general case, for nodes $u$ and $v$, the edge between
$u$ and $v$ is associated with two positive numbers:
$w_{u,v}^+$ denotes the strength of the similarity between $u$ and $w$;
$w_{u,v}^-$ denotes the strength of the dissimilarity between $u$ and $w$.
Note however that in many applications, only one of these two numbers is non zero in which case we
more conveniently set $w_{u,v} = \begin{cases}
	 w_{u,v}^+ & \quad \text{if } w_{u,v}^+ > 0 \text{ and } w_{u,v}^-=0 \\
	-w_{u,v}^- & \quad \text{if } w_{u,v}^+ = 0 \text{ and } w_{u,v}^->0 \\
\end{cases}$

Now consider a clustering \cluster{} of $V$, that is a function from $V$ to $\Nbb{}^{|V|}_{>0}$
that assigns to each node a cluster index. For instance, $\cluster(u) = 3$ means that $u$ belongs
to the third cluster. We can evaluate how \cluster{} fits our clustering paradigm in two ways,
either by the number of \emph{agreements}, that is the weighted number of positive edges inside
clusters plus the weighted number of negative edges across clusters; or by the number of
\emph{disagreements}, that is the weighted number of negative edges inside clusters plus the
weighted number of positive edges across clusters. Given a cost function $c$, which is usually the
identity, \pcc{} can then be seen as graph optimization problem, either of maximizing agreements
(\maxa{}):
\begin{equation}
	\max_{\cluster{}} \sum_{\cluster(u) = \cluster(v)} c(w_{uv}^+) +
	\sum_{\cluster(u) \neq \cluster(v)} c(w_{uv}^-)
	\label{eq:maxa}
\end{equation}
or minimizing disagreements (\mind{}):
\begin{equation}
	\min_{\cluster{}} \sum_{\cluster(u) = \cluster(v)} c(w_{uv}^-) +
	\sum_{\cluster(u) \neq \cluster(v)} c(w_{uv}^+)
	\label{eq:mind}
\end{equation}

Although an optimal clustering $\cluster^\star$ achieves the same value on both \eqref{eq:maxa}
and \eqref{eq:mind}, we will see in \autoref{sub:state_of_the_art} that the latter objective is in
some sense \enquote{easier}. Another interesting feature of the \pcc{} problem is that contrary to
other clustering formulations, it does not require us to set the number of clusters $k$ beforehand.
Instead, $k$ emerges as a natural property of the solution. Since clustering is an unsupervised
problem, this is generally handy. However, in some situations, we may have prior knowledge on how
many clusters are the data, or external constraints. This can be handled with parametrized version
of \pcc{}.
% \autocites{Giotis2006}{Fomin2014}\marginpars{According to many articles, \pcc{} and
% cluster editing are equivalent, but even on complete graphs or on $(V,E^+)$ I don't see why.}.
% The cluster edit distance of a graph is the smallest number of edges to change for it to admit a
% perfect clustering (i.e., a union of cliques).  Equivalently, it is the cost of the optimal
% correlation clustering. [From KDDTuto14, slides 204]

In \autoref{fig:cc_objectives}, we show a simple instance of \pcc{} and one of its optimal solution.
\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.8\linewidth]{assets/tikz/cc_objectives_tikz.pdf}
	\caption[Small example of \pcc{}]{A small graph with eight nodes and ten edges. Solid edges
	represent positive edges and dashed edges represent negative edges. A clustering \cluster{}
	is showed with 3 clusters: $\{1, 2, 4\}$, $\{3, 5, 6\}$ and $\{7, 8\}$. \cluster{} incurs two
	disagreements: the negative edge between nodes $1$ and $2$ within the blue cluster, and the
	positive edge $6,7$ between the orange and green clusters. Those disagreements are created
	by two cycles with one negative edge and thus cannot be avoided, meaning that \cluster{} is
	optimal. However it is not the unique solution, for instance merging the orange and green
	clusters would also yields two disagreements.}
	\label{fig:cc_objectives}
\end{figure}

\subsubsection{Applications}
\label{ssub:cc_applications}

According to \textcite[Section 5]{Demaine2006}, \pcc{} is well suited to several situations:
\begin{itemize}
   \item when the items to be clustered do not belong to a natural metric space (preventing
      approaches such as $k$-means) but we still know for some pairs whether they are similar or
      not.
   \item when we do not know the number of clusters beforehand but we have a similarity measure. In
      that case, we can select a problem-specific similarity threshold and set all edges with a
      similarity larger than the threshold to be positive while the others are set to negative.
   \item when we have a classic clustering problem (that is a set of objects, a distance between
      them and an objective function to minimize) with additional pairwise constraints of the form
      \emph{must-link/cannot-link}. Instead of restraining a clustering algorithm to the space of
      feasible solution, we convert the distances between objects and the constraints into signed
      edges and solve the resulting \pcc{} problem. 
\end{itemize}

In addition to these general considerations, \pcc{} has also been used in several domains:
%TODO turn those items into paragraphs?
\begin{description}
   \item[Computer Vision]
      The ubiquitous task of segmenting an image into homogeneous regions is prerequisite for many
      further processing, and can be tackled with \pcc{}. For instance, to segment cell in
      microscopy imagery, \textcite{CellSeg14} first use generic image features to classify pixel in
      belonging to region boundaries or not. Then, they extract small scale regions called
      superpixels. After building the adjacency graph of these superpixels, they assigned edge
      weight by averaging the boundary probability over the pixels separating the adjacent
      superpixels. They also add strong negative constraints between distant superpixels, and lastly
      cluster these superpixels according to the \pcc{} objective to obtain the final segmentation.
      A similar approach was used earlier in~\textcite{Kim2011}, which stressed the importance of
      considering such higher order constraints between distant superpixels in order to achieve good
      performance. This was also extended to 3D segmentation~\autocite{VolumeSegmentation12}, where
      additional tunings allowed to segment a volume image of a mouse cortex with up to billions
      voxels. \Textcite{Beier2015} segment 2D and 3D images with a energy based formulation of
      \pcc{} and iteratively improve their solution by merging it with another clustering given by a
      proposal generator. By developing another scalable energy based optimization procedure, and
      with the help of few user provided cues, \textcite{Bagon2011} were able to apply \pcc{}
      directly at the pixel level.
      % one more? https://link.springer.com/chapter/10.1007/978-3-642-33783-3_41

      Beyond image segmentation, \textcite{Shape3D17} develop a method to extract a network of
      descriptive curves from 3D shapes.  After an initial stage of generating many such
      \emph{flowlines}, they describe in Section 6 a \pcc{} formulation to extract \emph{reliable}
      representative flowlines, using geometric constraints to obtain positive or negative cues that
      two flowlines are from the same reliable representative.

      Finally, in order to track several targets across sequential video frames,
      \textcite{multiTracking15} propose a multistage framework. One step revolves around a matrix
      $A$ that defines the cost of assigning an object tracked in previous frames to an object
      detected in the current frame. This matrix is turned into a symmetric affinity matrix
      $\bar{A}_{sym}$ that can be seen as a signed graph. \pcc{} is then used to extract clusters
      (called zones), in which local processing is performed, whose complexity can be adapted to the
      difficulty of each zone.

   \item[Natural Language Processing] 
      Coreference resolution is the task of finding all expressions that refer to the same entity in
      a text. Like image segmentation, it is a preprocessing step that can later be used in document
      summarization, question answering, and information extraction. \Textcite[Section
      2.3]{graphicalCoreference04} tackle coreference resolution by using a undirected graphical
      model on which performing inference is equivalent to \pcc{}. On small scale instances,
      \textcite{Elsner2009} first obtain an upper bound of the optimal solution by solving a SDP
      relaxation of the problem. They then compared various heuristics and show that best performing
      ones are within few percents of the optimum, provided they are followed by a local search
      step, such as the Best One Element Move~\autocite{Gionis2007}. Another possible task would be
      to cluster words based on distributional embedding vector while adding antonym
      constraints~\autocite{SignedWordRatings}, although the authors of that paper choose to use
      signed spectral clustering.

   \item[Biology]
      The input is a similarity matrix between gene expressions in various experimental conditions
      and the goal is to cluster those genes into groups which react similarly. \Textcite[Section
      4]{Ben-Dor99} gives three examples: 112 genes involved in the rat central nervous system, 1246
      genes of the roundworm \emph{C. elegans} and 2000 human genes obtained from 40 tumor and 22
      normal colon samples. \Textcite{Mason2009} analyze a signed co-expression networks of genes
      involved in embryonic stem cells to find which genes are related to pluripotency (the ability
      to differentiate into any type of cell in the body) or self-renewal (the ability to replicate
      indefinitely). Another application is to study the variation of one individual
      DNA~\autocite{Das2015}. In the human organism, chromosomes are organized in pair, and both
      chromosomes of a pair have similar but not identical DNA sequences. This is mostly because of
      single nucleotide polymorphisms (SNPs), where a single base differs between the two DNA
      sequences, leading to different alleles of the corresponding gene. An haplotype is the list of
      all alleles at a contiguous sites in a region of a single chromosome and this information is
      used in several medical applications. The high-throughput sequencing of one individual genome
      yields many short \emph{reads} that provides information about the order of nucleotides in a
      fragment of one chromosome and that can be used to assemble haplotypes. Namely the authors
      build a graph of reads and define a similarity function between reads to assign weight on the
      edges. The clusters of that graph correspond to haplotypes, and are obtained by a SDP
      relaxation of the \pcc{} objective. \Textcite{monotoneBiology07} also consider graphs whose
      nodes are genes, but in a different context. In this case, positive edges represent an
      activating connection, while negative edges represent inhibiting connection. They also define
      a \emph{monotone system} as a balanced subgraph, that is a subgraph which does not contain a
      cycle with an odd number of edges. Such monotone system are stable, in the sense that
      modifying the concentration of one gene will have a predictable effect, even ignoring the
      precise kinematics of the chemical reactions involved. Their goal is to find the minimum
      number of edges to remove in order to decompose a dynamics system into a collection of
      monotone system. This corresponds to the \pcc{} \eqref{eq:mind} objective and allows to study
      the complete system more easily.
      % \enquote{They examined dynamical systems, where a gene is modeled as a vertex and an
      % activating connection is modeled as a positive edge and an inhibiting connection is modeled as
      % a negative edge. The claim is that biological dynamical systems are close to being balanced,
      % and that finding a minimum set of edges to delete to make the graph balanced can be used to
      % decompose the graph into “monotone subsystems”, which exhibit stable behavior and thus allow a
      % better understanding of the dynamics of a system.}\autocite{monotoneBiology07}

   \item[Network science]
      As mentioned in the first Chapter, one early use of signed graphs was to model social
      interactions and given the large body of work on community
      detection~\autocite{FortunatoSurvey10}, it is therefore not surprising that \pcc{} has been
      used to study such networks. For instance, one can extract all the votes of the member of
      political parliament and form a graph whose node are politicians and edge weight quantify how
      much they agree or disagree on various issues they have been voting on. This can be used to
      study various social science questions such as loyalty, leadership, coalitions, political
      crisis and polarization. It has been applied to the European
      parliament~\autocite{Mendonca2015}, Slovenian parliament~\autocite{Jiang2015} and the
      Brazilian parliament~\autocites{BrazilCC17}. This can also be used at international level. For
      instance, by considering a dataset of military alliances and disputes, \textcite{Traag2009}
      cluster countries into blocks that resemble those identified by Huntington in his \emph{Clash
      of Civilizations} book. Another source of data is the vote on resolutions during the United
      Nations General Assembly~\autocite{CommunityUN12}. Finally, one can also study how to exploit
      the information contained within the negative links to enhance the visualization of social
      networks~\autocite{Luca10}. Note however that those three last papers not explicitly use a
      \pcc{} formulation.

   \item[Others]
      \begin{itemize}[leftmargin=*]
	 \item
	    Deduplication, also called duplicate detection or entity resolution, is the process
	    of identifying objects from a real-world, noisy database that refer to the same entity.
	    On a high level, a solution to this problem is to build a graph of all the available
	    objects, define a similarity between them and run a \pcc{} algorithm. This was indeed
	    this kind of problems at Whizbang! Labs that partly motivated one of the early \pcc{}
	    paper~\autocite{Bansal2002}. The main challenge
	    thus lies in devising an appropriate similarity measure, given that object can have very
	    different features from one database to another. \Textcite{LargeScaleDeDup09} propose a
	    declarative language, expressing both hard constraints (that have to be satisfied) and
	    soft constraints (that can be seen as cues guiding the process). Because of these hard
	    constraints that admissible clusterings have to respect, the authors have to modify in
	    non trivial ways an existing \pcc{} algorithm. Another example is given by
	    \textcite{Crosslingual07}, who cluster together news article in different language
	    covering the same event. \pcc{} was also evaluated among other
	    solutions to that problem by \textcite{DeDup09}, who note that their non optimized
	    implementation does not perform the best.
	 \item 
	    Given an electrical circuit layout, \textcite{circuitDesign07} extract a graph of its
	    components (called shifter) that must be assigned one of two possible phases. Because
	    two shifters next to some specific shape must be in opposite phase and two shifters
	    separated by less than a specified distance must be of the same phase, the authors look
	    for a two-clustering of the nodes that will minimize the number of disagreements.
      \end{itemize}
\end{description}

\subsection{Relation with edge sign prediction}
\label{sub:relation_with_edge_sign_prediction}

\input{cc_sign_relation}

\subsection{State of the art}
\label{sub:state_of_the_art}

\input{cc_approx}

% ICML paper this year that touch something very related multicut and give recent applications in
% vision https://arxiv.org/abs/1503.03791

\subsection{Variants and extensions}
\label{sub:variants_and_extensions}

\input{cc_variants}

\subsubsection{\pcc{} under stability assumption}
\label{ssub:cc_under_stability_assumption}

\iffalse
Haris Angelidakis, Konstantin Makarychev, and Yury Makarychev. 2017.
Algorithms for Stable and Perturbation-Resilient Problems. STOC’17
\href{http://ttic.uchicago.edu/~yury/papers/two-stable.pdf}{10.1145/3055399.3055487}
improves over the one cited in the internship description

\subsubsection{Parallel \pcc{}}
\label{ssub:parallel_cc}

\subsection{Empirical evaluation?}
\label{sub:cc_empiracal_evaluation}
\fi
