% leftover:
% - something about fuzzy edges, ie we're not sure they are in the graph or not (not super
%   interesting in my opinion) Clustering with partial information, Theoretical Computer Science,
%   Volume 411, Issue 7, 2010, Pages 1202-1211, ISSN 0304-3975,
%   \url{http://dx.doi.org/10.1016/j.tcs.2009.12.016}
% - \paragraph{Hypergraphs} \Textcite{Kim2011} show a LP relaxation on hypergraph and
%   \textcite{Ricatte13} describe a class of hypergraphs that can be reduced to signed graph.  Jörg
%   Hendrik Kappes, Markus Speth, Gerhard Reinelt, Christoph Schnörr, Higher-order segmentation via
%   multicuts, Computer Vision and Image Understanding, Volume 143, 2016, Pages 104-119, ISSN
%   1077-3142, \url{http://dx.doi.org/10.1016/j.cviu.2015.11.005}
% - Interactive \pcc{}?~\autocite{interactiveCC16}, could it go into active?  Geerts, F. & Ndindi,
%   R. Bounded correlation clustering. Int. J. Data Sci. Anal. 1, 17–35 (2016)
%   \url{https://doi.org/10.1007/s41060-016-0005-2}

So far we focused on \pcc{} in its rawest form, that is solving the \mind{} and \maxa{} objectives
in the case where the general binary-labeled graph is known in advance. We will now see first some
special cases, namely when the weights obey the triangle inequality (to solve \msc{}) or when the
graph is bipartite and then move to variants. We will consider more general objectives, when the
edges are labeled categorically instead of binary, when nodes can belong simultaneously to several
clusters or when we optimize local objectives per nodes instead of global ones. Finally we will also look at
clustering in signed graphs in general, using spectral methods or heuristics from the community
detection literature.

% \paragraph{\msc{}}                                | special weights
% \paragraph{Bipartite \pcc{}}                      | special graphs
% \paragraph{Categorical edge labelling}            | more general objective
% \paragraph{Overlapping \pcc{}}                    | more general objective
% \paragraph{Local \pcc{}}                          | more general objective or at least different
% \paragraph{Spectral clustering}                   | related objective
% \paragraph{Communities detection}                 | related objective

% \paragraph{Online \& active setting}              | different setting
% \paragraph{recovery under noise}                  | non worst case analysis
% \subsubsection{\pcc{} under stability assumption} | non worst case analysis

\paragraph{\msc{}} 

In \msc{}, the goal is to output a clustering which best summarizes (or agrees with) the several
given input clusterings of the same set of objects.  Motivations include robustness ---by using an
ensemble of clusterings from diverse methods--- and privacy ---if the clusterings were computed by
different parties each considering only a subset of the objects attributes. We can build the
complete graph of these objects, with weights set to the fraction of clusterings that place two
objects in different clusters, thus representing a kind of distance in the space of clusterings.  As
first show by \textcite{EarlyConsensusClustering03} , finding the optimal clustering is therefore
an instance of \pcc{} where the weights obey the triangular inequality. \textcite{Gionis2007} give a
deterministic $3$-approximation using the \regionGrow{} method. Later \textcite{Bonizzoni2008} show that
the minimization version is \APXh{}, even when the input is made of three clusterings and give a
combinatorial $\nicefrac{4}{5}$-approximation for the maximization problem. Experimental evaluations are
conducted by \textcite{Bertolacci07} and \textcite{Filkov08}. The former describe a scalable
approach that first samples a small portion of the data, runs a (potentially computationally
expensive) approximation algorithm and finally augment the resulting partition by adding to it the
unsampled nodes one by one.  Experiments confirm that the running time is greatly improved compared
with the linear program methods while the resulting objective value is essentially the same. Note,
however, that LP methods can be applied in practice thanks to some tricks~\autocite{ConsensusLP10}.
If we have $k$ input clusterings $\cluster^1,\ldots,\cluster^k$ and we parameterized the problem by
$t$, which is the sum over the input clusterings of the number of pairs of objects that are clustered
differently by a solution $\cluster^\star$ and $\cluster^i$, then there is a polynomial algorithm
running in $O(4.24^{\nicefrac{t}{k}}\cdot \nicefrac{t}{k}^3 +
kn^2)$~\autocite{parameterizedConsensus14}.

\paragraph{Bipartite \pcc{}}

Bipartite graphs are an interesting special case for \pcc{}, as they often appear in the context of
recommendation systems, where users rate products positively or negatively, although in this setting
we cannot expect to have complete bipartite graphs in practice. The first results was given by
\textcite{Amit04}, who obtains an $11$-approximation for \mind{} by adapting the \regionGrow{} method.
The \ccpivot{} is adapted to the bipartite case by
\textcite{Bipartite12}, who prove it results in a randomized $4$-approximation (and provide a matching
deterministic approximation by rounding a LP). By using their idea of rounding the results of the LP
differently for positive, negative and in that case same-side absent edges, \textcite{Chawla2014}
bring down this approximation factor to $3$, even for $K\geq 2$-partite graphs. Through formulating the
\maxa{}$[k]$ problem as a bilinear maximization problem and computing a low-rank approximation of
the graph biadjacency matrix, \textcite{Asteris2016} obtain a efficient PTAS, that is a $(1-\delta)$
approximation running in time exponential in $k$ and $\delta^{-1}$ but linear in $n$. By an
appropriate choice of $k$, it is possible to use this PTAS to solve the general \maxa{} problem.

\paragraph{Categorical edge labelling}

In the so called \textsc{Chromatic}-\pcc{} setting, \enquote{positive} edges are now associated with
one of $L$ possible colors and the goal is to form clusters mostly made up of edges with the one same
color. Namely, a disagreement is now a negative edge between clusters or a within-cluster edge
whose color differs from the majority color of that cluster. This is motivated by edge-labeled graph
in social networks, biology and citation networks and will discuss such applications in
\autoref{chap:vector}. As a generalisation of \pcc{}, it is \NPc{} but \textcite{Bonchi2012a}
present a modification of the \ccpivot{} algorithm that pick edges instead of nodes as pivots, and
grow clusters by adding monochromatic triangles. This gives an approximation factor of six times the
maximum degree of the graph. They also present a method when the number $k$ of clusters is fixed
beforehand, starting with an initial partition and then alternating between finding the majority
color of clusters and finding better clusters. An improved heuristic algorithm is given in
\url{http://ieeexplore.ieee.org/document/7732322/}. Unfortunately, the maximum degree of a graph can be
as large as $n$. However, \textcite{Anava2015} present constant factor approximations. Namely, they
show the problem can be reduced to classical \pcc{} by setting all edges incident to a node $u$ to
negative if they are not of the majority color of $u$. They then apply the regular \ccpivot{} and
show this gives an $11$-approximation to the original problem. Furthermore, they also write a linear
program and round it using the \regionGrow{} method to obtain an
approximation factor of $4$. \Textcite{multiChromatic15} extend this line of work to the case were a
single edge can carry a \emph{set} of labels and adapt their randomized algorithm so that the
approximation factor is multiplied by the size of the input label set.

\paragraph{Overlapping \pcc{}}

While in \pcc{}, each node is assigned to a single cluster, in other settings we may want to relax
this constraint. Given a complete weighted graph, \textcite{Bonchi2012} want to output a clustering
\cluster{} that minimizes the following cost: \[ \sum_{(u,v)\in E} \left| H(\cluster(u),
\cluster(v)) - w_{uv}\right|\] where $H$ is a similarity function between two sets of labels, chosen
here to be the Jaccard similarity or a $0/1$ indicator of nonempty intersection. These problems are
showed \NPc{} and approximated by a local search algorithm, iteratively optimizing the assignment of
one node while all others are fixed. As one of the demonstration on their theoretical work,
\textcite{WeightedTheta15} show a faster solution based on a weighted extension of the Lovász's
theta function, the corresponding geometric embedding of graphs and a solver derived from one-class
SVM, while \textcite{GeneticOCC14} propose a genetic algorithm to solve this problem. Finally,
\Textcite{StochasticCC13} also deals with overlapping clustering by relaxing the problem to a
stochastic setting and using \enquote{the Baum-Eagon inequality, which provides an effective
iterative scheme for maximizing polynomial functions in probability domains}.

\paragraph{Local \pcc{}}

In classical \pcc{}, all nodes have an identical role, in the sense that they contribute equally to
the final objective in terms of (dis)agreements. Here we instead look at approaches where we either
add a local penalty to each in order to better control their behavior or where we altogether modify
the objective to focus on (dis)agreements at specific nodes.

% that one is not really local, except for the node penalty
\Textcite{Puleo2014} adapt the linear program of~\autocite{Charikar2003} and its \regionGrow{}
method to the case
where all clusters have to contain less than $K$ nodes, by assigning to each node $u$ a penalty
$\mu_u$. If $u$ is placed in a cluster $C_i$, the original \mind{} objective is penalized by an
extra $\mu_u\left(|C_i| - (K+1)\right)$. By varying $\mu_v$ between $0$ and $1$ and because the
positive weights are assumed to smaller than $1$, this cluster size constraint can be made hard or
soft. They also handle more general weights, since they allow $w^-_{uv}$ to be as large as $\tau$
for $\tau\in [1,\infty)$ while still guaranteeing a $5-\nicefrac{1}{\tau}$-approximation on complete
graphs, and adapt \ccpivot{} to unweighted graphs with the hard cluster size constraint, obtaining a
randomized $7$-approximation.
These soft constraints are for instance used in a biological application where nodes are genes and
where singleton and giant clusters are uninformative~\autocite{Hou2016}

\Textcite{pmlr-v48-puleo16} also modify the \mind{} objective to make it more general. Based on the
classic \pcc{} linear program, they define a \enquote{\emph{fractional clustering} of $G$ as a
vector $x$ indexed by $V$ such that $x_{uv} \in [0, 1]$ for all $uv \in \binom{V}{2}$ and such that
$x_{vz} \leq x_{vw} + x_{wz}$ for all distinct $v, w, z \in V$}. They also define \enquote{The
error vector $err(x)$ of $x$, as a real vector indexed by $V$ whose coordinates are}
\begin{equation*}
  \mathrm{err}(x)_u = \sum_{v\in\nei^+(u)} x_{uv} + \sum_{v\in\nei^-(u)} (1-x_{uv})
\end{equation*}

Given a function $f: \Rbb^n_{\geq 0} \rightarrow \Rbb$ verifying two elementary conditions, the
problem is then to find a fractional clustering $x$ minimizing $f(err(x))$. The classical \pcc{}
corresponds to setting $f(x) = \lhalf{}\ell^1(x)$ whereas the authors here are interested in Minimax
\pcc{} that arises by setting $f(x) = \ell^\infty(x)$. Minimizing the maximum number of
disagreements incurred by a single node is motivated by the example of recommendation systems: if
errors correspond to unsatisfying recommendations, we do not want a single user to suffer many
of them. Minimax \pcc{} is \NPc{} on both complete graphs and complete bipartite graphs but
by modifying the \regionGrow{} method, the authors respectively a $48$ and
$10$ approximation, the latter for the one-sided error (that only counts disagreements for the nodes
in one of the two clusters).
The idea is to chose pivots not randomly but by maximizing a given criteria and
to grow balls with a radius $\alpha$ computed numerically to optimize the approximation factor.
Interestingly, and in contrast with the classic \pcc{} situation, minimax \maxa{} is not easier than
minimax \mind{} and seems not to have a constant factor approximation, even on complete graphs.
Furthermore, these algorithms are deterministic, as opposed to many \pcc{} approximations, since
bounds on expected disagreements of an edge does not translate easily on their maximum.
\Textcite{Charikar2017} improve these two factors to $7$, using a simpler version of the algorithm
of \textcite{pmlr-v48-puleo16}. Namely, find the ball of radius $\nicefrac{1}{7}$ with the largest
number node and create a cluster from its center with a radius of $\nicefrac{3}{7}$. They also show
that on general weighted graphs, the LP has a large integrality gap of $\nicefrac{n}{2}$ yet they
combine it with a combinatorial approach to reach a $O(\sqrt{n})$ approximation. Finally, they
consider the complementary problem of maximizing the minimum number of agreements reach at a single
node, and provide a $\frac{1}{2+\epsilon}$ approximation.

\paragraph{Spectral Clustering}
\label{par:cc_spectral}

A classic method for clustering graphs is to leverage their spectral properties. Namely, if $A$ is
the adjacency matrix of $G$ and $D$ its degree diagonal matrix (that is $D_{u,u} = \degr(u)$), the
\emph{Laplacian} of $G$, defined by $L_G = D - A$, is a symmetric positive semidefinite matrix. As
such, it has $n$ real non-negative eigenvalues, and its spectrum provides additional information on
the connectivity of $G$. For instance, $0$ is always the smallest eigenvalue and its multiplicity is
equal to number of connected components of $G$, while ---if $G$ is connected--- the second
eigenvalue is the algebraic connectivity of $G$, whose magnitude is an indication of how well
connected is the graph. This matrix is typically used for clustering by computing its first $k$
eigenvectors, which embed the $n$ nodes of $G$ in $\Rbb^k$, where there are then clustered with the
$k$-means algorithm. This can be seen as a relaxation of the discrete \rcut{} objective, which asks
for the partition $\{C_1, \ldots, C_k\}$ minimizing $\frac{1}{2}\sum_{i=1}^k \frac{\mathrm{cut}(C_i,
\bar{C_i})}{|C_i|}$, where $\bar{C_i}$ is the complement of $C_i$ in $V$ and $\mathrm{cut}(B, C) =
\sum_{u\in B, v\in C} w_{uv}$ is the total weight of the edges between $B$ and
$C$~\autocite{tutoSpectralClustering07}. By considering the symmetric normalized Laplacian
$L_{sym}  = D^{\nicefrac{1}{2}}LD^{\nicefrac{1}{2}}$, it is also possible to approximate the
normalized cut objective (\ncut{}), where $|C_i|$ is replaced by $vol(C_i) = \sum_{u\in C_i}
\degr(u)$. We will now see how these kinds of approaches can be extended to signed graphs, noting
first that they require to fix the number of clusters beforehand and are looking for clusters
balanced in size, which makes the problem related but not equivalent to \pcc{}.

The first line of research consider only \mind{}$[2]$. For instance, \textcite{NcutAnd2CC08} show
that both normalized cut and \mind{}$[2]$ objectives can be written as a SDP (or equivalently as
eigenvalue problems) and thus combined, the intuition being that we look for \ncut{} solutions whose
number of disagreements is not too much more than the approximate optimal.

Letting \ncut{}$(C, \bar{C}) = \frac{\mathrm{cut}(C, \bar{C})}{\mathrm{bal}(C)}$ with
$\mathrm{bal}(C) = 2\frac{vol(C)vol(\bar{C})}{vol(V)}$, \textcite{mOneCC12} define a new objective:
\begin{equation*}
  \hat{F}_\gamma(C) = \frac{\mathrm{cut}(C, \bar{C}) + \gamma\left(\hat{M}(C)+\hat{N}(C)\right)}{\mathrm{bal}(C)}
\end{equation*}
where $\gamma \in \Rbb{}_+$ is a parameter, while $\hat{M}(C)$ and $\hat{N}(C)$ are respectively the
number of positive and negative disagreements of the $(C, \bar{C})$ clustering.
They show how to optimize a tight continuous relaxation of $\hat{F}_\gamma$ as the non-negative
ratio of a difference of convex function and a convex function.

On the other hand, one can also adapt these two cut objectives to directly include negative edges.
\Textcite{Luca10} define the signed Laplacian as $\bar{L} = \bar{D} - A$, where $\bar{D}$ is the
signed degree matrix such that $\bar{D}_{uu} = \sum_{v\in \nei(u)} |A_{uv}|$, as well as a signed
variant of the symmetric normalized Laplacian $\bar{L}_{sym}  = \bar{D}^{\nicefrac{1}{2}} \bar{L}
\bar{D}^{\nicefrac{1}{2}}$. They show that the signed Laplacian is positive semidefinite, and
even positive-definite as soon as the graph is unbalanced (that is contains a cycle with an odd
number of negative edges). From positive and negative cuts defined as $\mathrm{cut}^+(B, C) =
\sum_{u\in B, v\in C} w^+_{uv}$ and $\mathrm{cut}^-(B, C) = \sum_{u\in B, v\in C} w^-_{uv}$, a
natural signed cut is $\mathrm{scut}(B, C) = 2\mathrm{cut}^+(B, C) + \mathrm{cut}^-(B, B) +
\mathrm{cut}^-(C, C)$ which can then be used to defined signed \rcut{} and \ncut{}. Arguing that those
definitions force negatively linked nodes to be symmetric around the origin, do not take into
account the balance of negative edges in each cluster and are difficult to extend to more than two
clusters, \textcite{SignedEmbedding15} instead propose two new normalized cuts:
\begin{align*}
  SNScut(C_1, \ldots, C_k) &= \sum_{i=1}^k
  \frac{\mathrm{cut}^+(C_i, \bar{C_i})-\mathrm{cut}^-(C_i, \bar{C_i})}{vol(C_i)}  \\
  BNScut(C_1, \ldots, C_k) &= \sum_{i=1}^k
  \frac{\mathrm{cut}^+(C_i, \bar{C_i})-\mathrm{cut}^-(C_i, \bar{C_i})+vol^-(C_i)}{vol(C_i)}
\end{align*}

Noting that if $x_i\in\Rbb^n$ is the vector indicator of cluster $C_i$ (that is the \uth{} entry of
$x_i$ is $1$ is $u$ belongs to $C_i$ and $0$ otherwise), $x_i^T\bar{L}x_i = 2\mathrm{cut}^-(C_i,
C_i) + \mathrm{cut}^-(C_i, \bar{C}_i) + \mathrm{cut}^+(C_i, \bar{C}_i)$,
\textcite{mSemanticWordCC17} introduce the following cut objective:
\begin{equation*}
  sNcut(C_1, \ldots, C_k) = \sum_{i=1}^k
  \frac{2\mathrm{cut}^-(C_i, C_i) + \mathrm{cut}^-(C_i, \bar{C}_i) + \mathrm{cut}^+(C_i, \bar{C}_i)}{vol(C_i)}
\end{equation*}
Additional cut formulations for $k$-clusters are also presented in~\autocite{moreSignedCut12},
although \textcite{Knyazev2017l} argue that using the non signed Laplacian and considering negative
eigenvalues might be just as effective, citing for instance numerical instability of signed
Laplacian.

Finally, \textcite{mGeometricMean16} show that the Laplacians defined so far can be seen as
arithmetic means of the Laplacian $L^+$ of the positive subgraph $G^+=(V, E^+)$ and the signless
Laplacian $Q^-$ of the negative subgraph $G^-=(V, E^-)$, where $Q^- = D^- + A^-$. They suggest
instead to use a geometric mean defined for two positive matrices $A$ and $B$ as $A\#B =
A^{\shalf{}} \left(A^{-\shalf{}} BA^{-\shalf{}} \right)^{\shalf{}} A^{\shalf{}}$. This suggestion is
based on the fact that if $u$ is a common eigenvector of both $A$ and $B$ with eigenvalue $\lambda$
and $\mu$ respectively, then $u$ is an eigenvector of $A+B$ with eigenvalue $\lambda+\mu$ and an
eigenvector of $A\#B$ with eigenvalue $\sqrt{\lambda\mu}$. Therefore, the $k$ smallest eigenvalues
of the geometric mean Laplacian will be influenced by both smallest eigenvalues of $L^+$
(corresponding to assortative clusters in $G^+$) and of $Q^-$ (corresponding to disassortative
clusters in $G^-$), while this is note the case for the arithmetic mean of Laplacians.

\paragraph{Community detection}

The clustering problem is often named community detection in the context of social networks, and
several methods developed by practitioners have been extended to signed graphs. While they do not
necessarily considered the \pcc{} objective, and especially not its optimum, we still give a brief
overview of them, as they tend to have been more tested experimentally. For instance, to find the
cluster of node $u$, \textcite{Yang2007} use a random walk approach on the positive subgraph $G^+$
to compute the probability of each node to reach $u$ in $T$ steps, sort the nodes accordingly and
then find a threshold  based on the number of disagreements.
The one node moves local heuristic that we described in the Physics-inspired paragraph
\vpageref{par:cc_physics} can also be formulated as genetic algorithms that simultaneously try to
minimize the number of disagreements and maximize a signed variant of the
modularity~\autocites{Li2013}{Amelio2013}. \Textcite{Anchuri2012} also consider these two objectives
by seeing them as eigenvalue problems and devise a iterative splitting procedure.
The overlapping community detection variant is considered by \textcite{Chen14}, who used a signed
probabilistic mixture model. Namely, an edge selects a pair of cluster $r,s$ with probability
$\omega_{rs}$ (where $r=s$ if the edge is positive and $r\neq s$ otherwise) and chooses its endpoints
$u$ and $v$ with probability $\theta_{ru}$ and $\theta_{sv}$. $\theta_{ru}$ is therefore the soft
membership of node $u$ to cluster $r$, and those parameters are estimated using the
expectation-maximization algorithm. The same model is extended to directed graphs by
\textcite{Jiang2015}, who strangely enough names it stochastic blockmodel, although the focus is
still on edge and not nodes.
In a similar spirit to \maxa{}$[k]$, \textcite{SignedGang} focus on finding $k$ subgraphs dense in
positive edges and densely connected by negative edges to each other. They dub such subgraph
\emph{Oppositive Cohesive Groups}, or more vividly \emph{Gangs in War}, and after formulating the
problem as a constrained quadratic optimization, they propose a faster iterative local search
heuristic. When $k=2$, these subgraphs are called antagonistic communities and a specific data
mining approach was proposed by \textcite{quasiGang}.
% relaxed structural balance \autocite{Doreian2009} but used the same approach as \textcite{Early96}
