We now show how to apply the learning bias of \autoref{sec:new_bias} on undirected graphs.
Furthermore, we assume that the strong balance holds, meaning that there are only two groups
according to \autoref{thm:structural}. In other words, the labelling of $E$ is consistent with a
two-clustering of $V$. Namely, $V$ can be partitioned in two clusters such that edges within each
cluster are positive and edges across clusters are negative.
In that case, the following
\emph{multiplicative rule} holds: for any nodes $u$, $v$ in $V$, and any path $p$ between $u$ and
$v$ in $G$, the sign $\yuv$ is equal to the product of the signs along $p$. Hereafter, we call this
product the parity of $p$, and denote it by $\pi(p)$.
While it is a simple and convenient hypothesis, this is too strong
of a requirement to be satisfied in practice. Therefore, we relax it by assuming that, starting from
a consistent labeling $Y$, we can only observe a randomly perturbed version $Y'$ of $Y$.
Specifically, given a constant $q\in [0, \nicefrac{1}{2})$, every sign of $Y$ is flipped with a
probability smaller than $q$. We denote by $E_{\mathrm{flip}} \subset E$ the set of edges whose sign
has been flipped.

In this section, we are interested in active learning algorithms that first query a subset
$\etrain$ of the edges, observe the signs in $\etrain$ and use them to predict the remaining signs.
More precisely, we focus on an algorithm that queries a spanning tree $T$ of $G$ and predicts the
sign of an edge $(u,v) \in \etest = E \setminus E_T$ as the parity of $\pathtuv$. Intuitively, since
each sign has been potentially flipped, the longer the path in $T$, the more likely its parity will
be not be equal to the true sign $\yuv{}$. Therefore we would like each such path to be as short as
possible.
Formally, the number of mistakes of such an algorithm is upper bounded by~\autocite[Equation
(3)]{Cesa-Bianchi2012b}
\begin{equation*}
  |E_{\mathrm{flip}}| + \sum_{(u,v) \in \etest}
  \sum_{e\in E} \Ind{e \in \pathtuv} \Ind{e \in E_{\mathrm{flip}}}
\end{equation*}
which in expectation is equal to:
\begin{equation}
  \label{eq:stretch_mistakes}
  q\left(|E| + \sum_{(u,v) \in \etest} |\pathtuv| \right)
\end{equation}
In the following, we describe a way to build spanning trees tailored for this situation. More
precisely, we implement and analyze a suggestion made to us by~\autocite{gtxFabio}.

\subsection{\gtx{}: a spanning tree designed for sign prediction}
\label{sub:gtx_algo}

\input{gtx_algorithm}

\subsection{Related work}
\label{sub:gtx_related_work}

\input{gtx_related}

\subsection{Empirical evaluation}
\label{sub:gtx_empirical_evaluation}

\input{gtx_experiments}
