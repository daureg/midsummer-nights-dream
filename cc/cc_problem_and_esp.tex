Like other clustering frameworks, in \pcc{}, we are given a set of objects and we want to gather
them into groups (called clusters) so that objects belonging to one cluster are similar to each
other while being dissimilar to objects from all the other clusters.

In \pcc{}, we formalize this problem by considering objects as the nodes of a graph $G$, whose edge
weights encode similarity. Namely, in the most general case, for nodes $u$ and $v$, the edge between
$u$ and $v$ is associated with two positive numbers:
$w_{u,v}^+$ denotes the strength of the similarity between $u$ and $v$, while
$w_{u,v}^-$ denotes the strength of the dissimilarity between $u$ and $v$.
Note, however, that in many applications, only one of these two numbers is nonzero, in which case
we more conveniently set $w_{u,v} = \begin{cases}
	 w_{u,v}^+ & \quad \text{if } w_{u,v}^+ > 0 \text{ and } w_{u,v}^-=0 \\
	-w_{u,v}^- & \quad \text{if } w_{u,v}^+ = 0 \text{ and } w_{u,v}^->0 \\
\end{cases}$

%TODO replace N by [|K|] (and define that notation in the table)

Now consider a clustering \cluster{} of $V$, that is a function from $V$ to $\Nbb{}^{|V|}_{>0}$ that
assigns to each node a cluster index. For instance, $\cluster(u) = 3$ means that $u$ belongs to the
third cluster. We will also abuse the notation and let \cluster{} be a set of clusters $\{C_1, C_2,
\ldots, C_K\}$ that form a partition of $V$.\footnote{That is, $\forall i\,C_i\subset V$,
$\bigcup_{i=1}^K C_i = V$ and $\forall i\neq j,\, C_i\cap C_j = \emptyset$.}
We can evaluate how \cluster{} fits our clustering paradigm in two ways,
either by the number of \emph{agreements}, that is the weighted number of positive edges inside
clusters plus the weighted number of negative edges across clusters; or by the number of
\emph{disagreements}, that is the weighted number of negative edges inside clusters plus the
weighted number of positive edges across clusters. Given a cost function $c$, which is usually the
identity, \pcc{} can then be seen as a graph optimization problem, either of maximizing agreements
(\maxa{}):
\begin{equation}
	\max_{\cluster{}} \sum_{\cluster(u) = \cluster(v)} c(w_{uv}^+) +
	\sum_{\cluster(u) \neq \cluster(v)} c(w_{uv}^-)
	\label{eq:maxa}
\end{equation}
or minimizing disagreements (\mind{})\footnote{Note that in the data mining literature, \pcc{} may
refer to another problem. Namely, it is a special case of high-dimensional data clustering, where
features are locally correlated in various ways across different clusters that reside in arbitrarily
oriented subspaces~\autocite{otherCCproblem09}.}:
\begin{equation}
	\min_{\cluster{}} \sum_{\cluster(u) = \cluster(v)} c(w_{uv}^-) +
	\sum_{\cluster(u) \neq \cluster(v)} c(w_{uv}^+)
	\label{eq:mind}
\end{equation}

Although an optimal clustering $\cluster^\star$ achieves the same value on both \eqref{eq:maxa}
and \eqref{eq:mind}, we will see in \autoref{sub:state_of_the_art} that the latter objective is in
some sense \enquote{easier}. Another interesting feature of the \pcc{} problem is that contrary to
other clustering formulations, it does not require us to set the number of clusters $K$ beforehand.
Instead, $K$ emerges as a natural property of the solution. Since clustering is an unsupervised
problem, this is generally handy. However, in some situations, we may have prior knowledge on how
many clusters are the data, or external constraints. This can be handled with parametrized version
of \pcc{}, namely \maxa$[K]$ and \mind$[K]$ where the optimization is over clustering with exactly
$K$ clusters.

In \autoref{fig:cc_objectives}, we show a simple instance of \pcc{} and one of its optimal solution.
\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.63\linewidth]{assets/tikz/cc_objectives_tikz.pdf}
	\caption[Small example of \pcc{}]{A small graph with eight nodes and ten edges. Solid edges
	represent positive edges and dashed edges represent negative edges. A clustering \cluster{}
	is showed with 3 clusters: $\{1, 2, 4\}$, $\{3, 5, 6\}$ and $\{7, 8\}$. \cluster{} incurs two
	disagreements: the negative edge between nodes $1$ and $2$ within the blue cluster, and the
	positive edge $(6,7)$ between the orange and green clusters. Those disagreements are created
	by two cycles with one negative edge and thus cannot be avoided, meaning that \cluster{} is
	optimal. However, it is not the unique solution: for instance, merging the orange and green
	clusters would also yield two disagreements.}
	\label{fig:cc_objectives}
\end{figure}

\paragraph{Connection with \esp{} and other applications}
\label{ssub:cc_applications}

\esp{} can clearly be casted as a supervised classification problem where, given a labelled training
set of $m_0$ edges $\mathcal{X}=\{(e_1, y_1), \ldots, (e_{m_0}, y_{m_0})\}$, we can extract for
every edge a feature vector in $\Rbb^d$, pick an hypothesis class like linear models, find among
this class the hypothesis minimizing the empirical risk, and use it to predict the sign of edges in
the testing set. In contrast, \pcc{} in an agnostic problem. More precisely, the hypothesis class is
fixed, as it consists of all possible partitions of the nodeset. We are trying to find the partition
that best approximates the observed signs, but each weakly unbalanced cycle is gonna cost us at
least one unavoidable mistake. Despite this fundamental difference in their nature, \pcc{} is
connected with \esp{} at two levels, theoretical and practical. On the theoretical side, computing
the minimum number of disagreements gives a measure of the \esp{} problem complexity.  Intuitively,
given a labelled subgraph $H$ of $G$ (that is the training set), if there are few disagreements, the
\enquote{quasi} connected components of $H^+$ should give a reasonable estimation of the underlying
consistent clustering. Predicting the signs based on this clustering should results in few errors.
More formally, one can show that by finding a clustering $\cluster^\star$ of $H$ that has less
disagreements than $\kappa$ times the smallest number of disagreements $\Delta$, the number of
errors on the testing set will be at most of order $\kappa\Delta + \sqrt{mn\log
n}$~\autocite[Theorem 6]{Cesa-Bianchi2012b}. These theoretical considerations naturally lead to
practical a batch algorithm, which first finds a clustering minimizing as much as possible the number
of disagreements on the training set and then predicts signs according to this clustering. Our
assumption in doing so is that, according to our bias, our graph has few disagreements. Therefore,
provided we can find a clustering close to the optimal, we will not make too many prediction
mistakes.

\bigskip

Besides the special case of predicting edge sign, \textcite[Section 5]{Demaine2006} point out that
\pcc{} is well suited to several more general situations:
\begin{itemize}
   \item when the items to be clustered do not belong to a natural metric space (preventing
      approaches such as $k$-means) but we still know for some pairs whether they are similar or
      not.
   \item when we do not know the number of clusters beforehand but we have a similarity measure. In
      that case, we can select a problem-specific similarity threshold and set all edges with a
      similarity larger than the threshold to be positive while the others are set to negative.
   \item when we have a classic clustering problem (that is a set of objects, a distance between
      them and an objective function to minimize) with additional pairwise constraints of the form
      \emph{must-link/cannot-link}. Instead of restraining a clustering algorithm to the space of
      feasible solution, we convert the distances between objects and the constraints into signed
      edges and solve the resulting \pcc{} problem. 
\end{itemize}

Furthermore, as mentioned in the introduction of this thesis \vpageref{sub:intro_signed_graphs},
\pcc{} have been used in many
domains. For instance in computer vision, it is used to segment images in
2D~\autocites{Kim2011}{Bagon2011}{CellSeg14} or 3D~\autocites{VolumeSegmentation12}{Beier2015}, to
simplify 3D shapes~\autocite{Shape3D17} and to track target across sequential video
frames~\autocite{multiTracking15}. When the nodes are words, \pcc{} is employed to identify
coreference~\autocites{graphicalCoreference04}{Elsner2009} or cluster synonym
words~\autocite{SignedWordRatings}. It plays a large role in biology, to clusters
genes~\autocite{Ben-Dor99}, identify mutation regions in chromosomes~\autocite{Das2015} or finding
stable subsystems~\autocite{monotoneBiology07}. Applied to social networks, it helps analyze
political assemblies~\autocites{Mendonca2015}{Jiang2015}{BrazilCC17} and international
relations~\autocites{Traag2009}{CommunityUN12}. Finally, given large databases with duplicated
records, it naturally models entity
resolution~\autocites{Crosslingual07}{DeDup09}{LargeScaleDeDup09}{WeightedER14}.
