Like other clustering frameworks, in \pcc{}, we are given a set of objects and we want to gather
them into groups (called clusters) so that objects belonging to one cluster are similar to each
other while being dissimilar to objects from all the other clusters.

In \pcc{}, we formalize this problem by considering objects as the nodes of a graph $G$, whose edge
weights encode similarity. Namely, in the most general case, for nodes $u$ and $v$, the edge between
$u$ and $v$ is associated with two positive numbers:
$w_{u,v}^+$ denotes the strength of the similarity between $u$ and $v$;
$w_{u,v}^-$ denotes the strength of the dissimilarity between $u$ and $v$.
Note, however, that in many applications, only one of these two numbers is nonzero, in which case
we more conveniently set $w_{u,v} = \begin{cases}
	 w_{u,v}^+ & \quad \text{if } w_{u,v}^+ > 0 \text{ and } w_{u,v}^-=0 \\
	-w_{u,v}^- & \quad \text{if } w_{u,v}^+ = 0 \text{ and } w_{u,v}^->0 \\
\end{cases}$

%TODO replace N by [|K|] (and define that notation in the table)

Now consider a clustering \cluster{} of $V$, that is a function from $V$ to $\Nbb{}^{|V|}_{>0}$ that
assigns to each node a cluster index. For instance, $\cluster(u) = 3$ means that $u$ belongs to the
third cluster. We will also abuse the notation and let \cluster{} be a set of clusters $\{C_1, C_2,
\ldots, C_K\}$ that form a partition of $V$.\footnote{That is, $\forall i\,C_i\subset V$,
$\bigcup_{i=1}^K C_i = V$ and $\forall i\neq j,\, C_i\cap C_j = \emptyset$.}
We can evaluate how \cluster{} fits our clustering paradigm in two ways,
either by the number of \emph{agreements}, that is the weighted number of positive edges inside
clusters plus the weighted number of negative edges across clusters; or by the number of
\emph{disagreements}, that is the weighted number of negative edges inside clusters plus the
weighted number of positive edges across clusters. Given a cost function $c$, which is usually the
identity, \pcc{} can then be seen as a graph optimization problem, either of maximizing agreements
(\maxa{}):
\begin{equation}
	\max_{\cluster{}} \sum_{\cluster(u) = \cluster(v)} c(w_{uv}^+) +
	\sum_{\cluster(u) \neq \cluster(v)} c(w_{uv}^-)
	\label{eq:maxa}
\end{equation}
or minimizing disagreements (\mind{})\footnote{Note that in the data mining literature, \pcc{} may
refer to another problem. Namely, it is a special case of high-dimensional data clustering, where
features are locally correlated in various ways across different clusters that reside in arbitrarily
oriented subspaces~\autocite{otherCCproblem09}.}:
\begin{equation}
	\min_{\cluster{}} \sum_{\cluster(u) = \cluster(v)} c(w_{uv}^-) +
	\sum_{\cluster(u) \neq \cluster(v)} c(w_{uv}^+)
	\label{eq:mind}
\end{equation}

Although an optimal clustering $\cluster^\star$ achieves the same value on both \eqref{eq:maxa}
and \eqref{eq:mind}, we will see in \autoref{sub:state_of_the_art} that the latter objective is in
some sense \enquote{easier}. Another interesting feature of the \pcc{} problem is that contrary to
other clustering formulations, it does not require us to set the number of clusters $K$ beforehand.
Instead, $K$ emerges as a natural property of the solution. Since clustering is an unsupervised
problem, this is generally handy. However, in some situations, we may have prior knowledge on how
many clusters are the data, or external constraints. This can be handled with parametrized version
of \pcc{}, namely \maxa$[K]$ and \mind$[K]$ where the optimization is over clustering with exactly
$K$ clusters.

In \autoref{fig:cc_objectives}, we show a simple instance of \pcc{} and one of its optimal solution.
\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.63\linewidth]{assets/tikz/cc_objectives_tikz.pdf}
	\caption[Small example of \pcc{}]{A small graph with eight nodes and ten edges. Solid edges
	represent positive edges and dashed edges represent negative edges. A clustering \cluster{}
	is showed with 3 clusters: $\{1, 2, 4\}$, $\{3, 5, 6\}$ and $\{7, 8\}$. \cluster{} incurs two
	disagreements: the negative edge between nodes $1$ and $2$ within the blue cluster, and the
	positive edge $(6,7)$ between the orange and green clusters. Those disagreements are created
	by two cycles with one negative edge and thus cannot be avoided, meaning that \cluster{} is
	optimal. However, it is not the unique solution: for instance, merging the orange and green
	clusters would also yield two disagreements.}
	\label{fig:cc_objectives}
\end{figure}

\subsubsection{Connection with \esp{} and other applications}
\label{ssub:cc_applications}

According to \textcite[Section 5]{Demaine2006}, \pcc{} is well suited to several situations:
\begin{itemize}
   \item when the items to be clustered do not belong to a natural metric space (preventing
      approaches such as $k$-means) but we still know for some pairs whether they are similar or
      not.
   \item when we do not know the number of clusters beforehand but we have a similarity measure. In
      that case, we can select a problem-specific similarity threshold and set all edges with a
      similarity larger than the threshold to be positive while the others are set to negative.
   \item when we have a classic clustering problem (that is a set of objects, a distance between
      them and an objective function to minimize) with additional pairwise constraints of the form
      \emph{must-link/cannot-link}. Instead of restraining a clustering algorithm to the space of
      feasible solution, we convert the distances between objects and the constraints into signed
      edges and solve the resulting \pcc{} problem. 
\end{itemize}

In addition to these general considerations, \pcc{} has also been used in several domains:
