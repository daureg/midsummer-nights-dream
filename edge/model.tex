\section{Settings and learning problem}
\label{sec:edge_model}

As in the previous chapters, we are given an unweighted graph $G=(V,E)$\stodo{undirected?}. This
time though, we also have side information in addition to the graph topology. Namely, each node $u$
is associated with a $d$-dimensional feature vector $x_u \in \Rbb^d$. As an example, in a social
network, $x_u$ would be the profile of user $u$, describing $u$'s demographics and preferences. We
are also given a combination operator between feature vectors $s$ such that $s(x_u, x_v) = s_{uv}
\in \Rbb^d$. Finally, we consider distinguished feature vectors $w_{uv}$\stodo{is it better to
introduce $w$ with a single $\ell$ index, to emphasise that later, it will not be unique per edge?},
along which the combination $s_{uv}$ between two nodes $u$ and $v$ is evaluated. We call such
$w_{uv}$ \emph{directions}, and constrain them to have unit norm. As we will discuss, this
constraint can be seen as a way to couple the different dimensions of the profiles.

Depending on the application, and the semantic we ascribe to links between nodes, we can design and
interpret the combination operator $s$ as a difference, a similarity or a distance. Likewise, we
also have some freedom in defining how $s_{uv}$ and $w_{uv}$ are combined to assign a single real
value $g(s_{uv}, w_{uv})$ to the edge $(u,v)$. Let us call $g(w_{uv}, s_{uv})$ the \emph{goodness}
of $w_{uv}$ with respect to the edge $(u,v)$. We view this value as how adequately the direction
$w_{uv}$ \emph{explains}, or describes, the connection between $u$ and $v$.

\medskip

We assume in the following that both the profiles and the directions are vectors with components in
$[-1, 1]$. A natural choice for the operator $s$ is then the difference between the profile, that is
$s_{uv} = x_u - x_v$. In that spirit, if $u$ and $v$ are close along a direction $w_{uv}$, we expect
${x_u}^T w_{uv}$ to be close to ${x_v}^T w_{uv}$, as measured by $|{x_u}^T w_{uv} - {x_v}^T
w_{uv}|$. Because we prefer to work with smooth functions, we define the goodness to be $g(s_{uv},
w_{uv}) = \left({s_{uv}}^T w_{uv} \right)^2$, and we want to minimize it with respect to $w_{uv}$.
When ${x_u}_{;i} - {x_v}_{;i}$ is close to zero, the minimization and the fact that $w_{uv}$ is
unit-norm impose that ${w_{uv}}_{;i}$ should be large. On the other hand, with this formulation,
there is no way to set ${w_{uv}}_{;i}$ in order to \enquote{reward/favor} the fact that
${x_u}_{;i}$ and ${x_v}_{;i}$ are widely different.
Whereas there might be situations where this is not a problem, as we discussed in the introduction,
here we seek to model heterophilic interactions, at least along a subset of dimensions. Therefore, in
the following, we will adopt a refined approach.

Namely, we define $s$ to be the Hadamard (or component wise) product: $s_{uv} = x_u \circ x_v$, and
$g$ to be the dot product: $g(s_{uv}, w_{uv}) = {s_{uv}}^T w_{uv}$. The goodness of a direction $w$
for a given edge can thus be seen as a weighted sum of the per component similarity of $u$ and $v$
profiles. Therefore, it must be maximized with respect to $w_{uv}$. For a given component $i$, when
both ${x_u}_{;i}$ and ${x_v}_{;i}$ are large and of the same sign (that is close to either $+1$ or
$-1$), we see that $u$ and $v$ agree on the \ith{} dimension, and ${w_{uv}}_{;i}$ must accordingly
be a large positive value. On the other hand, when ${x_u}_{;i}$ and ${x_v}_{;i}$ are both large but
of different signs (\eg ${x_u}_{;i} = +1$ and ${x_v}_{;i} = -1$), $u$ and $v$ strongly disagree on
the \ith{} dimension, requiring in a large negative value for ${w_{uv}}_{;i}$.  Finally, if one of
${x_u}_{;i}$ or ${x_v}_{;i}$ is zero, denoting either an unimportant feature or a missing value,
then the value of ${w_{uv}}_{;i}$ does not matter, and will be set to zero by the maximization
procedure because of the unit norm constraint on $w_{uv}$.

\begin{aside}
When $s$ is a distance, and therefore positive, we impose $w_{uv}\geq 0$. A link $(u,v)$ is then
well explained by $w_{uv}$ when observed differences along this direction are small. More precisely,
when the quantity ${s_{uv}}^T w_{uv}$ is close to zero. Because $w_{uv}$ is unit norm, the \ith{}
component of $w_{uv}$ should be large when the \ith{} feature of $x_u$ and $x_v$ are close. Hence, a
null value for a component $i$ of $s_{uv}$ should not mean anything else than $u$ and $v$ share a
known and similar feature $i$ (\eg{} the unknown value should be different from $0$). When $s$ is
the difference $x_u - x_v$, it can be negative and the same reasoning can be applied, but with
$w_{uv}\in [-1,1]^d$ and a link value of $\left({s_{uv}}^T w_{uv}\right)^2$.

An alternative is to consider that the combination is a similarity. Links are created when the
observed similarity is high. Again, if $s$ is positive, it seems reasonable to impose that
$w_{uv}\geq0$ (and if $s$ can be negative, then $w_{uv}$ should be in $[-1,1]$). One main advantage
of similarities is maybe to avoid (or minimize) problems with missing values setting them to 0.
\end{aside}

\medskip

Based on these specific definitions of $s$ and $g$, we now formally write down the problem of
finding and assigning the most informative directions to every edge of $G$
\begin{problem}[]
  \label{p:edge_full}
  Given a graph $G=(V, E)$, node profiles $X\in [-1, 1]^{n\times d}$ and an integer $k \in \Nbb$,
  find a set of $k$ unit-norm directions $\mathcal{D}_k = \{w_1, w_2, \ldots, w_k\} \in \dsphere$
  and associate to every edge of $E$ the direction with the maximal goodness. Formally, solve
  \begin{equation}
    \label{eq:edge_full}
    \argmax_{w_1,\ldots,w_k \in \dsphere}
    \sum_{u,v \in E} \max_{\ell \in \rangesk} g(s_{uv}, w_{\ell}) =
    \sum_{u,v \in E} \max_{\ell \in \rangesk} \left(x_u \circ x_v\right)^T w_{\ell} = f(k)
  \end{equation}
\end{problem}


\paragraph{Generalization of signed graphs}
\label{par:generalization_of_signed_graphs}

Here we show why \autoref{p:edge_full} can model signed graphs as a special case, namely with $k=2$.
Remember the learning
bias we introduced in the previous chapter \vpageref{text:cc_new_bias}: every node belongs to
exactly one of the $K$ clusters available, is connected positively with the other nodes belonging to
the same cluster, and is connected negatively with the nodes belonging to every other clusters. Now
let the dimension of profiles be $d=K$ and the number of directions $k=2$. Moreover, we define the
two directions vectors as $w_1 = \frac{\onev}{\sqrt{d}} = w^+$ and $w_2 = -w_+ = w_-$, where
$\onev$ is the vector of dimension $d$ with only $1$ coordinates. Finally, for a node $u$ in
cluster $i$, we assume its profile $x_u$ is set as follow: all component are equal to $-b$ except
for the \ith{} one, which is equal to $a$, where $a$ and $b$ are constants we define now.

First, because $x_u$ is a unit vector, we have that $a^2 + (K-1)b^2=1$, from which $a =
\sqrt{1-(K-1)b^2}$ whenever $1-(K-1)b^2 \geq 0$, or equivalently $b \leq \frac{1}{\sqrt{K-1}}$.
Next, we look at $\onev^T\left(x_u \circ x_v\right) = \onev^T m_{uv} = \frac{1}{\sqrt{d}}
\sum_{i=1}^d m_{{uv}_i}$ for any edge $(u,v) \in E$. If $u$ and $v$ are in the same cluster, then
$\sum_{i=1}^d m_{{uv}_i} = a^2 + (K-1)b^2 = 1$, implying that $w_+^T m_{uv} = \frac{1}{d} > w_-^T
m_{uv} = -\frac{1}{d}$. Otherwise, $\sum_{i=1}^d m_{{uv}_i} = (K-2)b^2 - 2ab = (K-2)b^2 -
2b\sqrt{1-(K-1)b^2} = f_K(b)$. By computing the derivative of $f_K$ with respect to $b$, we find
that it reaches a minimum of $-\frac{1}{K-1}$ at $b = \frac{1}{\sqrt{K(K-1)}} \leq
\frac{1}{\sqrt{K-1}}$. In this case, $w_+^T m_{uv} = -\frac{1}{(K-1) \sqrt{d}} < w_-^T m_{uv} =
\frac{1}{(K-1) \sqrt{d}}$. To maximize $\sum_{(u,v)\in E} \max_{1 \leq \ell \leq d} w_\ell^T
m_{uv}$, we therefore assign $w_+$ to edges whose both endpoints belong to the same cluster, and
$w_-$ to edges joining two different clusters.

Moreover, given this assignment, we can show that $w_+$ and $w_-$ are the optimal directions, under
the additional uniformity assumption that there are exactly $r$ edges within every cluster, and $s$
edges between any two clusters. First, we define
\begin{equation*}
  m_{\mathrm{inner}} = \sum_{(u,v) \in E;\, \cluster(u)=\cluster(v)} m_{uv} \qquad \text{and} \qquad
  m_{\mathrm{outer}} = \sum_{(u,v) \in E;\, \cluster(u)\neq \cluster(v)} m_{uv}
\end{equation*}
to be the sum of all edge vectors within and between clusters respectively. By our choice of $a$ and
$b$, we have that $m_{\mathrm{inner}} = r\onev$ and $m_{\mathrm{outer}} = -\frac{s}{2}\onev$. Now
recall that the solution of $\argmax_{w\in \dsphere} w^T c$ is $\frac{c}{||c||}$. Therefore, the
best direction to assign to inner edges is indeed $w_+$, and so is $w_-$ to outer edges.

\begin{aside}
For inner edge vectors, $m_{uv}$ in cluster $i$ has $a^2$ as its \ith{} component, and $b^2$
elsewhere. There are $r$ of them in cluster $i$, so summing over all clusters we get
$m_{\mathrm{inner}} = r\left(a^2 + (K-1)b^2\right)\onev = r\onev$.
For outer edge vectors, there are a few more steps. Consider the $s$ edges between $C_1$ and $C_2$,
whose $m_{uv}$ is
$\begin{pmatrix}
  -ab \\
  -ab \\
  b^2 \\
  b^2 \\
  \vdots \\  
\end{pmatrix}$. Likewise, between $C_1$ and $C_3$ we have
$m_{uv}=\begin{pmatrix}
  -ab \\
  b^2 \\
  -ab \\
  b^2 \\
  \vdots \\  
\end{pmatrix}$. We thus we see that the first component is always $-ab$, that there is another
such term at the other cluster index, while the rest is $b^2$. Summing all the vectors incident to
$C_1$, we get
$s\begin{pmatrix}
  -ab(K-1) \\
  b^2(K-2) -ab \\
  b^2(K-2) -ab \\
  \vdots \\  
\end{pmatrix}$. Similarly, the sum of all vectors incident to $C_2$ is
$s\begin{pmatrix}
  b^2(K-2) -ab \\
  -ab(K-1) \\
  b^2(K-2) -ab \\
  \vdots \\  
\end{pmatrix}$, and so on for every cluster $i$. Summing over all clusters and dividing by $2$ to
avoid double counting, we get $m_{\mathrm{outer}} = \frac{s}{2}\left( (K-1)\left(b^2(K-2) -ab\right)
-ab(K-1) \right)\onev = \frac{s(K-1)}{2}\left(b^2(K-2) -2ab\right)\onev =
\frac{s(K-1)}{2}f_K(b)\onev = -\frac{s(K-1)}{2} \frac{1}{K-1} \onev = -\frac{s}{2}\onev$.
\end{aside}

Granted, the situation where all the nodes of a cluster have exactly the same profile and where the
connections are so regular is highly idealized. Moreover, we did not formally prove that there is
not another assignment (with different direction vectors) that would give a better objective
value\footnote{Although given the symmetry of the problem, that would be surprising.}. Our point is
rather that profiles and directions can be seen as a \enquote{latent} explanation for the signs in
a balanced signed graphs, like our generative model of the first chapter.

\bigskip

If $k=|E|$, the topology of the graph becomes irrelevant, as we can simply exploit the trivial
solution of having a single direction $w_{uv} = \frac{s_{uv}}{||s_{uv}||}$ for every edge $(u,v)$.
More generally, as $k$ increases, we expect the value $\max_{\mathcal{D}_k} f(k)$ to increases as
well, at the cost of interpretability. There could be a principled approach to finding the
\enquote{best} $k$, based on a information theoretical measure of the complexity of $\mathcal{D}_k$
and the minimum description length principle~\autocite{grunwald2005tutorial}. However, in the
interest of simplicity, in the following we focus on the formulation stated in
\autoref{p:edge_full}, where $k$ is given. In practice, we further constrain $k$ to be small (say
less than \np{10}). Within the multilayer framework, $k$ could also be seen as the number of
underlying layers\stodo{elaborate on that connection}.

\begin{aside}
Generalizing the balance theory from signed graphs, we could imagine that in triangles or short
cycles, only certain combinations of directions are valid (or at least desirable). As a nice side
effect, this would likely require more innovative optimization algorithms. However, it sounds very
application-dependant. Furthermore, in our current setting, we do not know in advance in the
semantic of each direction, and we have no labeled edges. Therefore, it is not clear how those
constraints would be specified or whether they can be learned, if applicable. 
\end{aside}

Another way to leverage the graph topology is to define local constraints at the node level. Since
the beginning we considered that the user profiles were given, but from now on we further assume
that they are normalized so that they are unit norm, just like directions. In practice, if nodes are
users of a social network and the profiles measure their activity across several domains, this
allows to compare users with various level of total activity. One way to integrate that fact in
our optimization formulation is to assume that profiles are linear combination of directions, that
is $x_u = b_u + \sum_{v \in \nei(u)} a_{uv} w_{uv}$, where $a_{uv}$ are real coefficients and $b_u
\in \Rbb^d$. Viewed in the other direction, this can be interpreted as follow: each time a node $u$
connect to another node $v$ along a direction $w_{uv}$, it consume a part of its profile
proportional to $w_{uv}$. Therefore, we subtract the term $\sum_{u\in V} \left|\left| x_u - b_u -
\sum_{v \in \nei(u)} a_{uv} w_{uv} \right|\right|^2$ to \eqref{eq:edge_full}.
%
We can also make the assumption that each node $u$ is only involved in $k_\mathrm{local} < k$
different directions. The intuition is that when $d$ is large enough, users have to focus their
energy and can only express their interest in a few number of dimensions. In other words, all the
edges incident to $u$ can only be associated with one of $k_\mathrm{local}$ directions. This is
difficult to incorporate directly in the optimization formulation, as for certain value of
$k_\mathrm{local}$, this is a \NPc{} graph colouring problem. Still, it can be seen as a learning
bias and will be used to generate synthetic data.
These two constraints can be combined, so that the profiles are close to a linear combination of a
small number of directions.

% $\theset{(u,v) \in E}{v \in \nei(u)}$

\bigskip

In order to make the optimization more tractable, we relax the formulation of \autoref{p:edge_full}
by replacing the $\max$ inside the sum of \eqref{eq:edge_full}, either by a continuous approximation
or by a low rank matrix approach.

One can check that given a set of numbers $S=\{a_1, a_2, \ldots, a_{|S|}\}$ and real number $\beta >
0$, we have
\begin{equation*}
  \max_{a_i \in S} a_i = \lim_{\beta \rightarrow + \infty} \frac{1}{\beta}
  \log\left( \sum_{i=1}^{|S|} \exp{\beta a_i} \right)
\end{equation*}
Using this fact, and averaging the two components of our objective function, we can rewrite
\eqref{eq:edge_full} as:
\begin{equation}
  \argmax_{w_1,\ldots,w_k \in \dsphere,\, a_{uv}\in\Rbb,\, b_u \in \Rbb^d}
  \frac{1}{|E|\beta} \sum_{u,v\in E}
  \log\Bigl(\sum_{\ell=1}^k\exp\left(\beta {s_{uv}}^T w_\ell \right)\Bigr)
  - \frac{1}{|V|}\sum_{u\in V}
  \left|\left| x_u - b_u - \sum_{v \in \nei(u)} a_{uv} w_{uv} \right|\right|^2
\end{equation}

Another way to remove the $\max$ is to look for a single direction per edge, stacked horizontally in
one matrix. Then we can directly minimize \eqref{eq:edge_full} while enforcing the matrix
$W\in\mathbb{R}^{d\times |E|}$ of all directions to be low-rank, so that each $w_{uv}$ (given by the
$(u-1)n+v$-th column of $W$) can be expressed as a linear combination of a small number of basis
vectors. Formally, if $W$ has rank $k$, then there exists $P\in\mathbb{R}^{d\times k}$ and
$Q\in\mathbb{R}^{|E|\times k}$ such that $W=PQ^T$ and $w_{uv} = Pq_{uv}^T$. $P$ defines $k$
representative directions in $\Rbb^d$ while $Q$ gives, for each edge, the weights of each
representative direction. Similar to what is done in spectral clustering, we could cluster the
$q_{uv}$'s to obtain an edge labeling.
%
Denoting by $\mathbb{M}^{d\times |E|}$ the set of $d\times |E|$ matrices with unit $L_2$ norm
columns, a convex formulation of the problem is as follows:
\begin{equation}
  \label{eq:fact_convex}
  \min_{W\in\mathbb{M}^{d\times |E|}} \quad
  \sum_{i,j \in E} {s_{uv}}^T w_{uv} + \lambda \|W\|_*,
\end{equation}
where $w_{uv} \in \Rbb$ denotes the $(u-1)n+v$-th column of $W$, $\|W\|_* =
\sum_{i=1}^{\min(d,|E|)}\sigma_i(W)$ is the sum of the singular values of $W$ (called the nuclear
norm or trace norm), and $\lambda \geq 0$ is a regularization parameter.\footnote{To make the
optimal solution unique, we can use an additional Frobenius norm regularization on $W$.} The nuclear
favors low-rank solutions as it is essentially an $L_1$ norm on the spectrum.
Problem~\eqref{eq:fact_convex} can be solved using proximal gradient descent (requires singular
value decomposition at each iteration) \autocite{Parikh2013a} or Frank-Wolfe (requires only to find
the largest singular vector) \autocite{Jaggi2013a}.

Alternatively, we can consider a nonconvex formulation where we make the low-rank decomposition of
$W$ explicit. Given a rank $k\leq \min(d,|E|)$:
\begin{equation}
  \label{eq:fact_nonconvex}
  \min_{P\in\mathbb{M}^{d\times k}, Q\in\mathbb{M}^{|E|\times k}} \quad
  \sum_{u,v \in E} {s_{uv}}^T w_{uv}.
\end{equation}
Problem~\eqref{eq:fact_nonconvex} can be solved via alternating optimization over $P$ and $Q$ using
standard (projected) gradient descent algorithms.
See \autocite[Section 6]{attributedSNE17} for a write up example of matrix optimization.

Note however that this matrix formulation requires learning more parameters, as it each edge has a
different, albeit made up of a small number of basis direction.  Impose sparsity on $Q$?

Max margin, replace the max by a difference of the currently assigned direction with the other
(trade off parameter). At least I think I used it in synthetic data.

