\section{Attributed graphs and problem definition}
\label{sec:edge_problem}

We start by introducing some notations and terminology about edge types in node-attributed graphs.
Then we present two options to model the interactions between nodes and the role of attributes in
\enquote{explaining} edges (\autoref{sub:edge_setting}). Choosing of these two options leads us to
formulate the \ecp{} problem in \autoref{sub:edge_constraints}. We then describe additional
constraints to take into account the topology of the graph. Finally we elaborate on the relation
between \ecp{} and \esp{} in signed graphs.

\subsection{Setting and modelling}
\label{sub:edge_setting}

As in the previous chapter, we are given an undirected, unweighted graph $G=(V,E)$. This
time though, we also have side information in addition to the graph topology. Namely, each node $u$
is associated with a $d$-dimensional feature vector $x_u \in \Rbb^d$. As an example, in a social
network, $x_u$ would be the profile of user $u$, describing $u$'s demographics and preferences. We
are also given a combination operator $s$ between feature vectors, such that $s(x_u, x_v) = s_{uv}
\in \Rbb^d$. Finally, we consider distinguished feature vectors $w_{uv}$,
along which the combination $s_{uv}$ between two nodes $u$ and $v$ is evaluated. We call such
$w_{uv}$ \emph{directions}, and constrain them to have unit norm.
While being exactly of norm one is not primordial, the role of bounding the norm of the directions
(\ie{} having $\sum_{i=1}^d {w_{uv}}_{;i}^2 \leq B^2$) is to tie the $d$ dimensions together.
Without such a constraint, we could simply study each dimension separately, whereas this bound
adds a coupling across different attributes, since increasing the weight of an attribute mechanically
affects the others.

Depending on the application, and the semantic we ascribe to links between nodes, we can design and
interpret the combination operator $s$ as a difference, a similarity or a distance. Likewise, we
also have some freedom in defining how $s_{uv}$ and $w_{uv}$ are combined to assign a single real
value $g(s_{uv}, w_{uv})$ to the edge $(u,v)$. Let us call $g(w_{uv}, s_{uv})$ the \emph{goodness}
of $w_{uv}$ with respect to the edge $(u,v)$. We view this value as how adequately the direction
$w_{uv}$ \emph{explains}, or describes, the connection between $u$ and $v$.

\medskip

As written in the introduction, because we are interested in (partial) heterophilic links, we assume
in the following that both the profiles and the directions are vectors with components in $[-1, 1]$.
Let us take the example of encoding the gender of a user in a social network, assuming there is only
two values possible: male and female. Traditionally, this attribute would be set to $1$ if the user
is female and $0$ otherwise. Instead, we use $+1$ and $-1$, to clearly emphasize the difference
between the two values. The same is valid for continuous variables. For instance, an attribute could
range from $-1$ to $+1$ to represent the ideological stance of a user on the conservative/liberal
scale~\autocite{newsBias15}.\footnote{Note of course that in both cases, negative values do not
represent a judgement but are merely a reversible mathematical convenience.} This applies even to
non diverging positive quantities such as age, which can be projected to $[-1, 1]$ using
pre-processing such as standardization or $z$-score.

A natural choice for the operator $s$ is then the difference between the profiles, that is
$s_{uv} = x_u - x_v$. In that spirit, if $u$ and $v$ are close along a direction $w_{uv}$, we expect
${x_u}^T w_{uv}$ to be close to ${x_v}^T w_{uv}$, as measured by $|{x_u}^T w_{uv} - {x_v}^T
w_{uv}|$. Because we prefer to work with smooth functions, we define the goodness to be $g(s_{uv},
w_{uv}) = \left({s_{uv}}^T w_{uv} \right)^2$, and we want to minimize it with respect to $w_{uv}$.
When, on the \ith{} dimension, ${x_u}_{;i} - {x_v}_{;i}$ is close to zero, the minimization objective
and the fact that $w_{uv}$ is
unit-norm impose that ${w_{uv}}_{;i}$ should be large. On the other hand, with this formulation,
there is no way to set ${w_{uv}}_{;i}$ in order to \enquote{highlight} the fact that
${x_u}_{;i}$ and ${x_v}_{;i}$ are widely different.
There might be situations where this is not a problem. But again,
here we seek to model heterophilic interactions, at least along a subset of dimensions. Therefore,
we now introduce a more flexible approach.

Namely, we define $s$ to be the Hadamard (or component wise) product: $s_{uv} = x_u \circ x_v$, and
$g$ to be the dot product: $g(s_{uv}, w_{uv}) = {s_{uv}}^T w_{uv}$. The goodness of a direction $w$
for a given edge can thus be seen as a weighted sum of the per component similarity of $u$ and $v$
profiles. Therefore, it must be maximized with respect to $w_{uv}$. For a given component $i$, when
both ${x_u}_{;i}$ and ${x_v}_{;i}$ are large and of the same sign (that is close to either $+1$ or
$-1$), we see that $u$ and $v$ agree on the \ith{} dimension, and ${w_{uv}}_{;i}$ must accordingly
be a large positive value. On the other hand, when ${x_u}_{;i}$ and ${x_v}_{;i}$ are both large but
of different signs (\eg ${x_u}_{;i} = +1$ and ${x_v}_{;i} = -1$), $u$ and $v$ strongly disagree on
the \ith{} dimension, requiring in a large negative value for ${w_{uv}}_{;i}$.  Finally, if one of
${x_u}_{;i}$ or ${x_v}_{;i}$ is zero, denoting either an unimportant feature or a missing value,
then the value of ${w_{uv}}_{;i}$ does not matter, and will be set to zero by the maximization
procedure because of the unit norm constraint on $w_{uv}$.

\iffalse
\begin{aside}
When $s$ is a distance, and therefore positive, we impose $w_{uv}\geq 0$. A link $(u,v)$ is then
well explained by $w_{uv}$ when observed differences along this direction are small. More precisely,
when the quantity ${s_{uv}}^T w_{uv}$ is close to zero. Because $w_{uv}$ is unit norm, the \ith{}
component of $w_{uv}$ should be large when the \ith{} feature of $x_u$ and $x_v$ are close. Hence, a
null value for a component $i$ of $s_{uv}$ should not mean anything else than $u$ and $v$ share a
known and similar feature $i$ (\eg{} the unknown value should be different from $0$). When $s$ is
the difference $x_u - x_v$, it can be negative and the same reasoning can be applied, but with
$w_{uv}\in [-1,1]^d$ and a link value of $\left({s_{uv}}^T w_{uv}\right)^2$.

An alternative is to consider that the combination is a similarity. Links are created when the
observed similarity is high. Again, if $s$ is positive, it seems reasonable to impose that
$w_{uv}\geq0$ (and if $s$ can be negative, then $w_{uv}$ should be in $[-1,1]$). One main advantage
of similarities is maybe to avoid (or minimize) problems with missing values setting them to 0.
\end{aside}
\fi

\medskip

This second choice of $s$ and $g$ is indeed more flexible, for it encompasses all cases of
homophily, indifference and heterophily. Furthermore, since the profiles are given, the resulting
$s_{uv}$ can be seen as constants, meaning that $g$ is linear and therefore interpretable. Finally,
it also supports categorical data. We shall demonstrate all these features on a small scale example,
but we first define formally our problem.

\subsection{Learning problem and additional constraints}
\label{sub:edge_constraints}

\paragraph{Problem definition}

With working definitions of $s$ and $g$ at hand, we now formally write down the problem of
finding and assigning the most informative directions to every edge of $G$
\begin{problem}[\ecp{}]
  \label{p:edge_full}
  Given a graph $G=(V, E)$, node profiles $X\in [-1, 1]^{n\times d}$ and an integer $k \in \Nbb$,
  find a set of $k$ unit-norm directions $\mathcal{D}_k = \{w_1, w_2, \ldots, w_k\} \in \dsphere$
  and associate to every edge of $E$ the direction with the maximal goodness. Formally, solve
  \begin{equation}
    \label{eq:edge_full}
    \argmax_{w_1,\ldots,w_k \in \dsphere}
    \sum_{u,v \in E} \max_{\ell \in \rangesk} g(s_{uv}, w_{\ell})
  \end{equation}
  where $s_{uv} = x_u \circ x_v$ and $g(s_{uv}, w_{\ell}) = {s_{uv}}^T w_{\ell}$.
\end{problem}

\paragraph{Example}

In \autoref{fig:edge_exe}, we present a small instance of the \ecp{} problem, where we are given the
graph shown in \autoref{fig:edge_exe_graph}, whose node attributes are listed
\autoref{tab:edge_exe}. The first two attributes are categorical, denoting the company employing
each user, either \textsf{G} or \textsf{F}. Instead of a classic binary one-hot encoding, we
additionally use $-1$ to denote that a user had been employed by a given company. \textsf{Gender} is
encoded as presented earlier, and we also apply a pre processing to transform \textsf{age} between
$-1$ and $1$. Finally, \textsf{opinion} represents the view of former \textsf{F} employees about a
societal issue, with $0$ denoting a neutral position. In \autoref{tab:edge_exe}, we also show two
directions that explain the connections in the graph $G$. The first covers being of the same age and
same companies, especially at \textsf{G}, while the second directions covers former employees of
\textsf{F} having different \textsf{age} and \textsf{opinion}. Not only are the results
interpretable as claimed before, but we can also assess the strength of each explanation, using
their goodness. For instance, we see that the edge between $x$ and $y$ has a low score, because
those two users have indeed the same \textsf{age} and \textsf{opinion}.

\begin{figure*}[htpb]
  % \centering
  \begin{subfigure}[t]{0.55\textwidth}
    \centering
    \includegraphics[width=.9\textwidth]{tikz/edge_example_tikz.pdf}
    \caption{The graph $G$. Larger nodes represent old users while smaller ones are younger. Their
    gender is inscribed in the node and their shape represent their opinion: rectangle for $-1$,
    circle for $0$ and diamond for $+1$. Finally, orange stands for being employed at company G and
    faded blue for having been employed at company F. As for the edges, the red ones are labeled by
    the first direction and the green ones by the second direction, and we label them with the
    associated (normalized) goodness. } \label{fig:edge_exe_graph}
  \end{subfigure}~
  \begin{subfigure}[t]{0.42\textwidth}
    \centering
    \small
    \begin{tabular}{lrrrrr}
      \toprule
      {}    & G   & F    & gender & age    & opinion \\
      \midrule
      $u$   &  $+1$ &   $0$ &   $-1$ &  $-0.8$ &     $0$ \\
      $v$   &  $+1$ &  $-1$ &   $-1$ &  $-0.7$ &    $+1$ \\
      $w$   &  $+1$ &   $0$ &   $+1$ &   $0.6$ &    $+1$ \\
      $x$   &  $+1$ &  $-1$ &   $+1$ &   $0.9$ &    $-1$ \\
      $y$   &   $0$ &  $-1$ &   $-1$ &   $0.7$ &    $-1$ \\
      $z$   &   $0$ &  $-1$ &   $+1$ &  $-0.6$ &    $+1$ \\
      \midrule
      $w_1$ &  $+3$ &  $+1$ &    $0$ &    $+1$ &     $0$ \\
      $w_2$ &  $0$  &  $+4$ &    $0$ &    $-2$ &    $-4$ \\
      \bottomrule
    \end{tabular}
    \caption{The numerical attributes of the six nodes. They are followed by the (non normalized)
    two directions $w_1$ and $w_2$.} \label{tab:edge_exe}
  \end{subfigure}
  \caption{A small instance of \ecp{} and an handcrafted solution, albeit non-optimal.}
  \label{fig:edge_exe}
\end{figure*}

\paragraph{Choice of $k$}
\label{par:edge_choose_k}

For clarity, the previous example of \autoref{fig:edge_exe} has only a handful of nodes and
dimensions, and can therefore reasonably be explained by two directions. But in larger graphs, the
choice of $k$ is a legitimate question.
At one extreme, picking $k=|E|$ renders the topology of the graph
irrelevant, as we can simply exploit the trivial
solution of having a single direction $w_{uv} = \frac{s_{uv}}{||s_{uv}||}$ for every edge $(u,v)$.
More generally, as $k$ increases, we expect the value $\max_{\mathcal{D}_k} \sum_{u,v \in E}
\max_{\ell \in \rangesk} g(s_{uv}, w_{\ell})$ to increase as
well, at the cost of interpretability. There could be a principled approach to finding the
\enquote{best} $k$, based on a information theoretical measure of the complexity of $\mathcal{D}_k$
and the minimum description length principle~\autocite{grunwald2005tutorial}. However, in the
interest of simplicity, in the following we focus on the formulation stated in
\autoref{p:edge_full}, where $k$ is given. In practice, we further constrain $k$ to be small (say
less than \np{10}). Within the multilayer framework, $k$ could also be seen as the number of
underlying layers. To retain a fair level of interpretability, we deem appropriate to have $k$ not
too large.

\paragraph{Topological constraints}

In addition to assume that the input graph is the superposition of a small number of layers,
another way to leverage the graph topology is to define local constraints at the node level.
For that we will consider node profiles to be normalized and having unit norm. A practical
justification is that if nodes are
users of a social network and the profiles measure their activity across several domains, this
allows to compare users with various level of total activity. Furthermore, we assume that profiles
are linear combination of directions, that
is $x_u = b_u + \sum_{v \in \nei(u)} a_{uv} w_{uv}$, where $a_{uv}$ are real weights and $b_u
\in \Rbb^d$ is the bias vector of $u$, its inherent profile take in isolation.
Viewed in the other direction, this can be interpreted as follow: each time a node $u$
connect to another node $v$ along a direction $w_{uv}$, it \enquote{consumes} a part of its profile
proportional to $w_{uv}$. Therefore, we subtract the term
\begin{equation}
  \label{eq:edge_node_loss}
  \mathcal{L}_{\mathrm{node}} =
  \sum_{u\in V} \left|\left| x_u - b_u - \sum_{v \in \nei(u)} a_{uv} w_{uv} \right|\right|^2_2
\end{equation}
from \eqref{eq:edge_full}.

\medskip

We also make the assumption that each node $u$ is only involved in $k_\mathrm{local} < k$
different directions. The intuition is that when $d$ is large enough, users have to focus their
energy and can only express their interest in a few number of dimensions. In other words, all the
edges incident to $u$ can only be associated with one of $k_\mathrm{local}$ directions. For
some values of
$k_\mathrm{local}$, this is a \NPc{} graph colouring problem and thus cannot necessarily be enforced
exactly. Still, it can be seen as a learning bias and will be used to generate synthetic data.
Furthermore, we can also express it as an optimization constraint. Specifically, suppose that for
an edge $(u,v)$ associated with the direction $w_i \in \mathcal{D}_k$, we have a $k$-dimensional
vector $y_{uv}$ equal to $e_i$ (that is all zero except for the \ith{} component set to one). Let
us stack all such vectors into a $|E| \times k$ matrix $Y$, and let $C \in \Rbb^{|V| \times |E|}$ be
the incidence matrix of $G$\footnote{The incidence matrix of a graph $G$, usually denoted by $B$, is
the $|V| \times |E|$ matrix such that $B_{i,j} = 1$ if the node $v_i$ and edge $e_j$ are incident
and 0 otherwise.}, with $c_u$ being its $u^{th}$ row. Then $c_u^TY$ is a $k$-dimensional vector that counts
how many times each direction is incident to node $u$. We thus want the $\ell_0$ norm of $c_u^TY$
(\ie the number of its non-zero component) to be upper bounded by $k_\mathrm{local}$.
This is not a convex constraint but we can relax it, for such sparsity inducing problems have been
well studied~\autocite{sparseOptim12}. Namely, we let $y_{uv}$ be a softmax membership
$k$-dimensional vector, that is we set $\forall i \in \rangesk\, y_{uv,i} =
\frac{\exp(\beta{s_{uv}}^T w_i)}{\sum_{j=1}^k \exp(\beta {s_{uv}}^T w_j)} $, with $\beta>0$ a large
positive constant.  And we replace the $\ell_0$ norm by a $\ell_1$ norm (or even a $k$-support
norm~\autocite{KsupportNorm12}), yielding the following $\mathcal{L}_{\mathrm{local}}$ to be
minimized:
\begin{equation}
  \label{eq:edge_local_loss}
  \mathcal{L}_{\mathrm{local}} =
  \sum_{u \in V} \left\| \sum_{v \in \nei(u)} y_{uv} \right\|_1 =
  \sum_{u \in V} \sum_{\ell = 1}^k \sum_{v \in \nei(u)}
  \frac{\exp(\beta{s_{uv}}^T w_\ell)}{\sum_{j=1}^k \exp(\beta {s_{uv}}^T w_j)}
\end{equation}

These two constraints can be combined, so that the profiles are close to a linear combination of a
small number of directions.

\input{signed}

\section{Proposed approaches}
\label{sec:edge_methods}

After having introduced \autoref{p:edge_full} and some additional constraints, we now present
methods to solve it. We start with a naive baseline, and propose a post-processing to improve it.
Then we describe a convex relaxation. Finally, we depart slightly from finding $k$ directions by
considering a low-rank matrix formulation, where each edge is assigned a linear combination of a
small number of base directions, paving the way to overlapping clustering.

\subsection{\kmeans{} baseline and improvement}
\label{sub:edge_baseline}

A simple and natural baseline is to cluster the set of all similarity edge vectors $
\theset{s_{uv}}{(u,v) \in E}$ using the $k$-means algorithm, where $k$ is the number of directions
set in \autoref{p:edge_full}. Formally, given the graph $G=(V,E)$ and the node profiles $X$ as
input, we order the edges from $e_1$ to $e_m$. Then we build the matrix $S \in \Rbb^{m\times d}$, whose
\ith{} row is the similarity $s_{uv} = x_u \circ x_v$ between the profiles of the endpoints of $e_i=(u,v)$.
Running $k$-means on the rows of $S$ will thus partition $E$ into $k$ sets of edges, call them $E_1,
\ldots, E_k$. A drawback of that simplicity is that it does not solve \autoref{p:edge_full}.
Indeed, the partition of $E$ does not provide the set of directions $\mathcal{D}_k = \{w_1, w_2,
\ldots, w_k\} \in \dsphere$ we are looking for. A natural way of obtaining directions from that
partition is to set $w_\ell$ to be the normalized cluster center of $E_\ell$. We refer to this
method as \kmeans{}, and assuming it requires $T$ iterations to converge, it has a linear complexity
of $O(T|E|)$.
\iffalse
\begin{center}
  \rule{\textwidth}{.3pt}
  \begin{algorithmic}[1]
    \Function{\kmeans{}}{graph $G=(V,E=\{e_1, \ldots, e_m \})$, profiles $X$}
    \State build the matrix $S$ such that $S_{i,j} = {s_{e_i}}_{;j}$
    \State let $\yhat{}$ and $\{c_1, \ldots, c_k\}$ be the assignment and centroids resulting of
    running $k$-means on the rows of $S$
    \State \textbf{return} \yhat{}, $\mathcal{D}_k = \left\{\frac{c_\ell}{\left\| c_\ell
    \right\|}\right\}_{\ell=1}^k $
    \EndFunction
  \end{algorithmic}
  \rule{\textwidth}{.3pt}
\end{center}
\fi

However,
this does not guarantee that we have assigned the best direction to every edge. We therefore propose
the following heuristic, inspired by the Lloyd algorithm for $k$-means, but where the directions in
$\mathcal{D}_k$ play the role of centroids. Namely, we alternate between two steps:
\begin{enumerate}[(i), nosep]
  \item create a new partition $\{E'_1, \ldots, E'_k\}$ by assigning every edge $(u,v)$ to the
    direction $w$ maximizing its goodness ${s_{uv}}^T w$, that is $E'_\ell = \theset{(u,v) \in E}{
    w_\ell = \argmax_{w_i \in \mathcal{D}_k} {s_{uv}}^T w_i}$, and
  \item update the directions to make them optimal with respect to the edges currently assigned to
    them. Specifically, set $\mathcal{D}_k = \left\{ w_\ell = \frac{s_\ell}{\left\| s_\ell
    \right\|}\right\}_{\ell=1}^k $, where $s_\ell = \sum_{u,v \in E'_\ell} s_{uv}$ is the sum of all
    the similarity vectors of the edge in $E'_\ell$,
\end{enumerate}
until the edge assignment is stable or we reach a maximum number of iterations. By analogy with the
$k$-means algorithm, obtaining convergence guarantees in the general case would be surprising, but in
practice we expect that only a small number $T$ of iterations would be needed. We call this method
\lloyd{} and note it also has a linear complexity of $O(T|E|)$.

\subsection{Convex relaxation}
\label{sub:edge_vector}

The difficulty in optimizing directly \eqref{eq:edge_full} stems from the inner $\max$ operator, which we
replace here by a convex relaxation. Indeed, one can check that given a set of numbers $S=\{a_1,
a_2, \ldots, a_{|S|}\}$ and real number $\beta > 0$, we have
\begin{equation*}
  \max_{a_i \in S} a_i = \lim_{\beta \rightarrow + \infty} \frac{1}{\beta}
  \log\left( \sum_{i=1}^{|S|} \exp{(\beta a_i)} \right)
\end{equation*}
Using this fact, and averaging the two components of our objective function, we can rewrite
\eqref{eq:edge_full} as:
\begin{equation}
  \label{eq:edge_soft_early}
  \argmax_{{\substack{w_1,\ldots,w_k \in \dsphere,\\
  a_{uv}\in\Rbb,\, b_u \in \Rbb^d}}}
  \frac{1}{|E|\beta} \sum_{u,v\in E}
  \log\Bigl(\sum_{\ell=1}^k\exp\left(\beta {s_{uv}}^T w_\ell \right)\Bigr)
  - \frac{\mu}{|V|}\sum_{u\in V}
  \left|\left| x_u - b_u - \sum_{v \in \nei(u)} a_{uv} w_{uv} \right|\right|^2,
\end{equation}
where $\mu > 0$ is a trade-off parameter.
Note that in the second term, because we do not know the edge assignment, we have to decide which of the
$w_1, \ldots, w_k$ is $w_{uv}$ equal to. According to our definition of the goodness function, we
want to let $w_{uv} = \argmax_{w_\ell \in \mathcal{D}_k} {s_{uv}}^T w_\ell$. However, the $\argmax$
function is again not convex and we thus use the same relaxation as before. Namely, this relaxation
$\mathcal{A}$ takes as input a $k$-tuple of directions and return their sum weighted by the softmax
function as follow:
$$ \begin{array}[t]{lrcl}
  \mathcal{A} : & \Rbb^d \times \ldots \times \Rbb^d & \longrightarrow & \Rbb^d \\
      & (w_1,\ldots,w_k) & \longmapsto &
  \sum_{i=1}^k \frac{\exp(\beta{s_{uv}}^T w_i)}{\sum_{j=1}^k \exp(\beta {s_{uv}}^T w_j)} w_i
\end{array} $$
This function has an explicit derivative with respect to any $w_i$, but it is rather costly to
compute and in practice we
found automatic differentiation~\autocite{autograd15} to be more efficient. Whereas the modified
objective function \eqref{eq:edge_soft_early} is now jointly convex in all its variables, the set
$\dsphere$ is not. To ease
the optimization procedure, we thus only impose the directions $w_\ell$ to lie within the unit ball
and expect them to have norm close to one in order to maximize the dot product in the ${s_{uv}}^T
w_\ell$ term. A further simplification is that while \eqref{eq:edge_soft_early} requires to optimize
$kd + |E| + |V|d$ parameters, we fix $a_{uv} = 1$ and $b_u = \onev$ for all edges and nodes in order
to be independent of the size of the graph. The final problem solved by our \combined{} method using
(full) projected gradient descent is therefore:
\begin{equation}
  \label{eq:edge_soft}
\!  \argmax_{w_1,\ldots,w_k \in \dball}
  \frac{1}{|E|\beta} \sum_{u,v\in E}\!
  \log\Bigl(\sum_{\ell=1}^k\exp\left(\beta {s_{uv}}^T w_\ell \right)\Bigr)
  - \frac{\mu}{|V|}\sum_{u\in V}
  \left|\left| x_u  - \sum_{v \in \nei(u)}\!\! \mathcal{A}(w_1,\ldots,w_k) \right|\right|^2
\end{equation}

\subsection{Matrix optimization}
\label{sub:edge_matrix}

Besides relaxing the $\max$ operator from \eqref{eq:edge_full}, another solution to remove it would
be to increase $k$ to be equal to $E$. As we already mentioned, this makes the problem trivial. To
avoid this, we add the constraint that all those $|E|$ directions are linear combination of a small
number $k$ of basis directions. In order to formulate this more clearly, we rewrite the previous
objectives with matrices. We first show this can express in a form that is amenable to the
Frank--Wolfe algorithm. Then we take the alternative road of explicit low-rank matrix factorization,
and we conclude by commenting on these approach.

\subsubsection{\fwa{} method}
\label{ssub:edge_frank_wolfe}

Seeking one direction for every edge, our goal is thus to maximize
$\sum_{u,v\in E} {s_{uv}}^T w_{uv}$. This objective can be written in matricial form as $\tr(SW) =
\frop{S^T}{W}$, where $S \in \Rbb^{|E| \times d}$ is the $s_{uv}$ vectors for all edges stacked
vertically and $W \in \Rbb^{d \times |E|}$ is the matrix of all directions stacked horizontally
and $\frop{A}{B} = \tr\left(A^T B\right)$ is the Frobenius inner product of two real matrices.
More specifically, because of the unit norm constraint on directions, we look for $W$ in the set
$\mathbb{M}^{d\times |E|}$ of $d\times |E|$ matrices with unit $\ell_2$ norm columns. To avoid the
trivial solution of letting each direction $w_{uv}$ merely be $\frac{s_{uv}}{\|s_{uv}\|}$, we
further constraint $W$ to be of low rank. That is, what we want to optimize is \[\min_{W\in
\mathbb{M}^{d\times |E|}} -\frop{S^T}{W} + \rank(W)\,.\] However, while the Frobenius inner product is
linear in $W$, the rank operator is highly irregular and optimizing it directly is \NPc{}.  Therefore,
we relax the rank term by the nuclear norm of $W$, defined as $\|W\|_* = \sum_{i=1}^{\min(d,|E|)}
\sigma_i(W)$, where $\sigma_i(W)$ is the \ith{} largest singular value of $W$. The intuition is
that the rank of $W$ is
equal to the number of non-zero singular value, while the nuclear norm is essentially an $\ell_1$ norm on
the spectrum of $W$. Therefore, as minimizing the $\ell_1$ enforces sparsity, minimizing the nuclear
norm tends to make the rank smaller by ensuring the presence of zeros in the singular values of $W$.
\footnote{As mentioned in \autoref{sec:troll_related} about \autocite{OnlineCompletion17}, a tighter
convex relaxation of the low-rank constraint is the max norm, but we do not pursue that venue
further.} This
results in the following optimization problem: \[\min_{W\in \mathbb{M}^{d\times |E|}} -\frop{S^T}{W}
+ \lambda \|W\|_*\,,\] where $\lambda$ is a regularization parameter. Since $\|\cdot\|_*$ is a norm, this
objective function is convex (although the domain over which we optimize is not), but it does not
have a gradient, forcing us to rely on potentially costly proximal gradient descent
methods~\autocite{Parikh2013a}. Instead, we consider the equivalent problem obtained by replacing
the regularization parameter with an upper bound $\delta$ on the nuclear norm of $W$, yielding:
\begin{equation*}
  \min_{\substack{W\in \mathbb{M}^{d\times |E|} \\ \|W\|_* \leq \delta}} -\frop{S^T}{W}\,.
\end{equation*}

At this stage, we could also add the node loss term \eqref{eq:edge_node_loss}, express in matricial
form as $\|X - B -
C\left(A\circ W^T \right)\|_F^2$, where $B\in \Rbb^{|V| \times d}$ is the node bias vectors $b_u$
stacked vertically, $C \in \Rbb^{|V| \times |E|}$ is the incidence matrix of $G$, $A \in \Rbb^{|E|
\times d}$ is the matrix whose each row is the corresponding $a_{uv}\onev$ vector and $\|\cdot\|_F$
denotes the Frobenius norm. Note that this term is jointly convex in $A$, $B$ and $W$.\footnote{As
one can check that the inequality defining convexity holds for this function.} However, for the
clarity of exposition, we refrain to do so for now.

The remaining issue is that $W\in \mathbb{M}^{d\times |E|}$ is not a convex constraint. As in the
previous formulation, we thus relax it by simply imposing that the $\ell_2$ norm of the columns of
$W$ are not too large. In fact, we already have a bound on these column norms, because of the
nuclear norm constraint.
\begin{prop}
For any $W\in \Rbb^{d\times |E|}$ such that $\|W\|_* \leq \delta$, the \ith{} column $W_{:,i}$ of
$W$ satisfies $\|W_{:,i}\|_2 \leq \delta\sqrt{d}$.
\end{prop}
\begin{proof}
  Let $W$ be a matrix in $\Rbb^{d\times m}$ with $d < m$ such that $\|W\|_* \leq \delta$.
  Furthermore, let its singular value decomposition be such that we can write $W = \sum_{l=1}^d
  \sigma_l u_l v_l^T$, where $u_l$ and $v_l$ are $d$ unit-norm vectors, of dimension $d$ and $m$
  respectively. We can thus express the general term $W_{k,i}$ as $\sum_{l=1}^d \sigma_l v_{l,i}
  u_{l,k}$. Because $v_l$ is unit norm, $v_{l,i} \leq 1$ and $$W_{k,i}^2 \leq \left( \sum_{l=1}^d
  \sigma_l u_{l,k} \right)^2 \leq \sum_{l=1}^d \sigma_l^2 \sum_{l=1}^d u_{l,k}^2$$ using the
  Cauchy--Schwartz inequality. The norm of the \ith{} column of $W$ then satisfies:
  \begin{equation*}
    \|W_{:,i}\|_2 = \left( \sum_{k=1}^m W_{k,i}^2 \right)^\lhalf
    \leq \left( \sum_{k=1}^m \left( \sum_{l=1}^d \sigma_l^2 \sum_{l=1}^d u_{l,k}^2 \right) \right)^\lhalf
    = \left( \left( \sum_{l=1}^d \sigma_l^2 \right) \left( \sum_{l=1}^d \sum_{k=1}^m u_{l,k}^2 \right) \right)^\lhalf
  \end{equation*}
  From $\sum_{l=1}^d |\sigma_l | \leq \delta$, we have that $\sum_{l=1}^d \sigma_l^2 \leq \delta^2$.
  Combined with $u_l$ being unit norm, we conclude that $\|W_{:,i}\|_2 \leq \delta\sqrt{d}$.
\end{proof}
However, setting $\delta = \nicefrac{1}{\sqrt{d}}$ is too stringent in high dimension, and because
this inequality is rather loose, it will likely results in very small columns norm. Instead of
relying solely on the nuclear norm constraint, we therefore add an additional regularization term
equal to $\mu \sum_{i=1}^{|E|} \|W_{:,i}\|_2 = \mu \frop{W}{W}$. Our final convex formulation is
then:
\begin{equation}
  \label{eq:edge_FW}
  \min_{\substack{W\in \Rbb^{d\times |E|} \\ \|W\|_* \leq \delta}} \frop{\mu W - S^T}{W} \,.
\end{equation}

Because \eqref{eq:edge_FW} is of the form $\min_{x\in \mathcal{X}} f(x)$ where $f$ and $\mathcal{X}$
are convex, we can solve it using the Frank--Wolfe algorithm~\autocites{FrankWolfe56}{Jaggi2013a},
which enjoy properties like a $O(\frac{1}{t})$ convergence rate, a low computational cost by avoiding
projection step and the sparsity of its solution. It is an iterative procedure that maintains an estimate
$x^{(t)}$ of the solution and works succinctly as follow: it linearizes the objective $f$ at the
current position $x^{(t)}$ by computing $\nabla f(x^{(t)})$, it finds a minimum $s^{(t)}$ of that
linearization within the domain $\mathcal{X}$ by solving $\argmin_{s\in \mathcal{X}}
\innerp{s}{\nabla f(x^{(t)})}$ and it moves toward that minimizer by a step $\gamma$, setting
$x^{(t+1)} = (1-\gamma) x^{(t)} + \gamma s^{(t)}$.\footnote{$\gamma$ is either found by a line search or,
in our case, set to $\nicefrac{t}{2+t}$.} In the case of \eqref{eq:edge_FW}, the linear minimization step amounts
to $\argmin_{\|W\|_* \leq \delta} 2\mu W - S^T = \argmax_{\|W\|_* \leq \delta} S^T - 2\mu W$.
Moreover, the $\delta$-ball of the nuclear norm is the convex hull of the matrix of the form $\delta
u v^T$ with $\|u\|_2 = 1 = \|v\|_2$~\autocite{Jaggi2013a}. By letting $u_1^{(t)}$ and $v_1^{(t)}$ be
respectively the left and right vectors associated with the largest singular value of $S^T - 2\mu
W^{(t)}$, the minimizer $s^{(t)}$ we are looking for is therefore $\delta u_1^{(t)} {v_1^{(t)}}^T$,
and we have $W^{(t+1)} = (1-\gamma) W^{(t)} + \frac{t}{2+t} \delta u_1^{(t)} {v_1^{(t)}}^T$. We
perform a small number $T$ of such iterations, and these two eigenvectors are computed in a
time linear in the number of non-zero entries of $S^T - 2\mu W^{(t)}$~\autocite{topEigenvalue92}.
Thus the overall complexity of this method, which we call \fwa{}, is $O(Td|E|)$.

As said at the beginning of this section, the matrix formulation is not exactly solving
\autoref{p:edge_full}. However, for evaluation purposes, here we describe two ways to extract $k$
directions and an edge assignment from the matrix $W$ obtained through this \fwa{} method. The first
is rather straightforward. It performs a $k$-means clustering of the columns of $W$ and use it as
assignment, while the directions are the normalized cluster centers. The second involves an extra
optimization step. Specifically, letting $r$ be the rank of $W$, we first compute a reduced SVD such
that $W = U \Sigma V^T$ and let $P = U \in \mathbb{M}^{d\times r}$ and $Q^T = \Sigma V^T \in \Rbb^{r
\times |E|}$. We then note that for any invertible matrix $R \in \Rbb^{r \times r}$, we have $W =
PR^{-1} RQ^T$. Thus we look for a matrix $R$ that makes the columns of $RQ^T$ as close as possible
of having unit $\ell_2$-norm, that is $R = \min_{R \in \Rbb^{r \times r}} \sum_{i=1}^E \left| \left\|
{RQ^T}_{:,i} \right\|_2^2 - 1 \right|$. Finally, as before,  we cluster the columns of $RQ^T$ using
$k$-means, and use the columns of $PR^{-1}$ as basis directions, noting there are not
necessarily unit norm anymore.

\subsubsection{\pqt{} low rank factorization}

We can avoid the rank regularization term altogether by making the low-rank decomposition of $W$
explicit, that by writing $W=PQ^T$ and optimizing over both $P$ and $Q$. We thus call this method
\pqt{}. However, this comes at cost
of convexity. Indeed, given a rank $k\leq \min(d,|E|)$, the problem becomes, assuming $A$ and $B$
are fixed:
\begin{equation}
  \label{eq:edge_PQ}
  \min_{P\in\mathbb{M}^{d\times k}, Q\in\mathbb{M}^{|E|\times k}} \quad \tilde{h}(P, Q) =
  - \frop{S}{PQ^T} + \mu \|X - B - C\left(A\circ QP^T \right)\|_F^2
\end{equation}
While \eqref{eq:edge_PQ} is not jointly convex in both $P$ and $Q$, it is convex in one of
them when the other is fixed. Therefore it can be solved via alternating optimization over $P$ and
$Q$ using standard (projected) gradient descent algorithms, with the following partial derivatives:
\begin{align*}
  \frac{\partial \tilde{h}}{\partial P} &=
  \frac{1}{|E|} S^T Q + \frac{2\mu}{|V|}
  A^T \circ (((X-B)^T -A^T \circ (PQ^T )C^T )C)Q 
  \\
  \frac{\partial \tilde{h}}{\partial Q} &=
  \frac{1}{|E|} S P + \frac{2\mu}{|V|}
  A\circ (C^T (X-B-C(A\circ (QP^T ))))P.
\end{align*}
Because the problem is not jointly convex, we are less concerned with the domains of the variables
not being convex, as we anyway have no guarantee to find a global minimum. We also do not add a
regularization term of the norm of the columns of $PQ^T$. Indeed, since $P\in\mathbb{M}^{d\times k}$
and $Q\in\mathbb{M}^{|E|\times k}$, each column of the resulting $PQ^T$ is a linear combination of
$k$ unit norm vectors of $P$ with weights whose square sum to $1$ and thus their norm is bounded by
$\sqrt{k}$. As opposed to all the previous methods, computing the full gradient is not linear in the
number of edges. Indeed, computing the product $S^T Q$ requires $O(|E|^\omega)$ arithmetic
operations, where $\omega \geq 2$\footnote{$\omega$ is the optimal exponent of matrix multiplication
algorithm.}. This could be alleviate by using stochastic gradient descent. Finally, to obtain a
solution of the \ecp{} \autoref{p:edge_full}, we take the directions to be the columns of the
resulting $P$, and the assignment to be a $k$-means clustering of the rows of $Q$.

\paragraph{Discussion}

While the \pqt{} formulation is not convex, in practice we initialize it with the solution from
\combined{} and it thus converge after few iterations. Namely, we set $P_0 \in \mathbb{M}^{d\times
k}$ to be the final directions found by \combined{} and the rows of $Q_0 \in \mathbb{M}^{|E|\times
k}$ to be equal to the corresponding $e_{yhat_{uv}}$\stodo{notation}. Furthermore, in the projection
step of the $Q$ variable, we actually ensure that all the entries of $Q$ are positive, which makes
them easier to  interpret as weights for the basis directions of $P$. This is something that we
cannot guarantee on the decomposition of the matrix $W$ obtained by the \fwa{} method. On the other
hand, if we run the \fwa{} algorithm for more than $k$ iterations, the rank of $W$ can be higher
than $k$, which might allow a finer clustering of the edges.

Compared with the \combined{} approach of the previous \autoref{sub:edge_vector}, these two matrix
formulations require learning more parameters. Indeed, it increases from $dk$ to $d|E|$, as it each
edge has a different direction, albeit made up of a small number of basis directions. In the case of
\pqt{}, we could furthermore impose a sparsity constraint on the columns of $Q$ to reduces the
number of coefficient to learn. Yet this additional complexity has the advantage of allowing
overlapping clustering of the edges. This is useful in cases where two nodes have a relationship
that is best described by a combination of the $k$ base directions. Moreover, in the case of \fwa{}
this additional flexibility comes with no increase in computational cost.
