\section{Learning problem}
\label{sec:edge_problem}

As in the previous chapter, we are given an undirected, unweighted graph $G=(V,E)$. This
time though, we also have side information in addition to the graph topology. Namely, each node $u$
is associated with a $d$-dimensional feature vector $x_u \in \Rbb^d$. As an example, in a social
network, $x_u$ would be the profile of user $u$, describing $u$'s demographics and preferences. We
are also given a combination operator $s$ between feature vectors, such that $s(x_u, x_v) = s_{uv}
\in \Rbb^d$. Finally, we consider distinguished feature vectors $w_{uv}$,
along which the combination $s_{uv}$ between two nodes $u$ and $v$ is evaluated. We call such
$w_{uv}$ \emph{directions}, and constrain them to have unit norm\stodo{or bounded?}. As we will discuss, this
constraint can be seen as a way to couple the different dimensions of the profiles%
\stodo{don't remember what I meant here}.

Depending on the application, and the semantic we ascribe to links between nodes, we can design and
interpret the combination operator $s$ as a difference, a similarity or a distance. Likewise, we
also have some freedom in defining how $s_{uv}$ and $w_{uv}$ are combined to assign a single real
value $g(s_{uv}, w_{uv})$ to the edge $(u,v)$. Let us call $g(w_{uv}, s_{uv})$ the \emph{goodness}
of $w_{uv}$ with respect to the edge $(u,v)$. We view this value as how adequately the direction
$w_{uv}$ \emph{explains}, or describes, the connection between $u$ and $v$.

\medskip

We assume in the following that both the profiles and the directions are vectors with components in
$[-1, 1]$. A natural choice for the operator $s$ is then the difference between the profiles, that is
$s_{uv} = x_u - x_v$. In that spirit, if $u$ and $v$ are close along a direction $w_{uv}$, we expect
${x_u}^T w_{uv}$ to be close to ${x_v}^T w_{uv}$, as measured by $|{x_u}^T w_{uv} - {x_v}^T
w_{uv}|$. Because we prefer to work with smooth functions, we define the goodness to be $g(s_{uv},
w_{uv}) = \left({s_{uv}}^T w_{uv} \right)^2$, and we want to minimize it with respect to $w_{uv}$.
When on the \ith{} dimension ${x_u}_{;i} - {x_v}_{;i}$ is close to zero, the minimization objective
and the fact that $w_{uv}$ is
unit-norm impose that ${w_{uv}}_{;i}$ should be large. On the other hand, with this formulation,
there is no way to set ${w_{uv}}_{;i}$ in order to \enquote{highlight} the fact that
${x_u}_{;i}$ and ${x_v}_{;i}$ are widely different.
Whereas there might be situations where this is not a problem, as we discussed in the introduction,
here we seek to model heterophilic interactions, at least along a subset of dimensions. Therefore, in
the following, we will adopt a refined approach.

Namely, we define $s$ to be the Hadamard (or component wise) product: $s_{uv} = x_u \circ x_v$, and
$g$ to be the dot product: $g(s_{uv}, w_{uv}) = {s_{uv}}^T w_{uv}$. The goodness of a direction $w$
for a given edge can thus be seen as a weighted sum of the per component similarity of $u$ and $v$
profiles. Therefore, it must be maximized with respect to $w_{uv}$. For a given component $i$, when
both ${x_u}_{;i}$ and ${x_v}_{;i}$ are large and of the same sign (that is close to either $+1$ or
$-1$), we see that $u$ and $v$ agree on the \ith{} dimension, and ${w_{uv}}_{;i}$ must accordingly
be a large positive value. On the other hand, when ${x_u}_{;i}$ and ${x_v}_{;i}$ are both large but
of different signs (\eg ${x_u}_{;i} = +1$ and ${x_v}_{;i} = -1$), $u$ and $v$ strongly disagree on
the \ith{} dimension, requiring in a large negative value for ${w_{uv}}_{;i}$.  Finally, if one of
${x_u}_{;i}$ or ${x_v}_{;i}$ is zero, denoting either an unimportant feature or a missing value,
then the value of ${w_{uv}}_{;i}$ does not matter, and will be set to zero by the maximization
procedure because of the unit norm constraint on $w_{uv}$.

\iffalse
\begin{aside}
When $s$ is a distance, and therefore positive, we impose $w_{uv}\geq 0$. A link $(u,v)$ is then
well explained by $w_{uv}$ when observed differences along this direction are small. More precisely,
when the quantity ${s_{uv}}^T w_{uv}$ is close to zero. Because $w_{uv}$ is unit norm, the \ith{}
component of $w_{uv}$ should be large when the \ith{} feature of $x_u$ and $x_v$ are close. Hence, a
null value for a component $i$ of $s_{uv}$ should not mean anything else than $u$ and $v$ share a
known and similar feature $i$ (\eg{} the unknown value should be different from $0$). When $s$ is
the difference $x_u - x_v$, it can be negative and the same reasoning can be applied, but with
$w_{uv}\in [-1,1]^d$ and a link value of $\left({s_{uv}}^T w_{uv}\right)^2$.

An alternative is to consider that the combination is a similarity. Links are created when the
observed similarity is high. Again, if $s$ is positive, it seems reasonable to impose that
$w_{uv}\geq0$ (and if $s$ can be negative, then $w_{uv}$ should be in $[-1,1]$). One main advantage
of similarities is maybe to avoid (or minimize) problems with missing values setting them to 0.
\end{aside}
\fi

\medskip

Based on these specific definitions of $s$ and $g$, we now formally write down the problem of
finding and assigning the most informative directions to every edge of $G$
\begin{problem}[]
  \label{p:edge_full}
  Given a graph $G=(V, E)$, node profiles $X\in [-1, 1]^{n\times d}$ and an integer $k \in \Nbb$,
  find a set of $k$ unit-norm directions $\mathcal{D}_k = \{w_1, w_2, \ldots, w_k\} \in \dsphere$
  and associate to every edge of $E$ the direction with the maximal goodness. Formally, solve
  \begin{equation}
    \label{eq:edge_full}
    \argmax_{w_1,\ldots,w_k \in \dsphere}
    \sum_{u,v \in E} \max_{\ell \in \rangesk} g(s_{uv}, w_{\ell}) =
    \sum_{u,v \in E} \max_{\ell \in \rangesk} \left(x_u \circ x_v\right)^T w_{\ell} = f(k)
  \end{equation}
  where $s_{uv} = x_u \circ x_v$ and $g(s_{uv}, w_{\ell}) = {s_{uv}}^T w_{\ell}$.
\end{problem}


\paragraph{Generalization of signed graphs}
\label{par:generalization_of_signed_graphs}

Here we show why \autoref{p:edge_full} can model signed graphs as a special case, namely with $k=2$
and specifically crafted profiles.
Remember the learning
bias we introduced in the previous chapter \vpageref{text:cc_new_bias}: every node belongs to
exactly one of the $K$ clusters available, is connected positively with the other nodes belonging to
the same cluster, and is connected negatively with the nodes belonging to every other clusters. Now
let the dimension of profiles be $d=K$ and the number of directions $k=2$. Moreover, we define the
two direction vectors as $w_1 = \frac{\onev}{\sqrt{d}} = w^+$ and $w_2 = -w_+ = w_-$, where
$\onev$ is the vector of dimension $d$ with only $1$ coordinates. Finally, for a node $u$ in
cluster $i$, we assume its profile $x_u$ is set as follow: all component are equal to $-b$ except
for the \ith{} one, which is equal to $a$, where $a$ and $b$ are real constants we define now.

First, because $x_u$ is a unit vector, we have that $a^2 + (K-1)b^2=1$, from which $a =
\sqrt{1-(K-1)b^2}$ whenever $1-(K-1)b^2 \geq 0$, or equivalently $b \leq \frac{1}{\sqrt{K-1}}$.
Next, we look at $\left(x_u \circ x_v\right)^T \onev = {s_{uv}}^T \onev = \frac{1}{\sqrt{d}}
\sum_{i=1}^d  {s_{uv}}_{;i}$ for any edge $(u,v) \in E$. If $u$ and $v$ are in the same cluster, then
$\sum_{i=1}^d  {s_{uv}}_{;i} = a^2 + (K-1)b^2 = 1$, implying that ${s_{uv}}^T w_+ = \frac{1}{d} >
{s_{uv}}^T w_- = -\frac{1}{d}$. Otherwise, $\sum_{i=1}^d  {s_{uv}}_{;i} = (K-2)b^2 - 2ab = (K-2)b^2 -
2b\sqrt{1-(K-1)b^2} = f_K(b)$. By computing the derivative of $f_K$ with respect to $b$, we find
that it reaches a minimum of $-\frac{1}{K-1}$ at $b = \frac{1}{\sqrt{K(K-1)}} \leq
\frac{1}{\sqrt{K-1}}$. In this case, ${s_{uv}}^T w_+ = -\frac{1}{(K-1) \sqrt{d}} < {s_{uv}}^T w_- =
\frac{1}{(K-1) \sqrt{d}}$. To maximize $\sum_{(u,v)\in E} \max_{1 \leq \ell \leq d} w_\ell^T
s_{uv}$, we therefore assign $w_+$ to edges whose both endpoints belong to the same cluster, and
$w_-$ to edges joining two different clusters.

Moreover, given this assignment, we can show that $w_+$ and $w_-$ are the optimal directions, under
the additional uniformity assumption that there are exactly $p$ edges within every cluster, and $q$
edges between any two clusters. First, we define
\begin{equation*}
  s_{\mathrm{inner}} = \sum_{(u,v) \in E;\, \cluster(u)=\cluster(v)} s_{uv} \qquad \text{and} \qquad
  s_{\mathrm{outer}} = \sum_{(u,v) \in E;\, \cluster(u)\neq \cluster(v)} s_{uv}
\end{equation*}
to be the sum of all edge vectors within and between clusters respectively. By our choice of $a$ and
$b$, we have that $s_{\mathrm{inner}} = p\onev$ and $s_{\mathrm{outer}} = -\frac{q}{2}\onev$. Now
recall that the solution of $\argmax_{w\in \dsphere} w^T c$ is $\frac{c}{||c||}$. Therefore, the
best direction to assign to inner edges is indeed $w_+$, and so is $w_-$ to outer edges.

\begin{aside}
For inner edge vectors, $s_{uv}$ in cluster $i$ has $a^2$ as its \ith{} component, and $b^2$
elsewhere. There are $p$ of them in cluster $i$, so summing over all clusters we get
$s_{\mathrm{inner}} = p\left(a^2 + (K-1)b^2\right)\onev = p\onev$.
For outer edge vectors, we first consider the $q$ edges between $C_1$ and $C_2$,
whose $s_{uv}$ is
$\begin{pmatrix}
  -ab \\
  -ab \\
  b^2 \\
  b^2 \\
  \vdots \\  
\end{pmatrix}$. Likewise, between $C_1$ and $C_3$ we have
$s_{uv}=\begin{pmatrix}
  -ab \\
  b^2 \\
  -ab \\
  b^2 \\
  \vdots \\  
\end{pmatrix}$. We thus we see that the first component is always $-ab$, that there is another
such term at the other cluster index, while the rest is $b^2$. Summing all the vectors incident to
$C_1$, we get
$q\begin{pmatrix}
  -ab(K-1) \\
  b^2(K-2) -ab \\
  b^2(K-2) -ab \\
  \vdots \\  
\end{pmatrix}$. Similarly, the sum of all vectors incident to $C_2$ is
$q\begin{pmatrix}
  b^2(K-2) -ab \\
  -ab(K-1) \\
  b^2(K-2) -ab \\
  \vdots \\  
\end{pmatrix}$, and so on for every cluster $i$. Summing over all clusters and dividing by $2$ to
avoid double counting, we get $s_{\mathrm{outer}} = \frac{q}{2}\left( (K-1)\left(b^2(K-2) -ab\right)
-ab(K-1) \right)\onev = \frac{q(K-1)}{2}\left(b^2(K-2) -2ab\right)\onev =
\frac{q(K-1)}{2}f_K(b)\onev = -\frac{q(K-1)}{2} \frac{1}{K-1} \onev = -\frac{q}{2}\onev$.
\end{aside}

Granted, the situation where all the nodes of a cluster have exactly the same profile and where the
connections are so regular is highly idealized. Moreover, we did not formally prove that there is
not another assignment (with different direction vectors) that would give a better objective
value\footnote{Although given the symmetry of the problem, that would be surprising.}. Our point is
rather that profiles and directions can be seen as a \enquote{latent} explanation for the signs in
a balanced signed graphs, like our generative model of the first chapter.

\bigskip

At the other extreme of having only $k=2$ directions, when $k=|E|$, the topology of the graph
becomes irrelevant, as we can simply exploit the trivial
solution of having a single direction $w_{uv} = \frac{s_{uv}}{||s_{uv}||}$ for every edge $(u,v)$.
More generally, as $k$ increases, we expect the value $\max_{\mathcal{D}_k} f(k)$ to increase as
well, at the cost of interpretability. There could be a principled approach to finding the
\enquote{best} $k$, based on a information theoretical measure of the complexity of $\mathcal{D}_k$
and the minimum description length principle~\autocite{grunwald2005tutorial}. However, in the
interest of simplicity, in the following we focus on the formulation stated in
\autoref{p:edge_full}, where $k$ is given. In practice, we further constrain $k$ to be small (say
less than \np{10}). Within the multilayer framework, $k$ could also be seen as the number of
underlying layers. To retain a fair level of interpretability, we deem appropriate to have $k$ not
too large.

In addition to assume that the input graph is the superposition of a small number of layers,
another way to leverage the graph topology is to define local constraints at the node level. 
Recall that we consider node profiles to be normalized and having unit norm. A practical
justification is that if nodes are
users of a social network and the profiles measure their activity across several domains, this
allows to compare users with various level of total activity. One way to integrate that fact in
our optimization formulation is to assume that profiles are linear combination of directions, that
is $x_u = b_u + \sum_{v \in \nei(u)} a_{uv} w_{uv}$, where $a_{uv}$ are real coefficients and $b_u
\in \Rbb^d$. Viewed in the other direction, this can be interpreted as follow: each time a node $u$
connect to another node $v$ along a direction $w_{uv}$, it consume a part of its profile
proportional to $w_{uv}$. Therefore, we subtract the term 
\begin{equation}
  \label{eq:edge_node_loss}
  \mathcal{L}_{\mathrm{node}} = 
  \sum_{u\in V} \left|\left| x_u - b_u - \sum_{v \in \nei(u)} a_{uv} w_{uv} \right|\right|^2
\end{equation}
from \eqref{eq:edge_full}.

We can also make the assumption that each node $u$ is only involved in $k_\mathrm{local} < k$
different directions. The intuition is that when $d$ is large enough, users have to focus their
energy and can only express their interest in a few number of dimensions. In other words, all the
edges incident to $u$ can only be associated with one of $k_\mathrm{local}$ directions. For
certain value of
$k_\mathrm{local}$, this is a \NPc{} graph colouring problem and thus cannot necessarily be enforced
exactly. Still, it can be seen as a learning bias and will be used to generate synthetic data.
Furthermore, we can also express it as an optimization constraint. Specifically, suppose that for
an edge $(u,v)$ associated with the direction $w_i \in \mathcal{D}_k$, we have a $k$-dimensional
vector $y_{u,v}$ equal to $e_i$ (that is all zero except for the \ith{} component set to one). Let
us stack all such vectors into a $|E| \times k$ matrix $Y$, and let $C \in \Rbb^{|V| \times |E|}$ be
the incidence matrix of $G$\footnote{The incidence matrix of a graph $G$, usually denoted by $B$, is
the $|V| \times |E|$ matrix such that $B_{i,j} = 1$ if the node $v_i$ and edge $e_j$ are incident
and 0 otherwise.} and $c_u$ be its $u^{th}$ row. Then $Yc_u$ is a $k$-dimensional vector that counts
how many times each direction is incident to node $u$. We thus want the $\ell_0$ norm of all $Yc_u$
(\ie its number of non-zero component) to be upper bounded by $k_\mathrm{local}$. As previously,
this is not convex but we can relax this constraint, as such sparsity inducing problem have been
well studied~\autocite{sparseOptim12}. Namely, we let $y_{uv}$ be a softmax membership vector, that
is we set $y_{uv,i} = \frac{\exp(\beta{s_{uv}}^T w_i)}{\sum_{j=1}^k \exp(\beta {s_{uv}}^T w_i)}$,
and we replace the $\ell_0$ norm by a $\ell_1$ or even a $k$-support norm~\autocite{KsupportNorm12}.
These two constraints can be combined, so that the profiles are close to a linear combination of a
small number of directions.

\section{Proposed approaches}
\label{sec:edge_methods}

After having introduced \autoref{p:edge_full} and some additional constraints, we now present
methods to solve it. We start with a naive baseline, and propose a post-processing to improve it.
Then we describe a convex relaxation. Finally, we depart slightly from finding $k$ directions by
considering a low-rank matrix formulation, where each edge is assigned a linear combination of a
small number of base directions, paving the way to overlapping clustering.

\subsection{\kmeans{} baseline and improvement}
\label{sub:edge_baseline}

A simple and natural baseline is to cluster the set of all similarity edge vectors $
\theset{s_{uv}}{(u,v) \in E}$ using the $k$-means algorithm, where $k$ is the number of directions
set in \autoref{p:edge_full}. Formally, given the graph $G=(V,E)$ and the node profiles $X$ as
input, we order the edges from $e_1$ to $e_m$ and build the matrix $S \in \Rbb^{m\times d}$ whose
\ith{} row is the similarity between the profiles of the endpoints of $e_i=(u,v)$, that is $s_{uv} =
x_u \circ x_v$. Running $k$-means will thus partition $E$ into $k$ sets of edges, call them $E_1,
\ldots, E_k$. The first drawback of that simplicity is that it does not solve \autoref{p:edge_full}.
Indeed, the partition of $E$ does not provide the set of directions $\mathcal{D}_k = \{w_1, w_2,
\ldots, w_k\} \in \dsphere$ we are looking for. A natural way of obtaining directions from that
partition is to set $w_\ell$ to be the normalized cluster center of $E_\ell$. Note however that
this does not guarantee that we have assigned the best direction to every edge. We therefore propose
the following heuristic, inspired by the Lloyd algorithm for $k$-means, but where the directions in
$\mathcal{D}_k$ play the role of centroids. Namely, we alternate between two steps
\begin{enumerate}[(i), nosep]
  \item create a new partition $\{E'_1, \ldots, E'_k\}$ by assigning every edge $(u,v)$ to the
    direction $w$ maximizing its score ${s_{uv}}^T w$, that is $E'_\ell = \theset{(u,v) \in E}{
    w_\ell = \argmax_{w_i \in \mathcal{D}_k} {s_{uv}}^T w_i}$, and
  \item update the directions to make them optimal with respect to the edges currently assigned to
    them, that is as previously, set $\mathcal{D}_k = \left\{ w_\ell = \frac{s_\ell}{\left\| s_\ell
    \right\|}\right\}_{\ell=1}^k $, where $s_\ell = \sum_{u,v \in E'_\ell} s_{uv}$ is the sum of all
    the similarity vectors of the edge in $E'_\ell$,
\end{enumerate}
until the edge assignment is stable or we reach a maximum number of iterations. By analogy with the
$k$-means algorithm, obtaining convergence guarantees in the general case would be surprising, but in
practice we expect that only a small number of iterations would be needed.

\subsection{Vector optimization}
\label{sub:edge_vector}

The difficulty in optimizing directly \eqref{eq:edge_full} stem from the $\max$ operator, which we
replace here by a convex relaxation. Indeed, one can check that given a set of numbers $S=\{a_1,
a_2, \ldots, a_{|S|}\}$ and real number $\beta > 0$, we have
\begin{equation*}
  \max_{a_i \in S} a_i = \lim_{\beta \rightarrow + \infty} \frac{1}{\beta}
  \log\left( \sum_{i=1}^{|S|} \exp{\beta a_i} \right)
\end{equation*}
Using this fact, and averaging the two components of our objective function, we can rewrite
\eqref{eq:edge_full} as:
\begin{equation}
  \label{eq:edge_soft}
  \argmax_{{\substack{w_1,\ldots,w_k \in \dsphere,\\
  a_{uv}\in\Rbb,\, b_u \in \Rbb^d}}}
  \frac{1}{|E|\beta} \sum_{u,v\in E}
  \log\Bigl(\sum_{\ell=1}^k\exp\left(\beta {s_{uv}}^T w_\ell \right)\Bigr)
  - \frac{1}{|V|}\sum_{u\in V}
  \left|\left| x_u - b_u - \sum_{v \in \nei(u)} a_{uv} w_{uv} \right|\right|^2
\end{equation}
Note that in second term, because we do not know the edge assignment, we have to decide which of the
$w_1, \ldots, w_k$ is $w_{uv}$ equal to. According to our definition of the goodness function, we
want to let $w_{uv} = \argmax_{w_\ell \in \mathcal{D}_k} {s_{uv}}^T w_\ell$. However, the $\argmax$
function is not convex and we thus use the same relaxation as before. Namely, by stacking all the
vectors of $\mathcal{D}_k$ into a single vector $W$ of size $k\times d$, the relaxation is:
$$ \begin{array}[t]{lrcl}
  \mathcal{A} : & \Rbb^m & \longrightarrow & \Rbb^d \\
      & (w_1,\ldots,w_k) & \longmapsto &
  \sum_{i=1}^k \frac{\exp(\beta{s_{uv}}^T w_i)}{\sum_{j=1}^k \exp(\beta {s_{uv}}^T w_i)} w_i
\end{array} $$
This function has an explicit derivative, but it is rather costly to compute and in practice we
found automatic differentiation~\autocite{autograd15} to be more efficient. While this modified
objective function is now jointly convex in all its variables, the set $\dsphere$ is not. To ease
the optimization procedure, we thus only impose the directions $w_\ell$ to lie within the unit ball
and expect them to have norm close to one in order to maximize the dot product in the ${s_{uv}}^T
w_\ell$ term.

\subsection{Matrix optimization}
\label{sub:edge_matrix}

Another way to remove the $\max$ is to first seek one direction for every edge, that is to maximize
$\sum_{u,v\in E} {s_{uv}}^T w_{uv}$. This objective can be written in matric form as $\tr(SW) =
\frop{S^T}{W}$, where $S \in \Rbb^{|E| \times d}$ is the $s_{uv}$ vectors for all edges stacked
vertically and $W \in \Rbb^{d \times |E|}$ is the matrix of all directions stacked horizontally
and $\frop{A}{B} = \tr\left(A^T B\right)$ is the Frobenius inner product of two real matrices.
More specifically, because of the unit norm constraint on directions, we look for $W$ in the set
$\mathbb{M}^{d\times |E|}$ of $d\times |E|$ matrices with unit $\ell_2$ norm columns. To avoid the
trivial solution of letting each direction $w_{uv}$ merely be $\frac{s_{uv}}{\|s_{uv}\|}$, we
further constraint $W$ to be of low rank. That is, what we really want to optimize is \[\min_{W\in
\mathbb{M}^{d\times |E|}} -\frop{S^T}{W} + \rank(W)\,.\] However, while the Frobenius inner product is
linear in $W$, the rank operator is highly irregular and optimizing it directly is \NPc{}. Therefore
we relax the rank term by the nuclear norm of $W$, defined as $\|W\|_* = \sum_{i=1}^{\min(d,|E|)}
\sigma_i(W)$, where $\sigma_i(W)$ is the \ith{} largest singular value of $W$. The intuition is
that the rank of $W$ is
equal to the number of non-zero singular value while the nuclear norm is essentially an $\ell_1$ norm on
the spectrum of $W$. Therefore, as minimizing the $\ell_1$ enforce sparsity, minimizing the nuclear
norm tends to make the rank smaller by ensuring the presence of zeros in the singular values of $W$.
\footnote{As mentioned in \autoref{sec:troll_related} about \autocite{OnlineCompletion17}, a tighter
convex relaxation of the low-rank constraint is the max norm, but we do not pursue that venue
further.} This
results in the following optimization problem: \[\min_{W\in \mathbb{M}^{d\times |E|}} -\frop{S^T}{W}
+ \lambda \|W\|_*\,,\] where $\lambda$ is a regularization parameter. Since $\|\cdot\|_*$ is a norm, this
objective function is convex (although the domain over which we optimize is not), but it does not
have a gradient, forcing us to rely on potentially costly proximal gradient descent
methods~\autocite{Parikh2013a}. Instead, we consider the equivalent problem obtained by replacing
the regularization parameter with an upper bound $\delta$ on the nuclear norm of $W$, yielding:
\begin{equation*}
  \min_{\substack{W\in \mathbb{M}^{d\times |E|} \\ \|W\|_* \leq \delta}} -\frop{S^T}{W}\,.
\end{equation*}

At this stage, we could also add the node loss term, express in matric form as $\|X - B -
C\left(A\circ W^T \right)\|_F^2$, where $B\in \Rbb^{|V| \times d}$ is the node bias vectors $b_u$
stacked vertically, $C \in \Rbb^{|V| \times |E|}$ is the incidence matrix of $G$, $A \in \Rbb^{|E|
\times d}$ is the matrix whose each row is the corresponding $a_{uv}\onev$ vector and $\|\cdot\|_F$
denotes the Frobenius norm. Note that this term is jointly convex in $A$, $B$ and $W$.\footnote{As
one can check that the inequality defining convexity holds for this function.} However, for the
clarity of exposition, we refrain to do so for now.

The remaining issue is that $W\in \mathbb{M}^{d\times |E|}$ is not a convex constraint. As in the
previous formulation, we thus relax it by simply imposing that the $\ell_2$ norm of the columns of
$W$ are not too large. In fact, we already have one bound on these column norms, because of the
nuclear norm constraint. One can indeed show that for $W\in \Rbb^{d\times |E|}$ such that $\|W\|_*
\leq \delta$, the \ith{} column $W_{:,i}$ of $W$ satisfies $\|W_{:,i}\|_2 \leq \delta\sqrt{d}$.
However, setting $\delta = \nicefrac{1}{\sqrt{d}}$ is too stringent in high dimension, and because
this inequality is rather loose, it will likely results in very small columns norm. Instead of
relying solely on the nuclear norm constraint, we therefore add an additional regularization term
equal to $\mu \sum_{i=1}^{|E|} \|W_{:,i}\|_2 = \mu \frop{W}{W}$. Our final convex formulation is
then:
\begin{equation}
  \label{eq:edge_FW}
  \min_{\substack{W\in \Rbb^{d\times |E|} \\ \|W\|_* \leq \delta}} \frop{\mu W - S^T}{W} \,.
\end{equation}

\begin{aside}
  Let $W$ be a matrix in $\Rbb^{d\times m}$ with $d < m$ such that $\|W\|_* \leq \delta$.
  Furthermore, let its singular value decomposition be such that we can write $W = \sum_{l=1}^d
  \sigma_l u_l v_l^T$, where $u_l$ and $v_l$ are $d$ unit-norm vectors, of dimension $d$ and $m$
  respectively. We can thus express the general term $W_{k,i}$ as $\sum_{l=1}^d \sigma_l v_{l,i}
  u_{l,k}$. Because $v_l$ is unit norm, $v_{l,i} \leq 1$ and $W_{k,i}^2 \leq \left( \sum_{l=1}^d
  \sigma_l u_{l,k} \right)^2 \leq \sum_{l=1}^d \sigma_l^2 \sum_{l=1}^d u_{l,k}^2$, using
  Cauchy--Schwartz inequality. The norm of the \ith{} column of $W$ then satisfies:
  \begin{equation*}
    \|W_{:,i}\|_2 = \left( \sum_{k=1}^m W_{k,i}^2 \right)^\lhalf
    \leq \left( \sum_{k=1}^m \left( \sum_{l=1}^d \sigma_l^2 \sum_{l=1}^d u_{l,k}^2 \right) \right)^\lhalf
    = \left( \left( \sum_{l=1}^d \sigma_l^2 \right) \left( \sum_{l=1}^d \sum_{k=1}^m u_{l,k}^2 \right) \right)^\lhalf
  \end{equation*}
  From $\sum_{l=1}^d |\sigma_l | \leq \delta$, we have that $\sum_{l=1}^d \sigma_l^2 \leq \delta^2$.
  Combined with $u_l$ being unit norm, we conclude that $\|W_{:,i}\|_2 \leq \delta\sqrt{d}$.
\end{aside}

Because \eqref{eq:edge_FW} is of the form $\min_{x\in \mathcal{X}} f(x)$ where $f$ and $\mathcal{X}$
are convex, we can solve it using the Frank--Wolfe algorithm~\autocites{FrankWolfe56}{Jaggi2013a},
which enjoy properties like a $O(\frac{1}{t})$ convergence rate, low computational cost by avoiding
projection step and sparse solution. It is an iterative procedure that maintains an estimate
$x^{(t)}$ of the solution and works succinctly as follow: it linearizes the objective $f$ at the
current position $x^{(t)}$ by computing $\nabla f(x^{(t)})$, it finds a minimum $s$ of that
linearization within the domain $\mathcal{X}$ by solving $\argmin_{s\in \mathcal{X}}
\innerp{s}{\nabla f(x^{(t)})}$ and it moves toward that minimizer by a step $\gamma$, setting
$x^{(t+1)} = (1-\gamma) x^{(t)} + \gamma s$.\footnote{$\gamma$ is either found by a line search of
set to $\nicefrac{t}{2+t}$.} In the case of \eqref{eq:edge_FW}, the linear minimization step amounts
to $\argmin_{\|W\|_* \leq \delta} 2\mu W - S^T = \argmax_{\|W\|_* \leq \delta} S^T - 2\mu W$.
Moreover, the $\delta$-ball of the nuclear norm is the convex hull of the matrix of the form $\delta
u v^T$ with $\|u\|_2 = 1 = \|v\|_2$~\autocite{Jaggi2013a}. By letting $u_1$ and $v_1$ be
respectively the left and right vectors associated with the largest singular value of $S^T - 2\mu
W$, the minimizer \enquote{$s$} we are looking for is therefore $\delta u_1 v_1^T$.

\medskip

We can avoid the rank regularization term altogether by making low-rank decomposition of $W$
explicit, that by writing $W=PQ^T$ and optimizing over both $P$ and $Q$. However, this comes at cost
of convexity. Indeed, given a rank $k\leq \min(d,|E|)$, the problem becomes, assuming $A$ and $B$
are fixed:
\begin{equation}
  \label{eq:edge_PQ}
  \min_{P\in\mathbb{M}^{d\times k}, Q\in\mathbb{M}^{|E|\times k}} \quad \tilde{h}(P, Q) =
  - \frop{S}{PQ^T} + \mu \|X - B - C\left(A\circ QP^T \right)\|_F^2
\end{equation}
While \eqref{eq:edge_PQ} is not jointly convex in both $P$ and $Q$, it is convex in one of
them while the other is fixed. Therefore it can be solved via alternating optimization over $P$ and
$Q$ using standard (projected) gradient descent algorithms, with the following partial derivatives:
\begin{align*}
  \frac{\partial \tilde{h}}{\partial P} &=
  \frac{1}{|E|} S^T Q + \frac{2\mu}{|V|}
  A^T \circ (((X-B)^T -A^T \circ (PQ^T )C^T )C)Q 
  \\
  \frac{\partial \tilde{h}}{\partial Q} &=
  \frac{1}{|E|} S P + \frac{2\mu}{|V|}
  A\circ (C^T (X-B-C(A\circ (QP^T ))))P.
\end{align*}
Because the problem is not jointly convex, we are less concerned with the domains of the variables
not being convex, as we anyway have no guarantee to find a global minimum. We also do not add a
regularization term of the norm of the columns of $PQ^T$. Indeed, since $P\in\mathbb{M}^{d\times k}$
and $Q\in\mathbb{M}^{|E|\times k}$, each column of the resulting $PQ^T$ is a linear combination of
$k$ unit norm vectors of $P$ with weights whose square sum to $1$ and thus their norm is bounded by
$\sqrt{k}$.

Compared with \refeq{eq:edge_soft}, these two matrix formulations require learning more parameters,
as it each edge has a different direction, albeit made up of a small number of basis directions. In
the second case, we could furthermore impose a sparsity constraint on $Q$.
