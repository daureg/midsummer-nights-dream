\section{Settings and learning problem}
\label{sec:edge_model}

As in the previous chapters, we are given an unweighted graph $G=(V,E)$\stodo{undirected?}. This
time though, we also have side information in addition to the graph topology. Namely, each node $u$
is associated with a $d$-dimensional feature vector $x_u \in \Rbb^d$. As an example, in a social
network, $x_u$ would be the profile of user $u$, describing $u$'s demographics and preferences. We
are also given a combination operator between feature vectors $s$ such that $s(x_u, x_v) = s_{uv}
\in \Rbb^d$. Finally, we consider distinguished feature vectors $w_{uv}$\stodo{is it better to
introduce $w$ with a single $\ell$ index, to emphasise that later, it will not be unique per edge?},
along which the combination $s_{uv}$ between two nodes $u$ and $v$ is evaluated. We call such
$w_{uv}$ \emph{directions}, and constrain them to have unit norm. As we will discuss, this
constraint can be seen as a way to couple the different dimensions of the profiles.

Depending on the application, and the semantic we ascribe to links between nodes, we can design and
interpret the combination operator $s$ as a difference, a similarity or a distance. Likewise, we
also have some freedom in defining how $s_{uv}$ and $w_{uv}$ are combined to assign a single real
value $g(s_{uv}, w_{uv})$ to the edge $(u,v)$. Let us call $g(w_{uv}, s_{uv})$ the \emph{goodness}
of $w_{uv}$ with respect to the edge $(u,v)$. We view this value as how adequately the direction
$w_{uv}$ \emph{explains}, or describes, the connection between $u$ and $v$.

\medskip

We assume in the following that both the profiles and the directions are vectors with components in
$[-1, 1]$. A natural choice for the operator $s$ is then the difference between the profile, that is
$s_{uv} = x_u - x_v$. In that spirit, if $u$ and $v$ are close along a direction $w_{uv}$, we expect
${x_u}^T w_{uv}$ to be close to ${x_v}^T w_{uv}$, as measured by $|{x_u}^T w_{uv} - {x_v}^T
w_{uv}|$. Because we prefer to work with smooth functions, we define the goodness to be $g(s_{uv},
w_{uv}) = \left({s_{uv}}^T w_{uv} \right)^2$, and we want to minimize it with respect to $w_{uv}$.
When ${x_u}_{;i} - {x_v}_{;i}$ is close to zero, the minimization and the fact that $w_{uv}$ is
unit-norm impose that ${w_{uv}}_{;i}$ should be large. On the other hand, with this formulation,
there is no way to set ${w_{uv}}_{;i}$ in order to \enquote{reward/favor} the fact that
${x_u}_{;i}$ and ${x_v}_{;i}$ are widely different.
Whereas there might be situations where this is not a problem, as we discussed in the introduction,
here we seek to model heterophilic interactions, at least along a subset of dimensions. Therefore, in
the following, we will adopt a refined approach.

Namely, we define $s$ to be the Hadamard (or component wise) product: $s_{uv} = x_u \circ x_v$, and
$g$ to be the dot product: $g(s_{uv}, w_{uv}) = {s_{uv}}^T w_{uv}$. The goodness of a direction $w$
for a given edge can thus be seen as a weighted sum of the per component similarity of $u$ and $v$
profiles. Therefore, it must be maximized with respect to $w_{uv}$. For a given component $i$, when
both ${x_u}_{;i}$ and ${x_v}_{;i}$ are large and of the same sign (that is close to either $+1$ or
$-1$), we see that $u$ and $v$ agree on the \ith{} dimension, and ${w_{uv}}_{;i}$ must accordingly
be a large positive value. On the other hand, when ${x_u}_{;i}$ and ${x_v}_{;i}$ are both large but
of different signs (\eg ${x_u}_{;i} = +1$ and ${x_v}_{;i} = -1$), $u$ and $v$ strongly disagree on
the \ith{} dimension, requiring in a large negative value for ${w_{uv}}_{;i}$.  Finally, if one of
${x_u}_{;i}$ or ${x_v}_{;i}$ is zero, denoting either an unimportant feature or a missing value,
then the value of ${w_{uv}}_{;i}$ does not matter, and will be set to zero by the maximization
procedure because of the unit norm constraint on $w_{uv}$.

\begin{aside}
When $s$ is a distance, and therefore positive, we impose $w_{uv}\geq 0$. A link $(u,v)$ is then
well explained by $w_{uv}$ when observed differences along this direction are small. More precisely,
when the quantity ${s_{uv}}^T w_{uv}$ is close to zero. Because $w_{uv}$ is unit norm, the \ith{}
component of $w_{uv}$ should be large when the \ith{} feature of $x_u$ and $x_v$ are close. Hence, a
null value for a component $i$ of $s_{uv}$ should not mean anything else than $u$ and $v$ share a
known and similar feature $i$ (\eg{} the unknown value should be different from $0$). When $s$ is
the difference $x_u - x_v$, it can be negative and the same reasoning can be applied, but with
$w_{uv}\in [-1,1]^d$ and a link value of $\left({s_{uv}}^T w_{uv}\right)^2$.

An alternative is to consider that the combination is a similarity. Links are created when the
observed similarity is high. Again, if $s$ is positive, it seems reasonable to impose that
$w_{uv}\geq0$ (and if $s$ can be negative, then $w_{uv}$ should be in $[-1,1]$). One main advantage
of similarities is maybe to avoid (or minimize) problems with missing values setting them to 0.
\end{aside}

\medskip

Based on these specific definitions of $s$ and $g$, we now formally write down the problem of
finding and assigning the most informative directions to every edge of $G$
\begin{problem}[]
  \label{p:edge_full}
  Given a graph $G=(V, E)$, node profiles $X\in [-1, 1]^{n\times d}$ and an integer $k \in \Nbb$,
  find a set of $k$ unit-norm directions $\mathcal{D}_k = \{w_1, w_2, \ldots, w_k\} \in \dsphere$
  and associate to every edge of $E$ the direction with the maximal goodness. Formally, solve
  \begin{equation}
    \label{eq:edge_full}
    \argmax_{w_1,\ldots,w_k \in \dsphere}
    \sum_{u,v \in E} \max_{\ell \in \rangesk} g(s_{uv}, w_{\ell}) =
    \sum_{u,v \in E} \max_{\ell \in \rangesk} \left(x_u \circ x_v\right)^T w_{\ell} = f(k)
  \end{equation}
  where $s_{uv} = x_u \circ x_v$ and $g(s_{uv}, w_{\ell}) = {s_{uv}}^T w_{\ell}$.
\end{problem}


\paragraph{Generalization of signed graphs}
\label{par:generalization_of_signed_graphs}

Here we show why \autoref{p:edge_full} can model signed graphs as a special case, namely with $k=2$.
Remember the learning
bias we introduced in the previous chapter \vpageref{text:cc_new_bias}: every node belongs to
exactly one of the $K$ clusters available, is connected positively with the other nodes belonging to
the same cluster, and is connected negatively with the nodes belonging to every other clusters. Now
let the dimension of profiles be $d=K$ and the number of directions $k=2$. Moreover, we define the
two directions vectors as $w_1 = \frac{\onev}{\sqrt{d}} = w^+$ and $w_2 = -w_+ = w_-$, where
$\onev$ is the vector of dimension $d$ with only $1$ coordinates. Finally, for a node $u$ in
cluster $i$, we assume its profile $x_u$ is set as follow: all component are equal to $-b$ except
for the \ith{} one, which is equal to $a$, where $a$ and $b$ are constants we define now.

First, because $x_u$ is a unit vector, we have that $a^2 + (K-1)b^2=1$, from which $a =
\sqrt{1-(K-1)b^2}$ whenever $1-(K-1)b^2 \geq 0$, or equivalently $b \leq \frac{1}{\sqrt{K-1}}$.
Next, we look at $\onev^T\left(x_u \circ x_v\right) = \onev^T m_{uv} = \frac{1}{\sqrt{d}}
\sum_{i=1}^d m_{{uv}_i}$ for any edge $(u,v) \in E$. If $u$ and $v$ are in the same cluster, then
$\sum_{i=1}^d m_{{uv}_i} = a^2 + (K-1)b^2 = 1$, implying that $w_+^T m_{uv} = \frac{1}{d} > w_-^T
m_{uv} = -\frac{1}{d}$. Otherwise, $\sum_{i=1}^d m_{{uv}_i} = (K-2)b^2 - 2ab = (K-2)b^2 -
2b\sqrt{1-(K-1)b^2} = f_K(b)$. By computing the derivative of $f_K$ with respect to $b$, we find
that it reaches a minimum of $-\frac{1}{K-1}$ at $b = \frac{1}{\sqrt{K(K-1)}} \leq
\frac{1}{\sqrt{K-1}}$. In this case, $w_+^T m_{uv} = -\frac{1}{(K-1) \sqrt{d}} < w_-^T m_{uv} =
\frac{1}{(K-1) \sqrt{d}}$. To maximize $\sum_{(u,v)\in E} \max_{1 \leq \ell \leq d} w_\ell^T
m_{uv}$, we therefore assign $w_+$ to edges whose both endpoints belong to the same cluster, and
$w_-$ to edges joining two different clusters.

Moreover, given this assignment, we can show that $w_+$ and $w_-$ are the optimal directions, under
the additional uniformity assumption that there are exactly $r$ edges within every cluster, and $s$
edges between any two clusters. First, we define
\begin{equation*}
  m_{\mathrm{inner}} = \sum_{(u,v) \in E;\, \cluster(u)=\cluster(v)} m_{uv} \qquad \text{and} \qquad
  m_{\mathrm{outer}} = \sum_{(u,v) \in E;\, \cluster(u)\neq \cluster(v)} m_{uv}
\end{equation*}
to be the sum of all edge vectors within and between clusters respectively. By our choice of $a$ and
$b$, we have that $m_{\mathrm{inner}} = r\onev$ and $m_{\mathrm{outer}} = -\frac{s}{2}\onev$. Now
recall that the solution of $\argmax_{w\in \dsphere} w^T c$ is $\frac{c}{||c||}$. Therefore, the
best direction to assign to inner edges is indeed $w_+$, and so is $w_-$ to outer edges.

\begin{aside}
For inner edge vectors, $m_{uv}$ in cluster $i$ has $a^2$ as its \ith{} component, and $b^2$
elsewhere. There are $r$ of them in cluster $i$, so summing over all clusters we get
$m_{\mathrm{inner}} = r\left(a^2 + (K-1)b^2\right)\onev = r\onev$.
For outer edge vectors, there are a few more steps. Consider the $s$ edges between $C_1$ and $C_2$,
whose $m_{uv}$ is
$\begin{pmatrix}
  -ab \\
  -ab \\
  b^2 \\
  b^2 \\
  \vdots \\  
\end{pmatrix}$. Likewise, between $C_1$ and $C_3$ we have
$m_{uv}=\begin{pmatrix}
  -ab \\
  b^2 \\
  -ab \\
  b^2 \\
  \vdots \\  
\end{pmatrix}$. We thus we see that the first component is always $-ab$, that there is another
such term at the other cluster index, while the rest is $b^2$. Summing all the vectors incident to
$C_1$, we get
$s\begin{pmatrix}
  -ab(K-1) \\
  b^2(K-2) -ab \\
  b^2(K-2) -ab \\
  \vdots \\  
\end{pmatrix}$. Similarly, the sum of all vectors incident to $C_2$ is
$s\begin{pmatrix}
  b^2(K-2) -ab \\
  -ab(K-1) \\
  b^2(K-2) -ab \\
  \vdots \\  
\end{pmatrix}$, and so on for every cluster $i$. Summing over all clusters and dividing by $2$ to
avoid double counting, we get $m_{\mathrm{outer}} = \frac{s}{2}\left( (K-1)\left(b^2(K-2) -ab\right)
-ab(K-1) \right)\onev = \frac{s(K-1)}{2}\left(b^2(K-2) -2ab\right)\onev =
\frac{s(K-1)}{2}f_K(b)\onev = -\frac{s(K-1)}{2} \frac{1}{K-1} \onev = -\frac{s}{2}\onev$.
\end{aside}

Granted, the situation where all the nodes of a cluster have exactly the same profile and where the
connections are so regular is highly idealized. Moreover, we did not formally prove that there is
not another assignment (with different direction vectors) that would give a better objective
value\footnote{Although given the symmetry of the problem, that would be surprising.}. Our point is
rather that profiles and directions can be seen as a \enquote{latent} explanation for the signs in
a balanced signed graphs, like our generative model of the first chapter.

\bigskip

At the other extreme of $k=2$, when $k=|E|$, the topology of the graph becomes irrelevant,
as we can simply exploit the trivial
solution of having a single direction $w_{uv} = \frac{s_{uv}}{||s_{uv}||}$ for every edge $(u,v)$.
More generally, as $k$ increases, we expect the value $\max_{\mathcal{D}_k} f(k)$ to increases as
well, at the cost of interpretability. There could be a principled approach to finding the
\enquote{best} $k$, based on a information theoretical measure of the complexity of $\mathcal{D}_k$
and the minimum description length principle~\autocite{grunwald2005tutorial}. However, in the
interest of simplicity, in the following we focus on the formulation stated in
\autoref{p:edge_full}, where $k$ is given. In practice, we further constrain $k$ to be small (say
less than \np{10}). Within the multilayer framework, $k$ could also be seen as the number of
underlying layers\stodo{elaborate on that connection}.

\stodo{move that in the conclusion}
\begin{aside}
Generalizing the balance theory from signed graphs, we could imagine that in triangles or short
cycles, only certain combinations of directions are valid (or at least desirable). As a nice side
effect, this would likely require more innovative optimization algorithms. However, it sounds very
application-dependant. Furthermore, in our current setting, we do not know in advance in the
semantic of each direction, and we have no labeled edges. Therefore, it is not clear how those
constraints would be specified or whether they can be learned, if applicable. 
\end{aside}
\stodo{Specification of constraints sounds related to network schema in \autocite[Definition
3]{HINSurvey17}, except we have a single node type and are looking at higher order patter.}

Another way to leverage the graph topology is to define local constraints at the node level. Since
the beginning we considered that the user profiles were given, but from now on we further assume
that they are normalized so that they are unit norm, just like directions. In practice, if nodes are
users of a social network and the profiles measure their activity across several domains, this
allows to compare users with various level of total activity. One way to integrate that fact in
our optimization formulation is to assume that profiles are linear combination of directions, that
is $x_u = b_u + \sum_{v \in \nei(u)} a_{uv} w_{uv}$, where $a_{uv}$ are real coefficients and $b_u
\in \Rbb^d$. Viewed in the other direction, this can be interpreted as follow: each time a node $u$
connect to another node $v$ along a direction $w_{uv}$, it consume a part of its profile
proportional to $w_{uv}$. Therefore, we subtract the term $\sum_{u\in V} \left|\left| x_u - b_u -
\sum_{v \in \nei(u)} a_{uv} w_{uv} \right|\right|^2$ to \eqref{eq:edge_full}.

We can also make the assumption that each node $u$ is only involved in $k_\mathrm{local} < k$
different directions. The intuition is that when $d$ is large enough, users have to focus their
energy and can only express their interest in a few number of dimensions. In other words, all the
edges incident to $u$ can only be associated with one of $k_\mathrm{local}$ directions. This is
difficult to incorporate directly in the optimization formulation\stodo{actually it's not that
difficult}, as for certain value of
$k_\mathrm{local}$, this is a \NPc{} graph colouring problem. Still, it can be seen as a learning
bias and will be used to generate synthetic data.
These two constraints can be combined, so that the profiles are close to a linear combination of a
small number of directions.

% $\theset{(u,v) \in E}{v \in \nei(u)}$

\bigskip

In order to make the optimization more tractable, we relax the formulation of \autoref{p:edge_full}
by replacing the $\max$ inside the sum of \eqref{eq:edge_full}, either by a continuous approximation
or by a low rank matrix approach.

One can check that given a set of numbers $S=\{a_1, a_2, \ldots, a_{|S|}\}$ and real number $\beta >
0$, we have
\begin{equation*}
  \max_{a_i \in S} a_i = \lim_{\beta \rightarrow + \infty} \frac{1}{\beta}
  \log\left( \sum_{i=1}^{|S|} \exp{\beta a_i} \right)
\end{equation*}
Using this fact, and averaging the two components of our objective function, we can rewrite
\eqref{eq:edge_full} as:
\begin{equation}
  \label{eq:edge_soft}
  \argmax_{{\substack{w_1,\ldots,w_k \in \dsphere,\\
  a_{uv}\in\Rbb,\, b_u \in \Rbb^d}}}
  \frac{1}{|E|\beta} \sum_{u,v\in E}
  \log\Bigl(\sum_{\ell=1}^k\exp\left(\beta {s_{uv}}^T w_\ell \right)\Bigr)
  - \frac{1}{|V|}\sum_{u\in V}
  \left|\left| x_u - b_u - \sum_{v \in \nei(u)} a_{uv} w_{uv} \right|\right|^2
\end{equation}
Note that in second term, because we do not know the edge assignment, we have to decide which of the
$w_1, \ldots, w_k$ is $w_{uv}$ equal to. According to our definition of the goodness function, we
want to let $w_{uv} = \argmax_{w_\ell \in \mathcal{D}_k} {s_{uv}}^T w_\ell$. However, the $\argmax$
function is not convex and we thus use the same relaxation as before. Namely, by stacking all the
vectors of $\mathcal{D}_k$ into a single vector $W$ of size $k\times d$, the relaxation is:
$$ \begin{array}[t]{lrcl}
  \mathcal{A} : & \Rbb^m & \longrightarrow & \Rbb^d \\
      & (w_1,\ldots,w_k) & \longmapsto &
  \sum_{i=1}^k \frac{\exp(\beta{s_{uv}}^T w_i)}{\sum_{j=1}^k \exp(\beta {s_{uv}}^T w_i)} w_i
\end{array} $$
This function has an explicit derivative, but it is rather costly to compute and in practice we
found automatic differentiation~\autocite{autograd15} to be more efficient. While this modified
objective function is now jointly convex in all its variables, the set $\dsphere$ is not. To ease
the optimization procedure, we thus only impose the directions $w_\ell$ to lie within the unit ball
and expect them to have norm close to one in order to maximize the dot product in the ${s_{uv}}^T
w_\ell$ term.

Another way to remove the $\max$ is to first seek one direction for every edge, that is to maximize
$\sum_{u,v\in E} {s_{uv}}^T w_{uv}$. This objective can be written in matric form as $\tr(SW) =
\frop{S^T}{W}$, where $S \in \Rbb^{|E| \times d}$ is the $s_{uv}$ vectors for all edges stacked
vertically and $W \in \Rbb^{d \times |E|}$ is the matrix of all directions stacked horizontally.
More specifically, because of the unit norm constraint on directions, we look for $W$ in the set
$\mathbb{M}^{d\times |E|}$ of $d\times |E|$ matrices with unit $\ell_2$ norm columns. To avoid the
trivial solution of letting each direction $w_{uv}$ merely be $\frac{s_{uv}}{\|s_{uv}\|}$, we
further constraint $W$ to be of low rank. That is, what we really want to optimize is \[\min_{W\in
\mathbb{M}^{d\times |E|}} -\frop{S^T}{W} + \rank(W)\,.\] However, while the Frobenius inner product is
linear in $W$, the rank operator is highly irregular and optimizing it directly is \NPc{}. Therefore
we relax the rank term by the nuclear norm of $W$, defined as $\|W\|_* = \sum_{i=1}^{\min(d,|E|)}
\sigma_i(W)$, where $\sigma_i(W)$ is the \ith{} largest singular value of $W$. The intuition is
that the rank of $W$ is
equal to the number of non-zero singular value while the nuclear norm is essentially an $\ell_1$ norm on
the spectrum of $W$. Therefore, as minimizing the $\ell_1$ enforce sparsity, minimizing the nuclear
norm tends to make the rank smaller by ensuring the presence of zeros in the singular values of $W$.
\footnote{As mentioned in \autoref{sec:troll_related} about \autocite{OnlineCompletion17}, a tighter
relaxation of the low-rank constraint is the max norm, but is it convex?} This
results in the following optimization problem: \[\min_{W\in \mathbb{M}^{d\times |E|}} -\frop{S^T}{W}
+ \lambda \|W\|_*\,,\] where $\lambda$ is a regularization parameter. Since $\|\cdot\|_*$ is a norm, this
objective function is convex (although the domain over which we optimize is not), but it does not
have a gradient, forcing us to rely on potentially costly proximal gradient descent
methods~\autocite{Parikh2013a}. Instead, we consider the equivalent problem obtained by replacing
the regularization parameter with an upper bound $\delta$ on the nuclear norm of $W$, yielding:
\begin{equation*}
  \min_{\substack{W\in \mathbb{M}^{d\times |E|} \\ \|W\|_* \leq \delta}} -\frop{S^T}{W}\,.
\end{equation*}

At this stage, we could also add the node loss term, express in matric form as $\|X - B -
C\left(A\circ W^T \right)\|_F^2$, where $B\in \Rbb^{|V| \times d}$ is the node bias vectors $b_u$
stacked vertically, $C \in \Rbb^{|V| \times |E|}$ is the incidence matrix of $G$\footnote{The
  incidence matrix of a graph $G$, usually denoted by $B$, is the $|V| \times |E|$ matrix such that
$B_{i,j} = 1$ if the node $v_i$ and edge $e_j$ are incident and 0 otherwise}, $A \in \Rbb^{|E|
\times d}$ is the matrix whose each row is the corresponding $a_{uv}\onev$ vector and $\|\cdot\|_F$
denotes the Frobenius norm. Note that this term is jointly convex in $A$, $B$ and $W$.\footnote{As
one can check that the inequality defining convexity holds for this function.} However, for the
clarity of exposition, we refrain to do so for now.

The remaining issue is that $W\in \mathbb{M}^{d\times |E|}$ is not a convex constraint. As in the
previous formulation, we thus relax it by simply imposing that the $\ell_2$ norm of the columns of
$W$ are not too large. In fact, we already have one bound on these column norms, because of the
nuclear norm constraint. One can indeed show that for $W\in \Rbb^{d\times |E|}$ such that $\|W\|_*
\leq \delta$, the \ith{} column $W_{:,i}$ of $W$ satisfies $\|W_{:,i}\|_2 \leq \delta\sqrt{d}$.
However, setting $\delta = \nicefrac{1}{\sqrt{d}}$ is too stringent in high dimension, and because
this inequality is rather loose, it will likely results in very small columns norm. Instead of
relying solely on the nuclear norm constraint, we therefore add an additional regularization term
equal to $\mu \sum_{i=1}^{|E|} \|W_{:,i}\|_2 = \mu \frop{W}{W}$. Our final convex formulation is
then:
\begin{equation}
  \label{eq:edge_FW}
  \min_{\substack{W\in \Rbb^{d\times |E|} \\ \|W\|_* \leq \delta}} \frop{\mu W - S^T}{W} \,.
\end{equation}

\begin{aside}
  Let $W$ be a matrix in $\Rbb^{d\times m}$ with $d < m$ such that $\|W\|_* \leq \delta$.
  Furthermore, let its singular value decomposition be such that we can write $W = \sum_{l=1}^d
  \sigma_l u_l v_l^T$, where $u_l$ and $v_l$ are $d$ unit-norm vectors, of dimension $d$ and $m$
  respectively. We can thus express the general term $W_{k,i}$ as $\sum_{l=1}^d \sigma_l v_{l,i}
  u_{l,k}$. Because $v_l$ is unit norm, $v_{l,i} \leq 1$ and $W_{k,i}^2 \leq \left( \sum_{l=1}^d
  \sigma_l u_{l,k} \right)^2 \leq \sum_{l=1}^d \sigma_l^2 \sum_{l=1}^d u_{l,k}^2$, using
  Cauchy--Schwartz inequality. The norm of the \ith{} column of $W$ then satisfies:
  \begin{equation*}
    \|W_{:,i}\|_2 = \left( \sum_{k=1}^m W_{k,i}^2 \right)^\lhalf
    \leq \left( \sum_{k=1}^m \left( \sum_{l=1}^d \sigma_l^2 \sum_{l=1}^d u_{l,k}^2 \right) \right)^\lhalf
    = \left( \left( \sum_{l=1}^d \sigma_l^2 \right) \left( \sum_{l=1}^d \sum_{k=1}^m u_{l,k}^2 \right) \right)^\lhalf
  \end{equation*}
  From $\sum_{l=1}^d |\sigma_l | \leq \delta$, we have that $\sum_{l=1}^d \sigma_l^2 \leq \delta^2$.
  Combined with $u_l$ being unit norm, we conclude that $\|W_{:,i}\|_2 \leq \delta\sqrt{d}$.
\end{aside}

Because \eqref{eq:edge_FW} is of the form $\min_{x\in \mathcal{X}} f(x)$ where $f$ and $\mathcal{X}$
are convex, we can solve it using the Frank--Wolfe algorithm~\autocites{FrankWolfe56}{Jaggi2013a},
which enjoy properties like a $O(\frac{1}{t})$ convergence rate, low computational cost by avoiding
projection step and sparse solution. It is an iterative procedure that maintains an estimate
$x^{(t)}$ of the solution and works succinctly as follow: it linearizes the objective $f$ at the
current position $x^{(t)}$ by computing $\nabla f(x^{(t)})$, it finds a minimum $s$ of that
linearization within the domain $\mathcal{X}$ by solving $\argmin_{s\in \mathcal{X}}
\innerp{s}{\nabla f(x^{(t)})}$ and it moves toward that minimizer by a step $\gamma$, setting
$x^{(t+1)} = (1-\gamma) x^{(t)} + \gamma s$.\footnote{$\gamma$ is either found by a line search of
set to $\nicefrac{t}{2+t}$.} In the case of \eqref{eq:edge_FW}, the linear minimization step amounts
to $\argmin_{\|W\|_* \leq \delta} 2\mu W - S^T = \argmax_{\|W\|_* \leq \delta} S^T - 2\mu W$.
Moreover, the $\delta$-ball of the nuclear norm is the convex hull of the matrix of the form $\delta
u v^T$ with $\|u\|_2 = 1 = \|v\|_2$~\autocite{Jaggi2013a}. By letting $u_1$ and $v_1$ be
respectively the left and right vectors associated with the largest singular value of $S^T - 2\mu
W$, the minimizer \enquote{$s$} we are looking for is therefore $\delta u_1 v_1^T$.

\iffalse
stacked horizontally in
one matrix. Then we can directly minimize \eqref{eq:edge_full} while enforcing the matrix
$W\in\mathbb{R}^{d\times |E|}$ of all directions to be low-rank, so that each $w_{uv}$ (given by the
$(u-1)n+v$-th column of $W$) can be expressed as a linear combination of a small number of basis
vectors. Formally, if $W$ has rank $k$, then there exists $P\in\mathbb{R}^{d\times k}$ and
$Q\in\mathbb{R}^{|E|\times k}$ such that $W=PQ^T$ and $w_{uv} = Pq_{uv}^T$. $P$ defines $k$
representative directions in $\Rbb^d$ while $Q$ gives, for each edge, the weights of each
representative direction. Similar to what is done in spectral clustering, we then cluster the
$q_{uv}$'s to obtain an edge labeling.
%
Denoting by $\mathbb{M}^{d\times |E|}$ the set of $d\times |E|$ matrices with unit $L_2$ norm
columns, a convex formulation\stodo{justify the convexity later: $L_edge$ is linear and both
$L_node$ and $\|W\|_*$ are norm and thus convex} of the problem is
as follows:
\begin{align}
  \label{eq:fact_convex}
  \max_{W\in\mathbb{M}^{d\times |E|}} h(W) &=
  \mathcal{L}_{\mathrm{edge}}^{\mathrm{mat}}(W) -
  \mu \mathcal{L}_{\mathrm{node}}^{\mathrm{mat}}(W) - \lambda \|W\|_* \\
  &= \frac{1}{|E|} \tr\left(SW\right) -
  \frac{\mu}{|V|} \|X - B - C\left(A\circ W^T \right)\|_2^2 - \lambda \|W\|_*
  % \sum_{i,j \in E} {s_{uv}}^T w_{uv} + \lambda \|W\|_*,
\end{align}
where $S \in \Rbb^{|E| \times d}$ is the $s_{uv}$ vectors for all edges stacked vertically, $\mu
\geq 0$ is a trade-off parameter, $B\in \Rbb^{|V| \times d}$ is the node bias vectors $b_u$ stacked
vertically, $C \in \Rbb^{|V| \times |E|}$ is the incidence matrix of $G$\footnote{The incidence
matrix of a graph $G$, usually denoted by $B$, is the $|V| \times |E|$ matrix such that $B_{i,j} =
1$ if the node $v_i$ and edge $e_j$ are incident and 0 otherwise}, $A \in \Rbb^{|E| \times d}$ is
the matrix whose each row is the corresponding $a_{uv}\onev$ vector, $\|\cdot\|_2$ denotes the
Frobenius norm, $\|W\|_* =
\sum_{i=1}^{\min(d,|E|)}\sigma_i(W)$ is the sum of the singular values of $W$ (called the nuclear
norm or trace norm), and $\lambda \geq 0$ is a regularization parameter.\footnote{To make the
optimal solution unique, we can use an additional Frobenius norm regularization on $W$.} The nuclear norm
favors low-rank solutions as it is essentially an $L_1$ norm on the spectrum.\footnote{As mentioned
in \autoref{sec:troll_related} about \autocite{OnlineCompletion17}, a tighter relaxation of the
low-rank constraint is the max norm.}
Both $\mathcal{L}_{\mathrm{edge}}^{\mathrm{mat}}(W)$ and
$\mathcal{L}_{\mathrm{edge}}^{\mathrm{mat}}(W)$ are convex in $W$, with respective gradient
\begin{align*}
  \frac{\partial \mathcal{L}_{\mathrm{edge}}^{\mathrm{mat}}(W)}{\partial W} &=
  \frac{1}{|E|} S^T \\
  \frac{\partial \mathcal{L}_{\mathrm{node}}^{\mathrm{mat}}(W)}{\partial W} &= \frac{-2}{|V|}
  A^T \circ \left(\Big(\left(X-B\right)^T -A^T \circ \left(W C^T\right) \Big) C\right)
\end{align*}
Problem~\eqref{eq:fact_convex} can be solved using proximal gradient descent (requires singular
value decomposition at each iteration) \autocite{Parikh2013a} or Frank-Wolfe (requires only to find
the largest singular vector) \autocite{Jaggi2013a}\stodo{look at the rest of the tutorial and
comment on the difficulty of the LMO step}.
\fi

We can avoid the rank regularization term altogether by making low-rank decomposition of $W$
explicit, that by writing $W=PQ^T$ and optimizing over both $P$ and $Q$. However, this comes at cost
of convexity. Indeed, given a rank $k\leq \min(d,|E|)$, the problem becomes, assuming $A$ and $B$
are fixed:
\begin{equation}
  \label{eq:edge_PQ}
  \min_{P\in\mathbb{M}^{d\times k}, Q\in\mathbb{M}^{|E|\times k}} \quad \tilde{h}(P, Q) =
  - \frop{S}{PQ^T} + \mu \frop{} \|X - B - C\left(A\circ QP^T \right)\|_F^2
\end{equation}
While \eqref{eq:fact_nonconvex} is not jointly convex in both $P$ and $Q$, it is convex in one of
them while the other is fixed. Therefore it can be solved via alternating optimization over $P$ and
$Q$ using standard (projected) gradient descent algorithms, with the following partial derivatives:
\begin{align*}
  \frac{\partial \tilde{h}}{\partial P} &=
  \frac{1}{|E|} S^T Q + \frac{2\mu}{|V|}
  A^T \circ (((X-B)^T -A^T \circ (PQ^T )C^T )C)Q 
  \\
  \frac{\partial \tilde{h}}{\partial Q} &=
  \frac{1}{|E|} S P + \frac{2\mu}{|V|}
  A\circ (C^T (X-B-C(A\circ (QP^T ))))P.
\end{align*}
Because the problem is not jointly convex, we are less concerned with the domains of the variables
not being convex, as we anyway have no guarantee to find a global minimum. We also do not add a
regularization term of the norm of the columns of $PQ^T$. Indeed, since $P\in\mathbb{M}^{d\times k}$
and $Q\in\mathbb{M}^{|E|\times k}$, each column of the resulting $PQ^T$ is a linear combination of
$k$ unit norm vectors of $P$ with weights whose square sum to $1$ and thus their norm is bounded by
$\sqrt{k}$.

Compared with \refeq{eq:edge_soft}, these two matrix formulations require learning more parameters,
as it each edge has a different direction, albeit made up of a small number of basis directions. In
the second case, we could furthermore impose a sparsity constraint on $Q$.

A simple and natural baseline is to cluster the set of all similarity edge vectors $
\theset{s_{uv}}{(u,v) \in E}$ using the $k$-means algorithm, where $k$ is the number of directions
set in \autoref{p:edge_full}. Formally, given the graph $G=(V,E)$ and the node profiles $X$ as
input, we order the edges from $e_1$ to $e_m$ and build the matrix $S \in \Rbb^{m\times d}$ whose
\ith{} row is the similarity between the profiles of the endpoints of $e_i=(u,v)$, that is $s_{uv} =
x_u \circ x_v$. Running $k$-means will thus partition $E$ into $k$ sets of edges, call them $E_1,
\ldots, E_k$. The first drawback of that simplicity is that it does not solve \autoref{p:edge_full}.
Indeed, the partition of $E$ does not provide the set of directions $\mathcal{D}_k = \{w_1, w_2,
\ldots, w_k\} \in \dsphere$ we are looking for. A natural way of obtaining directions from that
partition is to set $w_\ell$ to be the normalized cluster center of $E_\ell$. Note however that
this does not guarantee that we have assigned the best direction to every edge. We therefore propose
the following heuristic, inspired by the Lloyd algorithm for $k$-means, but where the directions in
$\mathcal{D}_k$ play the role of centroids. Namely, we alternate between two steps
\begin{enumerate}[(i), nosep]
  \item create a new partition $\{E'_1, \ldots, E'_k\}$ by assigning every edge $(u,v)$ to the
    direction $w$ maximizing its score ${s_{uv}}^T w$, that is $E'_\ell = \theset{(u,v) \in E}{
    w_\ell = \argmax_{w_i \in \mathcal{D}_k} {s_{uv}}^T w_i}$, and
  \item update the directions to make them optimal with respect to the edges currently assigned to
    them, that is as previously, set $\mathcal{D}_k = \left\{ w_\ell = \frac{s_\ell}{\left\| s_\ell
    \right\|}\right\}_{\ell=1}^k $, where $s_\ell = \sum_{u,v \in E'_\ell} s_{uv}$ is the sum of all
    the similarity vectors of the edge in $E'_\ell$,
\end{enumerate}
until the edge assignment is stable or we reach a maximum number of iterations. By analogy with the
$k$-means algorithm, obtaining convergence guarantees in the general case would be surprising, but in
practice we expect that only a small number of iterations would be needed.
