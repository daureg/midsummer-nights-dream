\section{Attributed graphs and problem definition}
\label{sec:edge_problem}

We start by introducing some notations and terminology about edge types in node-attributed graphs.
Then we present two options to model the interactions between nodes and the role of attributes in
\enquote{explaining} edges (\autoref{sub:edge_setting}). Choosing one of these two options leads us to
formulate the \ecp{} problem in \autoref{sub:edge_constraints}. We then describe additional
constraints allowing us to take into account the topology of the graph. Finally, we elaborate on the relation
between \ecp{} and \esp{} in signed graphs.

\subsection{Setting and modelling}
\label{sub:edge_setting}

Like we wrote in the introduction, to classify edges, we make use of both the topology of the graph
and the attributes of its nodes. Namely, we are given an undirected, unweighted graph $G=(V,E)$.  We
also have side information in addition to the graph topology. It takes the form, for every node $u$,
of a $d$-dimensional feature vector $x_u \in \Rbb^d$ that we call the \emph{profile} of $u$.
Because we want to model heterophily, we more explicitly have that $x_u \in [-1, 1]^d$. To justify
this statement, let us take the example of encoding the gender of a user in a social network,
assuming there is only two values possible: male and female. Traditionally, this attribute would be
set to $1$ if the user is female and $0$ otherwise. Instead, we use $+1$ and $-1$, to clearly
emphasize the difference between the two values. The same is valid for continuous variables. For
instance, an attribute could range from $-1$ to $+1$ to represent the ideological stance of a user
on the conservative/liberal scale~\autocite{newsBias15}.\footnote{Note of course that in both cases,
negative values do not represent a judgement but are merely a reversible mathematical convenience.}
This applies even to non diverging positive quantities such as age, which can be projected to $[-1,
1]$ using pre-processing like standardization or $z$-score.

Stacking all these node profiles in a $|V| \times d$ matrix $X$, our input so far is $(G, X)$. As it
is standard in classification problem, we are also given the number of edge types
$k$.\footnote{Because our problem is essentially unsupervised, assuming that we are given $k$ is a
simplification, on which we elaborate \vpageref{par:edge_choose_k}.} Moreover, recall that we do not
merely want to classify edges into type, but also provide an explanation of why two nodes are
connected. This requires the introduction of three further concepts:
\begin{enumerate*}[1)]
\item how to represent a connection (\ie{} an edge),
\item how to represent an edge's explanation, and
\item how to evaluate which explanation is the best.
\end{enumerate*}
We now present our modelling choice:

\begin{enumerate}[1),nosep,leftmargin=*]
  \item We naturally describe an edge $(u,v)$ through the profiles of its endpoints. More precisely,
    we combine $x_u$ and $x_v$ by an operator $s$ defined as:
    \begin{equation*}
      s(u, v) = x_u \circ x_v = s_{uv} \in \Rbb^d \, ,
    \end{equation*}
    where $\circ$ stands for the Hadamard (or component wise) product. As showed in
    \begin{marginfigure}
      % \small
      \captionsetup{font=small,type=table}
      \captionof{table}{We show the effect of the Hadamard product in combining information from two
        users $u$ and $v$. Instead of actual numbers, in this table we use $+$ to denote a large
        positive value, $-$ to denote a large negative value, $0$ to denote a small absolute value
        and $\star$ to denote an arbitrary value.
      \label{tab:edge_sim}}
      \begin{tabular}{ccc}
        \toprule
        ${x_u}_{;i}$ & ${x_v}_{;i}$ & ${s_{uv}}_{;i}$ \\
        \midrule
        $+$          & $+$          & $+$             \\
        $-$          & $-$          & $+$             \\
        $+$          & $-$          & $-$             \\
        $0$          & $\star$      & $0$             \\
        \bottomrule
      \end{tabular}
    \end{marginfigure}
    \autoref{tab:edge_sim}, for each dimension $i \in \rangesd$, the $s_{uv}$ holds a value
    indicating whether the profiles of $u$ and $v$ agree, disagree or are indifferent along that
    dimension.
  \item The $k$ explanations take the form of bounded norm vectors $\mathcal{D}_k = \{w_1, w_2,
    \ldots, w_k\} \subset \dball$, where $\dball$ is the $\ell_2$-unit ball of $\Rbb^d$. We call such a
    vector $w_\ell$, for $\ell \in \rangesk$, a \emph{direction}. As we shall explain more formally
    later on, those directions are exactly the parameters learned while solving our problem.
  \item Given an edge $(u,v)$, all the $k$ directions $\{w_1, w_2, \ldots, w_k\}$ provide a possible
    explanation of why $u$ and $v$ are connected. We score these explanations by assigning them a
    \emph{goodness} of explanation\footnote{Whereas the name is inspired by the concept of
    \emph{goodness of fit}, there is no formal relation.} defined as:
    \begin{equation*}
      g(s_{uv}, w_\ell) = {s_{uv}}^T w_\ell \, .
    \end{equation*}
    As the $w$ notation hints, a direction can be interpreted as a set of weights. These weights
    indicates to which extent each attribute contributes to the connection, according to that
    direction. For a given set of directions $\mathcal{D}_k$, the classification of edges is
    naturally performed by the following operator $\mathcal{E}$, returning the index of the
    direction with the maximal goodness:
    \begin{equation*}
      \begin{array}[t]{lrcl}
        \mathcal{E} : & E \times \left( \Rbb^d \times \ldots \times \Rbb^d \right)
                      & \longrightarrow & \rangesk \\
                      & (u,v), \mathcal{D}_k & \longmapsto
                      & \argmax_{\ell \in \rangesk} g(s_{uv}, w_\ell)
      \end{array}
    \end{equation*}
    Because it will be clear from context, we lighten the notation and simply write $\mathcal{E}(u,
    v)$ instead of $\mathcal{E}\left((u, v), \mathcal{D}_k \right)$.
\end{enumerate}

This choice of $s$ and $g$ enjoy two interesting properties. First, it encompasses all cases of
homophily, indifference and heterophily. For instance, given $x_u = \begin{pmatrix} 1 & 0 & -1
\end{pmatrix}^T$ and $x_v = \begin{pmatrix} -1 & 1 & -1 \end{pmatrix}^T$, the direction $w =
\begin{pmatrix} -\nicefrac{1}{\sqrt{2}} & 0 & \nicefrac{1}{\sqrt{2}} \end{pmatrix}^T \in \dball$
achieves the maximal goodness. In doing so, it highlights that $u$ and $v$ disagree on the first
component (\ie{} heterophily), agree on the last component (\ie{} homophily) and that the middle
component is irrelevant (\ie{} indifference). This also shows that, second, given the linear nature
of the goodness $g$ once $s_{uv}$ has been computed, it provides interpretable explanation of why
two nodes are connected.

Those two properties of $s$ and $g$ should not be taken for granted. Indeed, 
a natural choice for the operator $s$ is the difference between the profiles, that is
$s_{uv} = x_u - x_v$. In that spirit, if $u$ and $v$ are close along a direction $w_\ell$, we expect
${x_u}^T w_\ell$ to be close to ${x_v}^T w_\ell$, as measured by $|{x_u}^T w_\ell - {x_v}^T
w_\ell|$. And because it is easier to deal with smooth functions, we define the goodness to be $g(s_{uv},
w_\ell) = -\left({s_{uv}}^T w_\ell \right)^2$, where the minus preserve the semantic than the
\enquote{better} the direction $w_\ell$, the larger the goodness. However, with this formulation,
there is no way to set ${w_\ell}_{;i}$ in order to \enquote{highlight} the fact that ${x_u}_{;i}$
and ${x_v}_{;i}$ are widely different. Furthermore, the square in the goodness expression loses the
linearity.

We conclude with a last comment on the norm of the directions. While being exactly of norm one is
not primordial, the role of bounding the norm of the directions (\ie{} having $\sum_{i=1}^d
{w_\ell}_{;i}^2 \leq B^2$) is to tie the $d$ dimensions together. Without such a constraint, and we
our choice of $s$ and $g$, we could simply study each dimension separately, whereas this bound adds
a coupling across different attributes, since increasing the weight of an attribute mechanically
affects the weights of the other attributes.
For consistency, we also consider node profiles to be normalized and having unit norm. A further
practical justification is that if nodes are users of a social network and the profiles measure
their activity across several domains, this allows to compare users with various level of total
activity.

\subsection{Learning problem and additional constraints}
\label{sub:edge_constraints}

\paragraph{Problem definition}

With working definitions of $s$ and $g$ at hand, we now formally write down the problem of
finding the directions maximizing the goodness of the graph.
\begin{problem}[\ecp{}]
  \label{p:edge_full}
  Given a graph $G=(V, E)$, node profiles $X\in [-1, 1]^{n\times d}$ and an integer $k \in \Nbb$,
  find a set of $k$ norm-bounded directions $\mathcal{D}_k = \{w_1, w_2, \ldots, w_k\} \in \dball$
  and associate to every edge of $E$ the direction with the maximal goodness. Formally, solve
  \begin{equation}
    \label{eq:edge_full}
    \argmax_{\mathcal{D}_k = \{w_1,\ldots,w_k\} \subset \dball}
    \sum_{u,v \in E}  g(s_{uv}, w_{\mathcal{E}(u,v)})
  \end{equation}
  where $s_{uv} = x_u \circ x_v$, $g(s_{uv}, w_{\ell}) = {s_{uv}}^T w_{\ell}$ and $\mathcal{E}(u,v)
  = \argmax_{\ell \in \rangesk} g(s_{uv}, w_\ell)$.
\end{problem}

\paragraph{Example}

In \autoref{fig:edge_exe}, we present a small instance of the \ecp{} problem, where we are given the
graph shown in \autoref{fig:edge_exe_graph}, whose node attributes are listed
in Table~\ref{tab:edge_exe}. The first two attributes are categorical, denoting the company employing
each user, either \textsf{G} or \textsf{F}. Instead of a classic binary one-hot encoding, we
additionally use $-1$ to denote that a user had been employed by a given company. \textsf{Gender} is
encoded as presented earlier, and we also apply a pre processing to transform \textsf{age} between
$-1$ and $1$. Finally, \textsf{opinion} represents the view of former \textsf{F} employees about a
societal issue, with $0$ denoting a neutral position. In Table~\ref{tab:edge_exe}, we also show two
directions that explain the connections in the graph $G$. The first covers being of the same age and
same companies, especially at \textsf{G}, while the second directions covers former employees of
\textsf{F} having different \textsf{age} and \textsf{opinion}. Not only are the results
interpretable as claimed before, but we can also assess the strength of each explanation, using
their goodness. For instance, we see that the edge between $x$ and $y$ has a low score, because
although those two users have the same \textsf{age} and used to work at \textsf{F}, not both of them
works at \textsf{G}.

\begin{figure*}[htpb]
  % \centering
  \begin{subfigure}[t]{0.55\textwidth}
    \centering
    \includegraphics[width=.9\textwidth]{tikz/edge_example_tikz.pdf}
    \caption{The graph $G$. Larger nodes represent old users while smaller ones are younger. Their
    gender is inscribed in the node and their shape represent their opinion: rectangle for $-1$,
    circle for $0$ and diamond for $+1$. Finally, orange stands for being employed at company G and
    faded blue for having been employed at company F. As for the edges, the red ones are labeled by
    the first direction and the green ones by the second direction, and we label them with the
    associated (normalized) goodness. } \label{fig:edge_exe_graph}
  \end{subfigure}~
  \begin{subfigure}[t]{0.42\textwidth}
    \centering
    \small
    \begin{tabular}{lrrrrr}
      \toprule
      {} & \textsf{G} & \textsf{F} & \textsf{gender} & \textsf{age} & \textsf{opinion} \\
      \midrule
      $u$   &  $+1$ &   $0$ &   $-1$ &  $-0.8$ &     $0$ \\
      $v$   &  $+1$ &  $-1$ &   $-1$ &  $-0.7$ &    $+1$ \\
      $w$   &  $+1$ &   $0$ &   $+1$ &   $0.6$ &    $+1$ \\
      $x$   &  $+1$ &  $-1$ &   $+1$ &   $0.9$ &    $-1$ \\
      $y$   &   $0$ &  $-1$ &   $-1$ &   $0.7$ &    $-1$ \\
      $z$   &   $0$ &  $-1$ &   $+1$ &  $-0.6$ &    $+1$ \\
      \midrule
      $w_1$ &  $+3$ &  $+1$ &    $0$ &    $+1$ &     $0$ \\
      $w_2$ &  $0$  &  $+4$ &    $0$ &    $-2$ &    $-4$ \\
      \bottomrule
    \end{tabular}
    \caption{The numerical attributes of the six nodes. All are categorical encoded with a ternary
      scheme, except for \textsf{age}, which has been transformed to fit in $[-1, 1]$.
      They are followed by the (non normalized) two directions $w_1$ and $w_2$.}
      \label{tab:edge_exe}
  \end{subfigure}
  \caption{A small instance of \ecp{} and an handcrafted solution, albeit non-optimal.}
  \label{fig:edge_exe}
\end{figure*}

\paragraph{Choice of $k$}
\label{par:edge_choose_k}

For clarity, the previous example of \autoref{fig:edge_exe} has only a handful of nodes and
dimensions, and can therefore reasonably be explained by two directions. But in larger graphs, the
choice of $k$ is a legitimate question.
At one extreme, picking $k=|E|$ renders the topology of the graph
irrelevant, as we can simply exploit the trivial
solution of having a single direction $w_{uv} = \frac{s_{uv}}{||s_{uv}||}$ for every edge $(u,v)$.
More generally, as $k$ increases, we expect the value $\max_{\mathcal{D}_k} \sum_{u,v \in E}
\max_{\ell \in \rangesk} g(s_{uv}, w_{\ell})$ to increase as
well, at the cost of interpretability. There could be a principled approach to finding the
\enquote{best} $k$, based on a information theoretical measure of the complexity of $\mathcal{D}_k$
and the minimum description length principle~\autocite{grunwald2005tutorial}. However, in the
interest of simplicity, in the following we focus on the formulation stated in
\autoref{p:edge_full}, where $k$ is given. In practice, we further constrain $k$ to be small (say
less than \np{10}). Within the multilayer framework, $k$ could also be seen as the number of
underlying layers. To retain a fair level of interpretability, we deem appropriate to have $k$ not
too large.

\paragraph{Topological constraints}

In addition to assume that the input graph is the superposition of a small number of layers,
another way to leverage the graph topology is to define local constraints at the node level.
Indeed, one of our motivation is that it is not enough to look at edges in isolation, but rather we
expect the connections of one node to influence the connections of its neighbors. We now present two
of these local, node-level constraints. 

First, if a graph was to perfectly follow our model, every edge $(u,v)$ would be maximally explained
by one of the $k$ directions. This would happen if the profiles of $u$ and $v$ were collinear with
this direction, up to sign reversal. When considering several edges, we could imagine that the nodes
profiles result from the following process. Two nodes $u$ and $v$ establish a connection by first
picking a direction $w_\ell \in \mathcal{D}_k$ and add to their base vector $b_u$ and $b_v \in
\Rbb^d$ a fraction $a_{uv} > 0$ of $w_\ell$. We therefore subtract the following term
\begin{equation}
  \label{eq:edge_node_loss}
  \mathcal{L}_{\mathrm{node}} =
  \sum_{u\in V} \left|\left| x_u - b_u -
  \sum_{v \in \nei(u)} a_{uv} w_{\mathcal{E}(u,v)} \right|\right|^2_2
\end{equation}
from \eqref{eq:edge_full}. The cost we pay is introducing the extra parameters $\{a_{uv}\}_{uv \in
E}$ and $\{b_u\}_{u \in V}$ to be learned, but we will see in the next section how to handle that in
practice. On the other hand, it has two advantages. First, this is another way, besides goodness, in
which the (given) profiles provide prior information about the (unknown) directions, and thus
restrain the search space. Second, this makes the directions dependent of the topology of the graph,
by coupling them with the neighborhood of each node.

Second, we also make the assumption that each node $u$ is only involved in $k_\mathrm{local} < k$
different directions. The intuition is that when $d$ is large enough, users have to focus their
energy and can only express their interest in a few number of dimensions. In other words, all the
edges incident to $u$ can only be associated with one of $k_\mathrm{local}$ directions. For
some values of
$k_\mathrm{local}$, this is a \NPc{} graph colouring problem and thus cannot necessarily be enforced
exactly. Still, it can be seen as a learning bias and will be used to generate synthetic data.
Furthermore, we can also express it as an optimization constraint. Specifically, suppose that for
an edge $(u,v)$ associated with the direction $w_i \in \mathcal{D}_k$, we have a $k$-dimensional
vector $y_{uv}$ equal to $e_i$ (that is all zero except for the \ith{} component set to one). Let
us stack all such vectors into a $|E| \times k$ matrix $Y$, and let $C \in \Rbb^{|V| \times |E|}$ be
the incidence matrix of $G$\footnote{The incidence matrix of a graph $G$, usually denoted by $B$, is
the $|V| \times |E|$ matrix such that $B_{i,j} = 1$ if the node $v_i$ and edge $e_j$ are incident
and 0 otherwise.}, with $c_u$ being its $u^{th}$ row. Then $c_u^TY$ is a $k$-dimensional vector that counts
how many times each direction is incident to node $u$. We thus want the $\ell_0$ norm of $c_u^TY$
(\ie the number of its non-zero component) to be upper bounded by $k_\mathrm{local}$.
This is not a convex constraint but we can relax it, for such sparsity inducing problems have been
well studied~\autocite{sparseOptim12}. Namely, we let $y_{uv}$ be a softmax membership
$k$-dimensional vector, that is we set, $\forall i \in \rangesk$,
\begin{equation*}
  y_{uv,i} = \frac{\exp(\beta g(s_{uv}, w_i))}{\sum_{j=1}^k \exp(\beta g(s_{uv}, w_j))}\,,
\end{equation*}
with $\beta>0$ a large
positive constant.  And we replace the $\ell_0$ norm by a $\ell_1$ norm (or even a $k$-support
norm~\autocite{KsupportNorm12}), yielding the following $\mathcal{L}_{\mathrm{local}}$ term to be
minimized:
\begin{equation}
  \label{eq:edge_local_loss}
  \mathcal{L}_{\mathrm{local}} =
  \sum_{u \in V} \left\| \sum_{v \in \nei(u)} y_{uv} \right\|_1 =
  \sum_{u \in V} \left( \sum_{i = 1}^k \sum_{v \in \nei(u)}
  \frac{\exp(\beta g(s_{uv}, w_i))}{\sum_{j=1}^k \exp(\beta g(s_{uv}, w_j))} \right)
\end{equation}
It is reasonable to further expect that two neighboring nodes share one common direction among their
$k_\mathrm{local}$ ones. One way to ensure this condition is to imagine that the set of allowed
$k_\mathrm{local}$ directions changes smoothly over the graph. This property depends of the
topology, as we can see by considering two extreme cases: a line and a complete graph. On the line,
once a node has picked two allowed directions from its two neighbors, the remaining ones can be
chosen arbitrarily. On the complete graph on the other hand, this choice is must more constrained.

\input{signed}

\section{Proposed approaches}
\label{sec:edge_methods}

We now introduce the five methods solving \autoref{p:edge_full} that we will experimentally compare
in the next section. Recall that given a graph topology $(V, E)$ and node profiles $X$, we first
build the edges representation $\mathcal{S} = \{s_{uv}\}_{(u,v) \in E}$ and then look for $k$
bounded directions that maximize the total goodness of the graph, possibly under topological
constraints. We start with a straightforward baseline, which simply performs a \newline
\hspace*{14pt} 1) \kmeans{} clustering of $\mathcal{S}$, \newline
and propose a post-processing to improve it, plugging our custom goodness metric into the existing
\newline \hspace*{14pt} 2) \lloyd{} algorithm for clustering. \newline
Because these two methods consider each edge in isolation with no regard for the topology, we then
describe a convex relaxation of the objective in equation \eqref{eq:edge_full}, which is further
\newline \hspace*{14pt} 3) \combined{} with the node constraint of equation
\eqref{eq:edge_node_loss}. \newline
Finally, we depart slightly from finding $k$ directions by
considering a low-rank matrix formulation, where each edge is assigned a linear combination of a
small number of base directions, paving the way to overlapping clustering. More precisely, we
give a convex formulation of that objective that can be solved by a \newline
\hspace*{14pt} 4) \fwa{} algorithm, \newline
as well an alternating optimization method, where we make the low-rank factorization \newline
\hspace*{14pt} 5) \pqt{}.

\subsection{\kmeans{} baseline and improvement}
\label{sub:edge_baseline}

A simple and natural baseline is to cluster the set of all similarity edge vectors $
\theset{s_{uv}}{(u,v) \in E}$ using the $k$-means algorithm, where $k$ is the number of directions
set in \autoref{p:edge_full}. Formally, given the graph $G=(V,E)$ and the node profiles $X$ as
input, we order the edges from $e_1$ to $e_m$. Then we build the matrix $S \in \Rbb^{|E|\times d}$, whose
\ith{} row is the similarity $s_{uv} = x_u \circ x_v$ between the profiles of the endpoints of $e_i=(u,v)$.
Running $k$-means on the rows of $S$ will thus partition $E$ into $k$ sets of edges, call them $E_1,
\ldots, E_k$. A drawback of that simplicity is that it does not solve \autoref{p:edge_full}.
Indeed, the partition of $E$ does not provide the set of directions $\mathcal{D}_k = \{w_1, w_2,
\ldots, w_k\} \in \dsphere$ we are looking for. A natural way of obtaining directions from that
partition is to set $w_\ell$ to be the normalized cluster center of $E_\ell$. We refer to this
method as \kmeans{}, and assuming it requires $T$ iterations to converge, it has a linear complexity
of $O(Td|E|)$.
\iffalse
\begin{center}
  \rule{\textwidth}{.3pt}
  \begin{algorithmic}[1]
    \Function{\kmeans{}}{graph $G=(V,E=\{e_1, \ldots, e_m \})$, profiles $X$}
    \State build the matrix $S$ such that $S_{i,j} = {s_{e_i}}_{;j}$
    \State let $\yhat{}$ and $\{c_1, \ldots, c_k\}$ be the assignment and centroids resulting of
    running $k$-means on the rows of $S$
    \State \textbf{return} \yhat{}, $\mathcal{D}_k = \left\{\frac{c_\ell}{\left\| c_\ell
    \right\|}\right\}_{\ell=1}^k $
    \EndFunction
  \end{algorithmic}
  \rule{\textwidth}{.3pt}
\end{center}
\fi

However,
this does not guarantee that we have assigned the best direction to every edge. We therefore propose
the following heuristic, inspired by the Lloyd algorithm for $k$-means, but where the directions in
$\mathcal{D}_k$ play the role of centroids. Namely, we alternate between two steps:
\begin{enumerate}[(i), nosep]
  \item create a new partition $\{E'_1, \ldots, E'_k\}$ by assigning every edge $(u,v)$ to the
    direction $w$ maximizing its goodness ${s_{uv}}^T w$, that is $E'_\ell = \theset{(u,v) \in E}{
    w_\ell = \argmax_{w_i \in \mathcal{D}_k} {s_{uv}}^T w_i}$, and
  \item update the directions to make them optimal with respect to the edges currently assigned to
    them. Specifically, set $\mathcal{D}_k = \left\{ w_\ell = \frac{s_\ell}{\left\| s_\ell
    \right\|}\right\}_{\ell=1}^k $, where $s_\ell = \sum_{u,v \in E'_\ell} s_{uv}$ is the sum of all
    the similarity vectors of the edge in $E'_\ell$,
\end{enumerate}
until the edge assignment is stable or we reach a maximum number of iterations. By analogy with the
$k$-means algorithm, obtaining convergence guarantees in the general case would be surprising, but in
practice we expect that only a small number $T$ of iterations would be needed. We call this method
\lloyd{} and note it also has a linear complexity of $O(Td|E|)$.

In both cases, those methods do not take the topology of the graph into account, motivating the
following approaches.

\subsection{Convex relaxation}
\label{sub:edge_vector}

By unrolling the operator $\mathcal{E}$ in \eqref{eq:edge_full}, we can rewrite our optimization
objective as
\begin{equation}
  \label{eq:edge_unroll}
  \argmax_{\mathcal{D}_k = \{w_1,\ldots,w_k\} \subset \dball}
  \sum_{u,v \in E}  \max_{\ell \in \rangesk} g(s_{uv}, w_\ell)
\end{equation}
The difficulty in optimizing directly \eqref{eq:edge_unroll} stems from the inner $\max$ operator, which we
replace here by a convex relaxation. Indeed, one can check that given a set of numbers $S=\{a_1,
a_2, \ldots, a_{|S|}\}$ and real number $\beta > 0$, we have
\begin{equation*}
  \max_{a_i \in S} a_i = \lim_{\beta \rightarrow + \infty} \frac{1}{\beta}
  \log\left( \sum_{i=1}^{|S|} \exp{(\beta a_i)} \right)
\end{equation*}
  \label{}
Using this fact, and dividing the goodness term and the $\mathcal{L}_{\mathrm{node}}$ term of
\eqref{eq:edge_node_loss} respectively by the number of edges and nodes so that they have comparable
magnitude, we now have the following objective:
\begin{equation}
  \label{eq:edge_soft_early}
  \argmax_{{\substack{w_1,\ldots,w_k \in \dsphere,\\
  a_{uv}\in\Rbb,\, b_u \in \Rbb^d}}}
  \frac{1}{|E|\beta} \sum_{u,v\in E}
  \log\Bigl(\sum_{\ell=1}^k\exp\left(\beta {s_{uv}}^T w_\ell \right)\Bigr)
  - \frac{\mu}{|V|}\sum_{u\in V}
  \left|\left| x_u - b_u - \sum_{v \in \nei(u)} a_{uv} w_{\mathcal{E}(u,v)} \right|\right|^2,
\end{equation}
where $\mu > 0$ is a trade-off parameter.

Note that in the second term, $w_{\mathcal{E}(u,v)} = \argmax_{w_\ell \in \mathcal{D}_k} {s_{uv}}^T
w_\ell$. However, the $\argmax$
function is again not convex and we thus use the same relaxation as before. Namely, this relaxation
$\mathcal{A}$ takes as input a $k$-tuple of directions and return their sum weighted by the softmax
function as follow:
$$ \begin{array}[t]{lrcl}
  \mathcal{A} : & \Rbb^d \times \ldots \times \Rbb^d & \longrightarrow & \Rbb^d \\
      & (w_1,\ldots,w_k) & \longmapsto &
  \sum_{i=1}^k \frac{\exp(\beta{s_{uv}}^T w_i)}{\sum_{j=1}^k \exp(\beta {s_{uv}}^T w_j)} w_i
\end{array} $$
This function has an explicit derivative with respect to any $w_i$, but it is rather costly to
compute and in practice we
found automatic differentiation~\autocite{autograd15} to be more efficient.

We make two further simplifications. First, notice that the set $\dsphere \times \ldots \times
\dsphere$ is not convex. To ease
the optimization procedure, we thus only impose the directions $w_\ell$ to lie within the unit ball
and expect them to have norm close to one in order to maximize the dot product in the ${s_{uv}}^T
w_\ell$ term\footnote{Indeed, since the first term is convex, its maximum can only be reach at the
boundary of the feasible domain~\autocite[Theorem 32.1]{convexAnalysis70}. However, we are also
subtracting another convex term and the resulting function is therefore more complicated.}.
Second, while \eqref{eq:edge_soft_early} requires to optimize
$kd + |E| + |V|d$ parameters, we fix $a_{uv} = \deg(u)$ and $b_u = \zerov$ for all edges and nodes in order
to be independent of the size of the graph. The final problem solved by our \combined{} method using
(full) projected gradient descent is therefore:
\begin{equation}
  \label{eq:edge_soft}
\!  \argmax_{w_1,\ldots,w_k \in \dball}
  \frac{1}{|E|\beta} \sum_{u,v\in E}\!
  \log\Bigl(\sum_{\ell=1}^k\exp\left(\beta {s_{uv}}^T w_\ell \right)\Bigr)
  - \frac{\mu}{|V|}\sum_{u\in V} \left|\left| x_u  - \frac{1}{\deg(u)}
  \sum_{v \in \nei(u)}\!\! \mathcal{A}(w_1,\ldots,w_k) \right|\right|^2
\end{equation}

\subsection{Matrix optimization}
\label{sub:edge_matrix}

Besides relaxing the $\max$ operator from \eqref{eq:edge_full}, another solution to remove it would
be to increase $k$ to be equal to $|E|$. As we already mentioned, this makes the problem trivial. To
avoid this, we add the constraint that all those $|E|$ directions are linear combinations of a small
number $k$ of basis directions. In order to formulate this more clearly, we rewrite the previous
objectives with matrices. We first show this can be expressed in a form that is amenable to the
Frank--Wolfe algorithm. Then we take the alternative road of explicit low-rank matrix factorization,
and we conclude by commenting on these approaches.

\subsubsection{\fwa{} method}
\label{ssub:edge_frank_wolfe}

Seeking one direction for every edge, our goal is thus to maximize
$\sum_{u,v\in E} {s_{uv}}^T w_{uv}$. This objective can be written in matricial form as $\tr(SW) =
\frop{S^T}{W}$, where $S \in \Rbb^{|E| \times d}$ is the $s_{uv}$ vectors for all edges stacked
vertically and $W \in \Rbb^{d \times |E|}$ is the matrix of all directions stacked horizontally
and $\frop{A}{B} = \tr\left(A^T B\right)$ is the Frobenius inner product of two real matrices.
More specifically, because of the unit norm constraint on directions, we look for $W$ in the set
$\mathbb{M}^{d\times |E|}$ of $d\times |E|$ matrices with unit $\ell_2$ norm columns. To avoid the
trivial solution of letting each direction $w_{uv}$ merely be $\frac{s_{uv}}{\|s_{uv}\|}$, we
further constraint $W$ to be of low rank. That is, what we want to optimize is \[\min_{W\in
\mathbb{M}^{d\times |E|}} -\frop{S^T}{W} + \rank(W)\,.\] However, while the Frobenius inner product is
linear in $W$, the rank operator is highly irregular and optimizing it directly is \NPc{}.  Therefore,
we relax the rank term by the nuclear norm of $W$, defined as $\|W\|_* = \sum_{i=1}^{\min(d,|E|)}
\sigma_i(W)$, where $\sigma_i(W)$ is the \ith{} largest singular value of $W$. The intuition is
that the rank of $W$ is
equal to the number of non-zero singular value, while the nuclear norm is essentially an $\ell_1$ norm on
the spectrum of $W$. Therefore, as minimizing the $\ell_1$ enforces sparsity, minimizing the nuclear
norm tends to make the rank smaller by ensuring the presence of zeros in the singular values of $W$.
\footnote{As mentioned in \autoref{sec:troll_related} about \autocite{OnlineCompletion17}, a tighter
convex relaxation of the low-rank constraint is the max norm, but we do not pursue that venue
further.} This
results in the following optimization problem: \[\min_{W\in \mathbb{M}^{d\times |E|}} -\frop{S^T}{W}
+ \lambda \|W\|_*\,,\] where $\lambda$ is a regularization parameter. Since $\|\cdot\|_*$ is a norm, this
objective function is convex (although the domain over which we optimize is not), but it does not
have a gradient, forcing us to rely on potentially costly proximal gradient descent
methods~\autocite{Parikh2013a}. Instead, we consider the equivalent problem obtained by replacing
the regularization parameter with an upper bound $\delta$ on the nuclear norm of $W$, yielding:
\begin{equation*}
  \min_{\substack{W\in \mathbb{M}^{d\times |E|} \\ \|W\|_* \leq \delta}} -\frop{S^T}{W}\,.
\end{equation*}

The remaining issue is that $W\in \mathbb{M}^{d\times |E|}$ is not a convex constraint. As in the
previous formulation, we thus relax it by simply imposing that the $\ell_2$ norm of the columns of
$W$ are not too large. In fact, we already have a bound on these column norms, because of the
nuclear norm constraint.
\begin{prop}
For any $W\in \Rbb^{d\times |E|}$ such that $\|W\|_* \leq \delta$, the \ith{} column $W_{:,i}$ of
$W$ satisfies $\|W_{:,i}\|_2 \leq \delta\sqrt{d}$.
\end{prop}
\begin{proof}
  Let $W$ be a matrix in $\Rbb^{d\times m}$ with $d < m$ such that $\|W\|_* \leq \delta$.
  Furthermore, let its singular value decomposition be such that we can write $W = \sum_{l=1}^d
  \sigma_l u_l v_l^T$, where $u_l$ and $v_l$ are $d$ unit-norm vectors, of dimension $d$ and $m$
  respectively. We can thus express the general term $W_{k,i}$ as $\sum_{l=1}^d \sigma_l v_{l,i}
  u_{l,k}$. Because $v_l$ is unit norm, $v_{l,i} \leq 1$ and $$W_{k,i}^2 \leq \left( \sum_{l=1}^d
  \sigma_l u_{l,k} \right)^2 \leq \sum_{l=1}^d \sigma_l^2 \sum_{l=1}^d u_{l,k}^2$$ using the
  Cauchy--Schwartz inequality. The norm of the \ith{} column of $W$ then satisfies:
  \begin{equation*}
    \|W_{:,i}\|_2 = \left( \sum_{k=1}^m W_{k,i}^2 \right)^\lhalf
    \leq \left( \sum_{k=1}^m \left( \sum_{l=1}^d \sigma_l^2 \sum_{l=1}^d u_{l,k}^2 \right) \right)^\lhalf
    = \left( \left( \sum_{l=1}^d \sigma_l^2 \right) \left( \sum_{l=1}^d \sum_{k=1}^m u_{l,k}^2 \right) \right)^\lhalf
  \end{equation*}
  From $\sum_{l=1}^d |\sigma_l | \leq \delta$, we have that $\sum_{l=1}^d \sigma_l^2 \leq \delta^2$.
  Combined with $u_l$ being unit norm, we conclude that $\|W_{:,i}\|_2 \leq \delta\sqrt{d}$.
\end{proof}
However, setting $\delta = \nicefrac{1}{\sqrt{d}}$ is too stringent in high dimension, and because
this inequality is rather loose, it will likely results in very small column norms. Instead of
relying solely on the nuclear norm constraint, we therefore add an additional regularization term
equal to $\mu \sum_{i=1}^{|E|} \|W_{:,i}\|_2^2 = \mu \frop{W}{W}$. Our final convex formulation is
then:
\begin{equation}
  \label{eq:edge_FW}
  \min_{\substack{W\in \Rbb^{d\times |E|} \\ \|W\|_* \leq \delta}} \frop{\mu W - S^T}{W} \,.
\end{equation}

Because \eqref{eq:edge_FW} is of the form $\min_{x\in \mathcal{X}} f(x)$ where $f$ and $\mathcal{X}$
are convex, we can solve it using the Frank--Wolfe algorithm~\autocites{FrankWolfe56}{Jaggi2013a},
which enjoy properties like a $O(\frac{1}{t})$ convergence rate, a low computational cost by avoiding
projection step and the sparsity of its solution. It is an iterative procedure that maintains an estimate
$x^{(t)}$ of the solution and works succinctly as follow: it linearizes the objective $f$ at the
current position $x^{(t)}$ by computing $\nabla f(x^{(t)})$, it finds a minimum $s^{(t)}$ of that
linearization within the domain $\mathcal{X}$ by solving $\argmin_{s\in \mathcal{X}}
\innerp{s}{\nabla f(x^{(t)})}$ and it moves toward that minimizer by a step $\gamma$, setting
$x^{(t+1)} = (1-\gamma) x^{(t)} + \gamma s^{(t)}$.\footnote{$\gamma$ is either found by a line search or,
in our case, set to $\nicefrac{t}{2+t}$.} In the case of \eqref{eq:edge_FW}, the linear minimization step amounts
to $\argmin_{\|W\|_* \leq \delta} 2\mu W - S^T = \argmax_{\|W\|_* \leq \delta} S^T - 2\mu W$.
Moreover, the $\delta$-ball of the nuclear norm is the convex hull of the matrix of the form $\delta
u v^T$ with $\|u\|_2 = 1 = \|v\|_2$~\autocite{Jaggi2013a}. By letting $u_1^{(t)}$ and $v_1^{(t)}$ be
respectively the left and right vectors associated with the largest singular value of $S^T - 2\mu
W^{(t)}$, the minimizer $s^{(t)}$ we are looking for is therefore $\delta u_1^{(t)} {v_1^{(t)}}^T$,
and we have $W^{(t+1)} = (1-\gamma) W^{(t)} + \frac{t}{2+t} \delta u_1^{(t)} {v_1^{(t)}}^T$. We
perform a small number $T$ of such iterations, and these two eigenvectors are computed in a
time linear in the number of non-zero entries of $S^T - 2\mu W^{(t)}$~\autocite{topEigenvalue92}.
Thus the overall complexity of this method, which we call \fwa{}, is $O(Td|E|)$.

As said at the beginning of this section, the matrix formulation is not exactly solving
\autoref{p:edge_full}. However, for evaluation purposes, here we describe two ways to extract $k$
directions and an edge assignment from the matrix $W$ obtained through this \fwa{} method. The first
is rather straightforward. It performs a $k$-means clustering of the columns of $W$ and use it as
assignment, while the directions are the normalized cluster centers. The second involves an extra
optimization step. Specifically, letting $r$ be the rank of $W$, we first compute a reduced SVD such
that $W = U \Sigma V^T$ and let $P = U \in \mathbb{M}^{d\times r}$ and $Q^T = \Sigma V^T \in \Rbb^{r
\times |E|}$. We then note that for any invertible matrix $R \in \Rbb^{r \times r}$, we have $W =
PR^{-1} RQ^T$. Thus we look for a matrix $R$ that makes the columns of $RQ^T$ as close as possible
of having unit $\ell_2$-norm, that is $R = \min_{R \in \Rbb^{r \times r}} \sum_{i=1}^E \left| \left\|
{RQ^T}_{:,i} \right\|_2^2 - 1 \right|$. Finally, as before,  we cluster the columns of $RQ^T$ using
$k$-means, and use the columns of $PR^{-1}$ as basis directions, noting they are not
necessarily unit norm anymore.

\subsubsection{\pqt{} low rank factorization}

We can avoid the rank regularization term altogether by making the low-rank decomposition of $W$
explicit, that by writing $W=PQ^T$ and optimizing over both $P$ and $Q$. We thus call this method
\pqt{}. However, this comes at cost
of convexity. Indeed, given a rank $k\leq \min(d,|E|)$, the problem becomes, assuming $A$ and $B$
are fixed:
\begin{equation}
  \label{eq:edge_PQ}
  \min_{P\in\mathbb{M}^{d\times k}, Q\in\mathbb{M}^{|E|\times k}} \quad \tilde{h}(P, Q) =
  - \frop{S^T}{PQ^T} + \mu \|X - B - C\left(A\circ QP^T \right)\|_F^2
\end{equation}

The second term corresponds to the node loss term of equation \eqref{eq:edge_node_loss}, expressed
in matricial form as $\|X - B -
C\left(A\circ W^T \right)\|_F^2$, where $B\in \Rbb^{|V| \times d}$ is the node bias vectors $b_u$
stacked vertically, $C \in \Rbb^{|V| \times |E|}$ is the incidence matrix of $G$, $A \in \Rbb^{|E|
\times d}$ is the matrix whose each row is the corresponding $a_{uv}\onev$ vector and $\|\cdot\|_F$
denotes the Frobenius norm. As previously, we set $B=0$ and the rows of $A$ to be one over the
degree of the node at the origin of the corresponding edge.

While \eqref{eq:edge_PQ} is not jointly convex in both $P$ and $Q$, it is convex in one of
them when the other is fixed. Therefore it can be solved via alternating optimization over $P$ and
$Q$~\autocite{AlternatingOptim02} using standard (projected) gradient descent algorithms, with the
following partial derivatives:
\begin{align*}
  \frac{\partial \tilde{h}}{\partial P} &=
  S^T Q + 2\mu
  A^T \circ (((X-B)^T -A^T \circ (PQ^T )C^T )C)Q 
  \\
  \frac{\partial \tilde{h}}{\partial Q} &=
  S P + 2\mu
  A\circ (C^T (X-B-C(A\circ (QP^T ))))P.
\end{align*}
Because the problem is not jointly convex, we are less concerned with the domains of the variables
not being convex, as we anyway have no guarantee to find a global minimum. We also do not add a
regularization term of the norm of the columns of $PQ^T$. Indeed, since $P\in\mathbb{M}^{d\times k}$
and $Q\in\mathbb{M}^{|E|\times k}$, each column of the resulting $PQ^T$ is a linear combination of
$k$ unit norm vectors of $P$ with weights whose square sum to $1$ and thus their norm is bounded by
$\sqrt{k}$. As opposed to all the previous methods, computing the full gradient is not linear in the
number of edges. Indeed, computing the product $S^T Q$ requires $O(|E|^\omega)$ arithmetic
operations, where $\omega \geq 2$ is the optimal exponent of matrix multiplication
algorithm. This could be mitigated by using stochastic gradient descent. Finally, to obtain a
solution of the \ecp{} \autoref{p:edge_full}, we take the directions to be the columns of the
resulting $P$, and the assignment to be a $k$-means clustering of the rows of $Q$.

\paragraph{Discussion}

While the \pqt{} formulation is not convex, in practice we initialize it with the solution from
\combined{} and it thus converge after few iterations. Namely, we set $P_0 \in \mathbb{M}^{d\times
k}$ to be the final directions found by \combined{} and the rows of $Q_0 \in \mathbb{M}^{|E|\times
k}$ to be equal to the corresponding $e_{\mathcal{E}(u,v)}$. Furthermore, in the projection
step of the $Q$ variable, we actually ensure that all the entries of $Q$ are positive, which makes
them easier to  interpret as weights for the basis directions of $P$. This is something that we
cannot guarantee on the decomposition of the matrix $W$ obtained by the \fwa{} method. On the other
hand, if we run the \fwa{} algorithm for more than $k$ iterations, the rank of $W$ can be higher
than $k$, which might allow a finer clustering of the edges.

Compared with the \combined{} approach of the previous \autoref{sub:edge_vector}, these two matrix
formulations require learning more parameters. Indeed, it increases from $dk$ to $d|E|$, as it each
edge has a different direction, albeit made up of a small number of basis directions. In the case of
\pqt{}, we could furthermore impose a sparsity constraint on the columns of $Q$ to reduces the
number of coefficient to learn. Yet this additional complexity has the advantage of allowing
overlapping clustering of the edges. This is useful in cases where two nodes have a relationship
that is best described by a combination of the $k$ base directions. Moreover, in the case of \fwa{},
this additional flexibility comes with no increase in computational cost.
