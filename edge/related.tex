\section{Related work}
\label{sec:edge_related}

To the best of our knowledge, there are not many works that directly attempt to classify edges in
attributed networks. A natural idea is thus to rely on the numerous existing node clustering
algorithms, by considering the line graph $L(G)$ of $G$. $L(G)$ is a graph whose each of the $|E|$
nodes corresponds to an edge of $G$, and where two nodes of $L(G)$ are connected whenever their
corresponding edges share a common node in $G$. \Textcite{LineGraph09} apply a modularity algorithm
to $L(G)$ is order to partition the edges of $G$, but contrary to us, their final goal is to
discover an overlapping clustering of the nodes of $G$, and they rely solely on the graph topology.
In their review of mining social networks, \textcite[Section 2.3]{SemanticMining15} present several
methods to solve what they call \emph{inferring social ties}. The first one is based on a graphical
model where each node is an edge. The label of an edge is influenced by the attributes of its
endpoints, the correlation between the type of neighboring edges and some global constraints. These
three influences are modeled by pairwise factors. The parameters of the model are learned by
maximizing the likelihood of the observed label through gradient descent. Those parameters are then
use to predict the remaining label that further maximize the total
likelihood~\autocite{graphicalModelTies11}. An extension to this graphical model is to actively
query the most informative edges, in the case where it is costly to ask social network user to label
their relationships~\autocite{Zhuang2012}. Another way to leverage additional supervision is to use
transfer learning~\autocite{transferTiesPred16}. Namely, given two distinct and partially labeled
graphs, where the fraction of label is much higher for the first graph than the second one, the goal
is to fully label the edges of the second graph, exploiting the rich information of the first one.
This requires the attributes to be comparable between the two graphs, and such attributes are
therefore computed based on topological features. Finally, a last direct approach to labelling edges
is presented by \textcite{Aggarwal2016a}. They consider that the input graph has three type of
edges: labeled by $-1$, labeled by $+1$ or unlabeled. In each of the three induced subgraphs, they
define a similarity between two nodes $u$ and $v$ as the Jaccard coefficient of the two sets of
edges incident to $u$ and $v$. A weighted combination of these three coefficients is the final
similarity between $u$ and $v$. Then $S_k(u)$ are the top-$k$ most similar nodes to $u$, and the
label of the edge $(u,v)$ is chosen as the majority label in $S_k(u)\times S_k(v) \bigcap E$.
All these works assign a label to the edges of the input graph, but either they need supervision or
they do not use attributes, and in both cases they provide little interpretability on their results.

Besides social network, the task of predicting the semantic of an edge has also been addressed in
the context of knowledge graphs. In such graphs, nodes are abstract concepts and concrete entities
from the real world, and edges are directed predicates representing facts connecting two
entities~\autocite{KnowledgeGraphSurvey16}. A typical example is an edge $(h, \ell, t) =
($\textsf{Donald J. Trump}, \textsf{President of}, \textsf{USA}$)$\footnote{At least at the time of
writing} where the $(h, \ell, t)$ notation stands respectively for head, type of relation and tail.
This illustrates some differences with our setting. First, there might be a very large number of
type of edges in knowledge graphs, since they cover a domain as large as possible. Second, this
coverage makes it difficult to describe directly entities by a consistent set of $d$ attributes, and
one usually has to rely on low dimensional embeddings. \autocite{transE13} is primarily concerned
with learning such embeddings in $\Rbb^k$ for both nodes and relations, that are denoted by the
boldface letters $\bm{h}$, $\bm{\ell}$, $\bm{t}$. The intuition, similar to word vector, is that
relations can be modeled by translations, so the existence of an $(h,l,t)$ edge offers evidence for
$\bm{h} + \bm{\ell} \approx \bm{l}$. Given a training set of existing triplet $S$ and a
dissimilarity measure $D$, the authors learn embeddings in a fashion reminiscent of metric
learning~\autocite{MetricLearning15}. Namely, they create negative examples by corrupting at random
training examples and then minimize with a margin the positive part of the difference between
dissimilarity of positive and negative examples. More generally, we refer the interested reader to a
recent survey on embedding knowledge graphs~\autocite{KnowledgeGraphSurvey17}, which among other
ideas present the interesting notion of composing relation along paths between two entities. We note
that, in contrast with us, those methods require some form of supervision when it comes to predict
edge types.  One way to avoid supervision is to rely on a large amount of unstructured text, extract
entities from this corpus, map entities and possible relations to a knowledge base and finally embed
them in a low dimensional space using text features such as POS tags~\autocite{Ren2017}.

% One can also graph convolutional network to learn such embeddings~\autocite{Schlichtkrull2017}

A related idea is embedding edges in general graphs, not necessarily in the context of knowledge
graphs. This is the counterpart of node embeddings techniques we mentioned in
\autoref{sec:troll_related}. The most straightforward way is to keep track of edges instead of nodes
when performing random walks, learn an embedding from this corpus using word2vec-like methods and
then cluster these representations using $k$-means~\autocite{edgerep17}. We cannot directly compare
this with our work since there is no attribute and thus the clustering is not interpretable (indeed
the goal of \autocite{edgerep17} is eventually to find an overlapping clustering of the nodes, not
the edges). More closely related to our work, \textcite{ahmed2017roles} seek to assign a role to the
edges of a graph. First they learn edge features as follow: starting from topological features, they
use combination operators (like max, min, mean, sum, product and so on) to iteratively learn higher
order features while pruning those that are correlated in order to avoid a combinatorial explosion.
Once they obtain such a $d$-dimensional feature vector for each edge, they stack them in a matrix $S
\in \Rbb^{|E|\times d}$ and look for $U^{|E| \times r}$ and $B^{r \times d}$ such that the distance
between $S$ and $UV^T$ is minimized. $V$ is a description of $r$ roles in terms of features while
$U$ is the mixed membership of each edge. Finally the authors choose the number of roles $r$ using
the Minimum Description Length principle. While their approach scales linearly with the number of
edges and is partially parallelizable, the process in which edge feature are learned does not lend
itself easily to interpretation.
% Learn a function that take as input the embedding of two nodes and output the predicted strength of
% the relationship~\autocite{EdgeAwareEmbedding17}, could it be modified to output a cluster index?
% (I think this is too far actually, sure they learn this edge function, but their motivations are
% edge asymmetry and model size, working on non attributed, non-typed graph. so leave it out)
%\autocites[section 2.5]{representationLearning17}{ahmed2017roles}{Ahmed2016}

Another direction is to see the problem as clustering the similarity vectors $s_{uv}$. Because of
our bias that nodes are connected through partial homophily and heterophily, this can more precisely
be casted as a subspace clustering problem~\autocite{SCSurvey11}. Namely, we are given a set of $m$
points in $\Rbb^d$, with the assumption they belong to the union of affine subspaces of unknown
dimension. The goal is to recover the number $K$ of such subspaces, their dimension, their
parameters and the assignment of the $m$ point to those subspace. If $K$ is known, one can use
iterative, $k$-mean like methods (reminiscent of our \lloyd{} method). Otherwise it also possible to
factorize the matrix of all the data points and interpret as a similarity matrix, use iterative
statistical approaches such as random sample consensus or build a similarity graph and apply
spectral clustering algorithms to it. Finally, it is possible to leverage sparsity assumptions and
express each point as a sparse linear combination of other points, and use these coefficients as
weights of a similarity graph, which is then clustered into subspaces by a spectral
method~\autocite{SparseSC13}.
While subspace clustering comes with information theoretic guarantees of its ability to retrieve
optimal subspaces, when formulating directly over the $\{s_{uv}\}_{(u,v) \in E}$, it makes no use of
graph topology, as we consider each edge independently of all others. As an example of applying
related ideas to graph data, \textcite{Huang2015} build a $d$-dimensional grid of the attributes
space and use subspace clustering to find cells that have low entropy and high connectivity, before
merging such cells into node clusters.
% \textcite{subgraphEdgeLabels13} propose a method to extract subgraphs that are dense across
% several edge types. (was updated in \autocite{Boden2017}


\Textcite{Taskar04} represent a node-attributed graphs with partially labelled edges as a
probabilistic graphical model (called Relational Markov Model) and learns its parameters from the
data using gradient descent. It is a flexible modeling approach that let the designer express
complex graph patterns. Using a slightly less principled approach, \textcite{conceptualLinks12}
first cluster nodes based solely on their attributes and then count the links between such groups to
keep only the maximally frequent ones, arguing this reveals the set of attributes that support the
connection between two nodes. In the same data mining vein, and related to our departure from global
homophily, \textcite{beyondHome16} look for link patterns whose support and confidence is not only
above some given thresholds, but that also diverge from homophily. In the author explicitly exclude
attributes taking the same value within such link patterns.

Groups of nodes are first created when they have common attributes. The set of links between two
groups is called a conceptual link in the sense that it corresponds to a relationship between two
sets of attributes, namely two concepts in the field of formal concept analysis [7]. The number of
links between two groups is then evaluated and when the frequency is greater than a given support
threshold $\beta$, we talk about frequent conceptual link. Note that we only keep maximal frequent
conceptual links (mfcl), i.e. those who are not included in others.  Finally, the set of maximal
frequent conceptual links is used to create a new network structure called a conceptual view which
summarizes all the knowledge extracted from the initial network. In a conceptual view, each node
corresponds to a set of attributes (in this context we call them Meta-nodes) and a link corresponds
to maximal frequent conceptual links~\autocite{conceptualLinks12}.
The objective of \textcite{ZhangModelFree16} is eventually to hard cluster the nodes into
communities but their work similarities with ours. Namely, they tweak the traditional modularity
objective (maximizing the density of intra community edges) by weighting the edges with the
similarity of their endpoints, and simultaneously learning a weight vector $w_\ell$ for a community
$C_\ell$. Formally they defined the $R$ criterion as :
\begin{equation*}
  R(\mathcal{C}, \beta;\alpha, \mu) = \sum_{\ell=1}^k \frac{1}{|\mathcal{C}_\ell|^\alpha}
  \sum_{u,v \in \mathcal{C}_\ell} A_{uv} \mu -e^{-g(s_{uv}, w_\ell)}
\end{equation*}
where $A$ is the adjacency matrix, $\alpha$ penalizes unbalanced communities, $\mu$ is a trade off
parameter between information from edges and node features, and $g$ is exactly our goodness
function.  They optimize it by alternately optimizing over the labels with fixed parameters and over
the parameters with fixed labels, using block coordinate descent.
Finally we mention a more quantitative work. \Textcite{Abraham2012a} assume that there are $K$
social categories modelled by $K$ Euclidean spaces $\mathcal{D}_i$. Nodes of the graph have an
associated point in each of these spaces. The key assumption is that space have small local
correlation: informally, the intersection of any two small balls from two distinct spaces is small.
These $K$ spaces give rise to small world networks $\mathcal{G}_i$, where the edge probability
$\propto \mathcal{D}_i(u, v)^{-d}$ and we observe the real network $\mathcal{G} =\bigcup_i
\mathcal{G}_i$. From $\mathcal{G}$, the proposed algorithm recovers in $n \mathrm{polylog} n$ time a
bounded approximation $\mathcal{D}'_i$ of all $\mathcal{D}_i$, that is
\begin{equation*}
  \sigma \mathcal{D}_i(u, v) \leq \mathcal{D}'_i(u, v) \leq \delta \mathcal{D}_i(u, v) + \Delta
\end{equation*}

\iffalse
adhoc~\autocites{conceptualLinks12}{Abraham2012a}{Taskar04}.
A supervised probabilistic model
find a partition maximizing the density of intra community edges ---weighted by the endpoints
profile similarities and a learned community weight vector~\autocite{ZhangModelFree16}

Let $A$ be the unweighted adjacency matrix and  $\mathcal{E} = \bigcup_{i=1}^K
\mathcal{E}_k$ be a partition of the node, which alternatively is represented by
the community memberships vector $e$.  They start by presenting a simple
community goodness criterion, which like modularity basically score a partition
based on the number (or density) of edges within communities
$$ R(e;\alpha) = \sum_{k=1}^K \frac{1}{|\mathcal{E}_k|^\alpha} \sum_{i,j\in \mathcal{E}_k}A_{ij} $$
where $\alpha$ is a tuning parameters which penalizes unbalanced communities as
it gets larger.
The idea would be to add weight on the edges based on the node features
$f_i\in \mathbb{R}^p$ so as to increase that score. Because features and
communities don't correlate perfectly, they introduce a more flexible criterion
$$ R(e,\beta;\alpha,w_n) = \sum_{k=1}^K \frac{1}{|\mathcal{E}_k|^\alpha}
\sum_{i,j\in \mathcal{E}_k}A_{ij}W(f_i, f_j,\beta_k;w_n) $$
where
$\beta_k\in \mathbb{R}^p$ is a weight vector for the $k$th community and $w_n$
is trade off parameters between information from edges and node features.
Basically, $W(f_i, f_j,\beta_k;w_n)$ is a function of the similarity
$\phi_{ij}=\phi(f_i, f_j)$ between two nodes. This reminds of our approach, by
assigning one vector $w$ to each community. As a practical example, they use
$$
w_{ijk} = W(f_i, f_j,\beta_k;w_n) = w_n -e^{-\langle \phi_{ij},\beta_k\rangle}
$$


This \enquote{criterion needs to be optimized over both the community
assignments $e$ and the feature parameters $\beta$. Using block coordinate
descent, we optimize it by alternately optimizing over the labels with fixed
parameters and over the parameters with fixed labels, and iterating until
convergence}. Again, this is similar to what we tried at first.



\fi

\bigskip

Although there has not been so many work on clustering edges in node attributed networks, there is a
wealth of papers on clustering nodes in such networks. Since this is not exactly our topic, we only
mention a selection of the most relevant to our objective (especially the overlapping ones), and
refer the interested reader to the survey of
\textcite{surveyAttributedClustering15}\footnote{Especially the section 2 dealing with edge labeled
graphs.}. We also reiterate the warning given in the introduction that in large graphs, node
attributes are not necessarily aligned with annotated communities. Such attributes thus need to be
used \emph{in addition} to topological information. This is further covered in~\autocite[Section
3.4]{Fortunato2016}.

An early attempt is the \textsf{SA-Cluster} algorithm of \textcite{Zhou2009} that partitions the
nodes of a graph based on a distance combining nodes structural and attributes similarities, along
with its faster incremental version~\autocite{Zhou2010}. Nodes in the same cluster are well
connected and have a set of similar attributes. The balance between these two objectives was further
studied in a setting where it is tunable by the user of the algorithm~\textcite{Baroni2017}. In a
more \enquote{data mining fashion}, the same problem can also be worded as finding subgraphs induced
by a set of attributes that are more dense than what one would expect in a null
model~\autocite{Silva2012}. Instead of detecting all such subgraphs or communities, one can adopt a
query-based approach. Namely, given a query node $u$ and a set $S$ of attributes, find all the
connected subgraphs containing $u$ that are both tightly connected and share enough common
attributes~\autocite{AttributedCommunity16}. In that spirit of focusing on a given node,
\textcite{LeskovecEgo12} introduce a generative model for ego networks in social networks where the
neighbors of a node $u$ can belong to $k$ categories (such as family, colleagues, school friends).
Using our notations, those $k$ categories (or circle) $\{C_i\}_{i=1}^k$ of a node $u$ are defined by
a vector $w_i$ and a weight $\alpha_i$. The probability $p(e)$ of an edge $e=(u,v)$ favors the
presence of edges with high goodness within circles:
\begin{equation}
  \label{eq:edge_ego}
  p((u, v)=e \in E) \propto \exp \left(
    \sum_{C_i  \supseteq {u,v}} g(s_{uv}, w_i) -
    \sum_{C_i \nsupseteq {u,v}} \alpha_i g(s_{uv}, w_i)
  \right)% = \exp\left(\Phi(e)\right)
\end{equation}
The unsupervised problem of maximizing the likelihood of observing the input graph according to this
probability distribution is solved by alternating two steps: assigning nodes to circles and
optimizing the parameters $\{w_i, \alpha_i\}_{i=1}^k$ given a circle assignment. Similar to us, the
vector $w_i$ explain why some nodes belong to the \ith{} circle. However, it is not obvious how to
transfer this knowledge to the edges themselves.

In fact, this generative model approach has proved very fruitful when it comes to community
detection in node attributed graphs~\autocites{Xu2014}{Zhao2017}{Yang2013}{Kataoka2016}{Weng2016}{Newman2016}.
The general idea is the following. First, design a model to generate some of the following aspects:
the nodes attributes, the topology of the graph and the community membership of the nodes. Then,
infer the parameters that maximize the likelihood of observing the current graph. Finally, extract
from these parameters community membership for every nodes. We give a very succinct description of
the generative models of some representative recent works in \autoref{tab:edge_genmodel}. Seen at
the light of our \autoref{p:edge_full}, given the overlapping membership obtained by inference of a
generative model, and an edge $(u,v)$, what could look at the highest share community coefficient
between $u$ and $v$ and use that to explain the edge (among $\frac{K(K-1)}{2}$ edge type). However,
this is clearly an ad-hod post processing, as indeed these works are concerned with node and not
edge clustering.

\setlength{\fullpage}{\textwidth+\marginparsep+\marginparwidth}
\begin{table}[htpb]
  \centering
  \small
  \caption{We summarize how each model generates: 1) the membership of a node $u$ to a community $c
  \in \rangesk$, 2) the attributes of $u$ knowing its community membership and 3) the edges between
nodes.}
  \label{tab:edge_genmodel}
  \bgroup
  \def\arraystretch{1.5}
  \begin{tabulary}{\fullpage}{LLLL}
    \toprule
    ref & community & attributes & links \\
    \midrule

    \autocite{Xu2014} &
    single community drawn from a multinomial &
    drawn from a distribution parametrized by the node community &
    drawn from a distribution parametrized by the pair of endpoints communities \\

    \autocite{Yang2013} &
    intensity of membership $F_{uc} \in [0, \infty)$ are given &
    logistic function of $F_{u1},\ldots,F_{uC}$ &
    sharing more communities makes link more likely:
    $P_{uv} = 1-\exp(-\sum_c F_{uc}F_{vc})$ \\

    \autocite{Kataoka2016} &
    multinomial of dimension $k$ &
    drawn from one normal distribution per community &
    Stochastic Block Model (SBM), where blocks are identified with communities \\

    \midrule

    \autocite{Zhao2017} &
    gamma distribution, whose parameters depends of the nodes attribute &
    given binary attributes &
    SBM, with the block matrix drawn from a hierarchical relational gamma process \\

    \autocites{LeskovecEgo12}{LeskovecEgo14} &
    overlapping circle defined by $\theta_k$, but not generated &
    given &
    higher probability of appearing within common circle \eqref{eq:edge_ego} \\

    \autocite{Weng2016} &
    logistic function of the attributes &
    given &
    SBM \\

    \autocite{Newman2016} &
    one multinomial for each discrete value of the single attribute &
    given &
    degree corrected SBM \\
    \bottomrule
  \end{tabulary}
  \egroup
\end{table}

We also point to the work of~\textcite{DeBacco2017}, which despite not considering node attributes,
present a model with interesting applications. Specifically, the nodes have mixed membership to $K$
overlapping groups and each $L$ layer is generated by a specific $K\times K$ block matrix, taking
into account the group membership, like in a degree corrected SBM. Unlike us, this allows for
multigraphs, although the authors assume for simplicity that this does not happen. Once its
parameters are found, this model can then be used to predict the existence of extra edges in each
layer, which can be seen as edge type prediction. Furthermore, measuring the extent to which one
layer helps predict links in another layer provides a way to measure the relationships between the
layers, from redundancy to complete independence, allowing information compression.

\iffalse
\begin{itemize}[leftmargin=*]
\item \href{http://dl.acm.org/citation.cfm?id=2629616}{A General Bayesian
  Framework for Attributed Graph Clustering}~\autocite{Xu2014}

They present a generative model of clustered attributed graph. That is, given a
vertex set $V$, \enquote{a joint probability distribution over the space of all
possible attributed graphs and all possible partitions over $V$. The
clustering with the highest probability is this clustering best explains the
attribute values and edge patterns of the given graph.}

Let $X$ be the $n\times n$ adjacency matrix, $Y$ be the $n\times T$ attribute
matrix and $Z$ be the $n$-long cluster indicator vector (taking values in
${1,\ldots,k}$). The generative process is as follow, for each node $v_i\in V$:

\begin{itemize}
\item
  Choose the cluster according to a multinomial distribution:
  $Z_i \sim \mathtt{Multinomial}(\alpha)$. That is
  $P(Z_i = k|\alpha) = \alpha_k$ with $\alpha_k \in [0,1]$ and
  $\sum_k \alpha_k = 1$.
\item
  Then sample the attribute of the node, from a distribution
  parametrized according to the chosen cluster
  $Y_i \sim P(Y_i |\theta_{Z_i}) = \prod_{t=1}^{T} P(Y_{it}|\theta_{Z_{i},t})$.
  The exact form of $P$ depends of the kind of attribute being
  modelled (binary, categorical, continuous and so on).
\item
  Finally, we potentially connect the vertex $v_i$ to the previous
  ones ${v_j, j<i}$ according to
  $X_{ij} \sim P(X_{ij} |\Phi_{Z_iZ_j})$. The intuition is that two
  vertices from the same cluster will have similar connectivity pattern
  with the rest of the graph.
\end{itemize}

In Section 3.2.2, the authors also described the prior placed on $\alpha$,
$\theta$ and $\Phi$. Note that contrary to the previous model (and because the
focus is on clustering) the edge probabilities only depend indirectly of the
attributes (through the $Z$ variable).

They use a variational approach to perform the parameters inference.


TODO: add this one \fullcite{Zhao2017}


\item \href{https://cs.stanford.edu/people/jure/pubs/cesna-icdm13.pdf}{Community
  Detection in Networks with Node Attributes}~\autocite{Yang2013}

CESNA is based on the following properties:

\begin{quote}
\begin{itemize}[leftmargin=*,nosep]
\item
  ``Nodes that belong to the same communities are likely to be connected
  to each other.
\item
  Communities can overlap, as individual nodes may belong to multiple
  communities.
\item
  If two nodes belong to multiple common communities, they are more
  likely to be connected than if they share only a single common
  community (i.e., overlapping communities are denser).
\item
  Nodes in the same community are likely to share common attributes ---
  for example, a community might consist of friends attending a same
  school.''
\end{itemize}
\end{quote}

The graph $G$ has $C$ communities and $n$ nodes, each $u$ of
them having $K$ binary attributes in its profile vector $X_u$ and
belonging to a community $c$ with intensity
$F_{uc} \in [0, \infty)$.

Link generation is done by considering that the probability of linking
two nodes $u$ and $v$ belonging to the same community $c$ is
$P_{uv}(c) = 1-\exp(F_{uc}F_{vc})$ (which is $0$ is one node does
not belong to $c$). Then, assuming that each community connects $u$
and $v$ independently, we have that
$1-P_{uv} = \prod_c (1-P_{uv}(c)) = \exp(-\sum_c F_{uc}F_{vc})$ from
which $P_{uv} = 1-\exp(-\sum_c F_{uc}F_{vc})$.

\begin{minipage}[t]{0.6\linewidth}
Likewise community memberships generate attributes. The intuition is
that given its community memberships, it should be possible to predict a
node's attributes. That is, considering $F_{u1},\ldots,F_{uC}$ as
input feature of a logistic model, and given the logistic weight
$W_{kc}$ (for attribute $k$ and community $c$), we have
\[Q_{uk} = \frac{1}{1+\exp(-\sum_c W_{kc}F_{uc})}\] and
$X_{uk} \sim \mathrm{Bernoulli}(Q_{uk})$.
\end{minipage}
\begin{minipage}[t]{0.37\linewidth}
  \centering
  \vspace{-\ht\strutbox}
  \includegraphics[width=1\linewidth]{raw/cesna.png}
\end{minipage}


\item add this one (not a generative model) \fullcite{Chesnokov2017}

\item \href{https://arxiv.org/abs/1608.00920}{Community Detection Algorithm
  Combining Stochastic Block Model and Attribute Data Clustering}~\autocite{Kataoka2016}

That one is familiar. First they also made the assumption that the adjacency
matrix is conditionally independent of attribute data given the community
assignment. Then for a given node $i$, first they pick its community $x_i$ from
a multinomial parametrized by $\gamma$.  Then its (one dimensional continuous)
attribute is drawn from the normal distribution of the community $\ell$:
$\mathcal{N}(\mu_\ell, \sigma_\ell)$. Finally the edges are created by a
Stochastic Block Model.

\item \href{https://arxiv.org/abs/1610.09735}{Community Detection with Nodal
  Information}~\autocite{Weng2016}

Here $A$ is the adjacency matrix, $X$ the attribute matrix and $c$ the community
assignment. Their first assumption is that $A \bot X | c$, \enquote{a strong
constraint stating that given the community membership, what nodes are like
(described by covariates $X$) does not affect how they are connected (encoded
in $A$)}.

This motivates them to consider the following model \emph{Node-coupled
Stochastic Block Model (NSBM)}:
\begin{align*}
  P(c|X) &= \prod_i \frac{\exp( \beta^T_{c_i} x_i)}{\sum^K_{k =1}\exp( \beta^T_{c_i} x_i)} \\
  P(A|c) &= \prod_{ i<j} B^{A_{ij}}_{c_i c_j}(1 - B_{c_i c_j})^{(1-A_{ij})}
\end{align*}
where $B = ( B_{ab}) \in [0, 1]^{K\times K}$ is symmetric and
$\beta = ( \beta_1 , \ldots, \beta_K) \in \mathbb{R}^{Kp}$.

\begin{quote}
\enquote{
The distribution $P(A|c)$ follows the stochastic block model (SBM), which, as
the fundamental model, has been extensively studied in the literature. The SBM
implies that the distribution of an edge between node $i$ and $j$ only depends
on their community membership $c_i$ and $c_j$.

The nodes from the same community are stochastically equivalent. The element
$B_{ab}$ in the matrix $B$ represents the probability of edge connection between
a node in community $a$ and a node in community $b$.

The $P ( c | X)$ simply takes a multi-logistic regression form, where we will
assume $\beta_K = 0$ for identifiability.}
\end{quote}

\item\href{https://arxiv.org/abs/1507.04001}{Structure and Inference in
  Annotated Networks}~\autocite{Newman2016}

Consider $n$ nodes labeled $u=1,\ldots,n$, divided into $k$ communities ($u$
belongs to community $s_u$), and associated with one metadata $x_u \in [1, K]$
(that is a simplified version for introductory purpose, later metadata can also
be continuous). Given $\{x_u\}_{u=1}^n$ and the degree sequence
$\{d_u\}_{u=1}^n$, the network is generated as follow. First each node $u$ is
assigned to community $s$ with a probability $\gamma_{s,x_u}$ (the $k\times K$
matrix $\Gamma$ holds these parameters). One each node is assigned a community,
we use a degree corrected block model to create the network.  Edge between $u$
and $v$ is created with probability $p_{uv} = d_ud_v\theta_{s_u,s_v}$. This
time, $\theta$ parameters are stored in a $k\times k$ symmetric matrix $\Theta$.

\item \href{http://doi.org/10.1103/PhysRevE.95.042317}{\citefield{DeBacco2017}{title}}
  \autocite{DeBacco2017}


\end{itemize}

It has been implemented in many ways (make a table?)
Replace distance measure by a Bayesian model~\autocites{Xu2012}{Xu2014}{Zhao2017}.
CESNA (see maingraphSC)~\autocite{Yang2013}, overlapping, compare with CESNA so put them together
\autocite{Chesnokov2017}
generative models and then node membership inference~\autocites{Kataoka2016}{Weng2016}{Newman2016}
\fi





% keep checking in mendeley and community from maingraphSC
%
% \subsection{Mendeley}


% \begin{description}
% \item[\citefield{Zhou2009}{title}] -- \fullcite{Zhou2009}\\ \citefield{Zhou2009}{abstract}
% \item[\citefield{Zhou2010}{title}] -- \fullcite{Zhou2010}\\ \citefield{Zhou2010}{abstract}

% \item[\citefield{Xu2012}{title}] -- \fullcite{Xu2012}\\ \citefield{Xu2012}{abstract}
% \item[\citefield{Xu2014}{title}] -- \fullcite{Xu2014}\\ \citefield{Xu2014}{abstract}

% \item[\citefield{Silva2012}{title}] -- \fullcite{Silva2012}\\ \citefield{Silva2012}{abstract}

  % too far
% \item[\citefield{Ruan2013}{title}] -- \fullcite{Ruan2013}\\ \citefield{Ruan2013}{abstract}

% CESNA
% \item[\citefield{Yang2013}{title}] -- \fullcite{Yang2013}\\ \citefield{Yang2013}{abstract}

% subspace
% \item[\citefield{Huang2015}{title}] -- \fullcite{Huang2015}\\ \citefield{Huang2015}{abstract}

% Too far an not super clear, I think they do node classification
% \item[\citefield{Bayer2015}{title}] -- \fullcite{Bayer2015}\\ \citefield{Bayer2015}{abstract}

% \item[\citefield{Kataoka2016}{title}] -- \fullcite{Kataoka2016}\\ \citefield{Kataoka2016}{abstract}

% \item[\citefield{Weng2016}{title}] -- \fullcite{Weng2016}\\ \citefield{Weng2016}{abstract}

% link with k-CC
% \item[\citefield{Barnes2016a}{title}] -- \fullcite{Barnes2016a}\\ \citefield{Barnes2016a}{abstract}

% \item[\citefield{Newman2016}{title}] -- \fullcite{Newman2016}\\ \citefield{Newman2016}{abstract}

  % Unpublished, benchmarks? (Mason Porter though)
% \item[\citefield{Bazzi2016}{title}] -- \fullcite{Bazzi2016}\\ \citefield{Bazzi2016}{abstract}

  % KG
% \item[\citefield{Ren2017}{title}] -- \fullcite{Ren2017}\\ \citefield{Ren2017}{abstract}

  % fun extension, each nodes is itself a single-attribute graph…
% \item[\citefield{Guo2017}{title}] -- \fullcite{Guo2017}\\ \citefield{Guo2017}{abstract}

% MiMAG mining dense subgraphs featuring a single edge type, so it's more of downstream application
  % “we aim at finding clusters of vertices that are densely connected by edges with similar labels
  % in a subset of the graph layers” paper version of
% \item[\citefield{Boden2017}{title}] -- \fullcite{Boden2017}\\ \citefield{Boden2017}{abstract}

  % Unpublished, quite far anyway
% \item[\citefield{Wu2017}{title}] -- \fullcite{Wu2017}\\ \citefield{Wu2017}{abstract}

  % share the idea of explaining edges: “Given a collection of attributed subgraphs from different
  % classes, how can we discover the attributes that characterize their differences?” thing is, it's
  % about differences so… and there Focus attribute KDD'14 paper is only about node
% \item[\citefield{Rezaei2017}{title}] -- \fullcite{Rezaei2017}\\ \citefield{Rezaei2017}{abstract}

  % Anomaly detection is a nice application in edge-labeled graphs: Novel distributed algorithms
  % that identify problematic nodes whose influence can only be detected on their neighbors,
  % validated through the analysis of data breaches in bank transactions. (except there's no label
  % here) cite it rather for the tensor stuff
% \item[\citefield{Araujo2017}{title}] -- \fullcite{Araujo2017}\\ \citefield{Araujo2017}{abstract}

% \item[\citefield{Baroni2017}{title}] -- \fullcite{Baroni2017}\\ \citefield{Baroni2017}{abstract}

  % nice keywords in the abstract: overlapping, disassortative, layer merging for compression,
  % generative model: the model assigns probabilities to each pair of nodes that they have an
  % unobserved link of each type. However, it needs supervision
  % node have mixed membership to K groups and each layer is generated by a K×K matrix given the
  % density of edges between groups in that specific layer, taking into account the group
  % membership, like in a degree corrected SBM. Unlike us, this allows for multigraphs, but the EM
  % procedure presented to recover the model parameters require observing all the layers, that is
  % full supervision. Once its parameters are found, this model can then be used to predict the
  % existence of extra edges in each layer, which can be seen as edge type prediction. Furthermore,
  % “measuring the extent to which one layer helps predict links in another layer provide a
  % way to measure the relationships between the layers, from redundancy to independence, allowing
  % information compression.
% \item[\citefield{DeBacco2017}{title}] -- \fullcite{DeBacco2017}\\ \citefield{DeBacco2017}{abstract}

  % SNE embedding, move it to conclusion
% \item[\citefield{Liao2017}{title}] -- \fullcite{Liao2017}\\ \citefield{Liao2017}{abstract}

  % Too far, detecting localized attributes on a graph
% \item[\citefield{Chen2017}{title}] -- \fullcite{Chen2017}\\ \citefield{Chen2017}{abstract}

  %ICML, another generative model with fast, fully Bayesian inference but no edge type.
% \item[\citefield{Zhao2017}{title}] -- \fullcite{Zhao2017}\\ \citefield{Zhao2017}{abstract}

  % overlapping, compare with CESNA so put them together
% \item[\citefield{Chesnokov2017}{title}] -- \fullcite{Chesnokov2017}\\ OVERLAPPING \citefield{Chesnokov2017}{abstract}

% \end{description}

% \cite{Zhou2009,Zhou2010,Xu2012,Silva2012,Ruan2013,Yang2013,Xu2014,Huang2015,Bayer2015,BOTHOREL2015,Kataoka2016,Weng2016,Barnes2016a,Newman2016,Bazzi2016,Ren2017,Guo2017,Boden2017,Wu2017,Rezaei2017,Araujo2017,Baroni2017,DeBacco2017,Liao2017,Chen2017,Zhao2017}

\iffalse


\Textcite[section 7.2 edge labelling]{nodeClassif11} mention a cool paper from Darja Krushevskaja
but I can't find it online.

\fullcite{Zhuang2012} and
section 2.3 of Semantic Mining of Social Networks (same thing I believe.
Actually they also talk about a WSDM paper on infering ties using transfer
learning~\autocite{transferTiesPred16})

\Textcite[Section 4.2.2]{miningSocialTheoriesSurvey14} is about \emph{Social Tie
Prediction}, but they only mention \autocite{transferTiesPred16} and
\autocite{Yang2012}.

H. Liang, K. Wang and F. Zhu, "Mining social ties beyond homophily," 2016 IEEE 32nd International Conference on Data Engineering (ICDE), Helsinki, 2016, pp. 421-432.
\url{http://doi.org/10.1109/ICDE.2016.7498259} very datamining approach, they
look for link pattern with support and confidence above some threshold, that
also diverge from homophily by excluding attributes taking the same value (so
it's not exactly like us once again)

The kind of graphs with talked in this paper is a special case of what is
called \emph{heterogenous information network} in \autocite{HINSurvey17}.

learn a function that take as input the embedding of two nodes and output the predicted strength of
the relationship~\autocite{EdgeAwareEmbedding17}, could it be modified to output a cluster index?
\fi
